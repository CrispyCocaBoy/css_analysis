post_id,title,author,score,created_utc,selftext,num_comments,top_comment
1ew1hws,[P] Illustrated book to learn about Transformers &amp; LLMs,shervinea,317,2024-08-19T13:14:52,"I have seen several instances of folks on this subreddit being interested in long-form explanations of the inner workings of Transformers &amp; LLMs.

This is a gap my twin brother and I have been aiming at filling for the past 3 1/2 years. Last week, we published “[Super Study Guide: Transformers &amp; Large Language Models](https://superstudy.guide/transformers-large-language-models/)”, a 250-page book with more than 600 illustrations aimed at visual learners who have a strong interest in getting into the field.

This book covers the following topics in depth:

* **Foundations**: primer on neural networks and important deep learning concepts for training and evaluation.
* **Embeddings**: tokenization algorithms, word embeddings (word2vec) and sentence embeddings (RNN, LSTM, GRU).
* **Transformers**: motivation behind its self-attention mechanism, detailed overview on the encoder-decoder architecture and related variations such as BERT, GPT and T5, along with tips and tricks on how to speed up computations.
* **Large language models**: main techniques to tune Transformer-based models, such as prompt engineering, (parameter efficient) finetuning and preference tuning.
* **Applications**: most common problems including sentiment extraction, machine translation, retrieval-augmented generation and many more.

(In case you are wondering: this content follows the same vibe as the Stanford illustrated study guides we had shared on this subreddit 5-6 years ago about [CS 229: Machine Learning](https://www.reddit.com/r/MachineLearning/comments/98wrkw/illustrated_machine_learning_cheatsheets_covering/), [CS 230: Deep Learning](https://www.reddit.com/r/MachineLearning/comments/a0xfc2/p_illustrated_deep_learning_cheatsheets_covering/) and [CS 221: Artificial Intelligence](https://www.reddit.com/r/MachineLearning/comments/bse25u/p_illustrated_artificial_intelligence_cheatsheets/))

Happy learning!

https://preview.redd.it/n6zraaltemjd1.jpg?width=1905&amp;format=pjpg&amp;auto=webp&amp;s=1110f750df0d8a60d5fdf1d4967b41e1b5617efe",110,Any plans to have an ebook / pdf version? I have way too many physical books and no room for more.
1jbs7xg,"[R] Transformers without Normalization (FAIR Meta, New York University, MIT, Princeton University)",Nunki08,271,2025-03-15T10:30:09,"Transformers without Normalization  
Jiachen Zhu, Xinlei Chen, Kaiming He, Yann LeCun, Zhuang Liu  
arXiv:2503.10622 \[cs.LG\]: [https://arxiv.org/abs/2503.10622](https://arxiv.org/abs/2503.10622)  
Abstract: Normalization layers are ubiquitous in modern neural networks and have long been considered essential. This work demonstrates that Transformers without normalization can achieve the same or better performance using a remarkably simple technique. We introduce Dynamic Tanh (DyT), an element-wise operation DyT(x)=tanh(αx), as a drop-in replacement for normalization layers in Transformers. DyT is inspired by the observation that layer normalization in Transformers often produces tanh-like, S-shaped input-output mappings. By incorporating DyT, Transformers without normalization can match or exceed the performance of their normalized counterparts, mostly without hyperparameter tuning. We validate the effectiveness of Transformers with DyT across diverse settings, ranging from recognition to generation, supervised to self-supervised learning, and computer vision to language models. These findings challenge the conventional understanding that normalization layers are indispensable in modern neural networks, and offer new insights into their role in deep networks.  
code and website: [https://jiachenzhu.github.io/DyT/](https://jiachenzhu.github.io/DyT/)  
Detailed thread on X by Zhuang Liu: [https://x.com/liuzhuang1234/status/1900370738588135805](https://x.com/liuzhuang1234/status/1900370738588135805)

https://preview.redd.it/c017auy7ztoe1.jpg?width=1116&amp;format=pjpg&amp;auto=webp&amp;s=e87b7d0ddd44df8f5a7f789365bf128113307539

",56,What I think this is actually doing is separating feature transformation from feature aggregation. CNNs have gone through a similar development with depthwise separable convolutions.
1hzn0gg,[D] Have transformers won in Computer Vision?,Amgadoz,194,2025-01-12T13:47:30,"Hi,

Transformers have reigned supreme in Natural Language Processing applications, both written and spoken, since BERT and GPT-1 came out in 2018.

For Computer Vision, last I checked it was starting to gain momentum in 2020 with [An Image is Worth 16x16 Words](https://arxiv.org/abs/2010.11929) but the sentiment then was ""Yeah transformers might be good for CV, for now I'll keep using my resnets""

Has this changed in 2025? Are Vision Transformers the preferred backbone for Computer Visions?

Put another way, if you were to start a new project from scratch to do image classification (medical diagnosis, etc), how would you approach it in terms of architecture and training objective?

I'm mainly an NLP guy so pardon my lack of exposure to CV problems in industry.  ",85,If you are literally *only* interested in image classification I would probably try both CNNs and vision transformers. But transformers more easily mix different modality types which is a big advantage.
13j0spj,[R] Tiny Language Models (below 10m parameters or only one transformer block) can generate paragraphs of coherent text and reason...provided training is limited to stories that only contain words that a typical 3 to 4-year-olds usually understand.,MysteryInc152,578,2023-05-16T10:00:43,Paper - https://arxiv.org/abs/2305.07759,123,"This is amazing and fascinating. I admit I had concluded that more data was a key finding of the recent LLM improvements and of course these little models don't compare with the state of the art ones, but identifying key elements that seem to make for really effective toy models is a really useful thing."
1hmitcz,[D] Everyone is so into LLMs but can the transformer architecture be used to improve more ‘traditional’ fields of machine learning ,noithatweedisloud,158,2024-12-26T06:40:47,"
i’m thinking things like recommendation algorithms, ones that rely on unsupervised learning or many other unsupervised algos 

i’ll look more into it but wanted to maybe get some thoughts on it ",87,"Transformers are already massively used in modern rec sys. A classic example is a paper from pinterest: PinnerFormer. 

More recent advancements include something like the HSTU, paper title is like “actions speak louder than words: trillion parameter…”. This is not tsfmr tho, more generative rec sys inspired from llm and etc"
1ipvau4,[D] What's the most promising successor to the Transformer?,jsonathan,177,2025-02-15T06:17:01,"All I know about is MAMBA, which looks promising from an efficiency perspective (inference is linear instead of quadratic), but AFAIK nobody's trained a big model yet. There's also [xLSTM](https://arxiv.org/pdf/2405.04517) and [Aaren](https://arxiv.org/pdf/2405.13956).

What do y'all think is the most promising alternative architecture to the transformer?",65,"Last i heard, google was working on 'Titan', prolly worth a look."
1gaxscv,[D] Transformers are a type of CNN,Ozqo,327,2024-10-24T08:31:06,"https://arxiv.org/abs/2309.10713

I was randomly googling Dynamic Convolutions since I thought they were cool and found this paper that shows transformers are equivalent to a type of CNN that uses dynamic convolutions. The dynamic convolution paper (https://arxiv.org/abs/1912.03458) was released in 2019 so it did come after the attention is all you need paper.

Sadly this paper has only one citation. I think it's incredible. Knowing that transformers can be viewed as a CNN gives them insight into optimising its design, including removing the softmax activation and replacing it with a Relu+normalisation layer. I think there's a ton more improvements that can be made by continuing their work.",65,"Transformers are everything..

Transformers are modern Hopfield Networks: https://arxiv.org/abs/2008.02217

Transformers are State Space Models: https://arxiv.org/abs/2405.21060"
12omnxo,[R] Timeline of recent Large Language Models / Transformer Models,viktorgar,771,2023-04-16T19:53:45,,86,I wonder if GPT-4 could make this graph?
t7qe6b,[R] End-to-End Referring Video Object Segmentation with Multimodal Transformers,Illustrious_Row_9971,2025,2022-03-06T03:52:43,,47,How cherry picked are these? :)
r76igz,[Discussion] (Rant) Most of us just pretend to understand Transformers,sloppybird,564,2021-12-02T12:34:57,"I see a lot of people using the concept of Attention without really knowing what's going on inside the architecture and *why* it works rather than the *how*. Others just put up the picture of attention intensity where the word ""dog"" is ""attending"" the most to ""it"". People slap on a BERT in Kaggle competitions because, well, it is easy to do so, thanks to Huggingface without really knowing what even the abbreviation means. Ask a self-proclaimed person on LinkedIn about it and he will say oh it works on attention and masking and refuses to explain further.  I'm saying all this because after searching a while for ELI5-like explanations, all I could get is a trivial description.",180,"Transformers and attention are a huge hole in my knowledge currently - for some reason I'm just super unmotivated to read about them despite the hype (been spending my time delving into normalising flows, cool shit). However, when my lab-mates are discussing ways to apply transformers to our research and I'm like ""idk what that is"" they look at me like I'm crazy so probably need to fix this lol - anyone got any good review paper recommendations?"
1kpalhd,[P] I built a transformer that skips layers per token based on semantic importance,Silent_Status_4830,165,2025-05-18T03:25:06,"I’m a high school student who’s been exploring how to make transformers/ai models more efficient, and I recently built something I’m really excited about: a transformer that routes each token through a different number of layers depending on how ""important"" it is.

The idea came from noticing how every token, even simple ones like “the” or “of”, gets pushed through every layer in standard transformers. But not every token needs the same amount of reasoning. So I created a lightweight scoring mechanism that estimates how semantically dense a token is, and based on that, decides how many layers it should go through.

It’s called SparseDepthTransformer, and here’s what it does:

* Scores each token for semantic importance
* Skips deeper layers for less important tokens using hard gating
* Tracks how many layers each token actually uses
* Benchmarks against a baseline transformer

In my tests, this reduced memory usage by about 15% and cut the average number of layers per token by \~40%, while keeping output quality the same. Right now it runs a bit slower because the skipping is done token-by-token, but batching optimization is next on my list.

Here’s the GitHub repo if you’re curious or want to give feedback:  
[https://github.com/Quinnybob/sparse-depth-transformer](https://github.com/Quinnybob/sparse-depth-transformer)

Would love if you guys check it out/want to work with me!",37,sounds similar to mixture of depths [https://arxiv.org/abs/2404.02258](https://arxiv.org/abs/2404.02258)
190q1vb,"[D] So, Mamba vs. Transformers... is the hype real?",Instantinopaul,334,2024-01-07T11:19:08,"Heard all the buzz about Mamba, the new kid on the sequence modeling block. Supposedly it's faster, handles longer sequences better, and even outperforms Transformers on some tasks. But is it really a throne-stealer or just another flash in the pan?

My perception:

Strengths: Mamba boasts efficient memory usage, linear scaling with sequence length, and impressive performance in language and DNA modeling. Plus, it ditches the attention mechanism, potentially paving the way for faster inference. 

Weaknesses: Still early days, so Mamba's long-term stability and performance across diverse tasks remain to be seen. And while it doesn't need attention, its state space approach might be trickier to grasp for some folks. 

To the AI aficionados out there, is Mamba just the next shiny toy, or a genuine paradigm shift in sequence modeling? Will it dethrone the mighty Transformer, or coexist as a specialized tool? Let's hear your thoughts!

[https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)",110,"I’ve read the paper. The S6 layers of Mamba have a memory which they modify with each new token. All state space modeling nets work this way but the advantage here is that S6 has control over how each token is remembered (if at all), as opposed to just trying to memorize a compressed version of the entire input sequence. This means it can in theory hold on to important info from a million tokens ago while only keeping short-term details for as long as they’re needed.

Whether or not it works like that in practice for practical-sized models remains to be seen, but even if it doesn’t, more sophisticated versions of the memory state will be developed. Intuitively it makes sense for a system to accept an input one token at a time and have an internal memory (instead of just taking in the entire uncompressed sequence in one go as attention does), so I’m optimistic."
141pxvc,"[d] Apple claims M2 Ultra ""can train massive ML workloads, like large transformer models.""",jl303,282,2023-06-05T20:01:27,"Here we go again... Discussion on training model with Apple silicon.

""Finally, the 32-core Neural Engine is 40% faster. And M2 Ultra can support an enormous 192GB of unified memory, which is 50% more than M1 Ultra, enabling it to do things other chips just can't do. For example, in a single system, it can train massive ML workloads, like large transformer models that the most powerful discrete GPU can't even process because it runs out of memory.""

[WWDC 2023 — June 5](https://www.youtube.com/watch?v=GYkq9Rgoj8E&amp;t=666s)

What large transformer models are they referring? LLMs?

Even if they can fit onto memory, wouldn't it be too slow to train?",169,"Yes. I'm pretty sure it will be leaps and bounds above whatever a regular Intel chipped laptop can do, but I'd debate the usefulness of being able to fit a 100GB model into memory when you have a fraction of processing cores available vs. even a consumer grade GPU, I'm a bit unsure about the usefulness of it. 

Maybe you could fit a 100GB model into the memory and freeze all the layers except a few that you'd then train? 

Okay I'm actually starting to convince myself it could be kinda useful lol"
18qh1hp,"[Discussion] In this age of LLMs, What are the limitations of Transformer architecture and downside to it?",dontgimmehope,99,2023-12-25T11:35:07,"&amp;#x200B;

[Transformer](https://preview.redd.it/x9x11ieagf8c1.jpg?width=1320&amp;format=pjpg&amp;auto=webp&amp;s=f1a9b52c0588840352ede064575046c06a0da67e)",180,"The main limitation is the self attention mechanism's quadratic memory complexity. That is, if you want to add *n* more tokens to the context considered by the model when predicting the next token, you need *n*^2 more memory.

There's also evidence that extending the context window has decreasing marginal benefit. In other words, even if the context window could be scaled up infinitely, the model performance would eventually stall at some point, and apparently we're already closing in to that point. 

It should be noted that these considerations apply to the classic transformer architecture, and there are several proposed workarounds for both issues, though, as per my knowledge, neither is a ""solved problem"" thus far.

Edit: I should point out that the second point is an empirical observation rather than a theoretical limit."
ncdy6m,"[R] Google Replaces BERT Self-Attention with Fourier Transform: 92% Accuracy, 7 Times Faster on GPUs",Yuqing7,694,2021-05-14T17:22:45,"A research team from Google shows that replacing transformers’ self-attention sublayers with Fourier Transform achieves 92 percent of BERT accuracy on the GLUE benchmark with training times seven times faster on GPUs and twice as fast on TPUs.

Here is a quick read: [Google Replaces BERT Self-Attention with Fourier Transform: 92% Accuracy, 7 Times Faster on GPUs.](https://syncedreview.com/2021/05/14/deepmind-podracer-tpu-based-rl-frameworks-deliver-exceptional-performance-at-low-cost-19/)

The paper *FNet: Mixing Tokens with Fourier Transforms* is on [arXiv](https://arxiv.org/abs/2105.03824).",97,How much faster is BERT to train if you stop at 92% accuracy?
1lcja93,[R] Vision Transformers Don't Need Trained Registers,avd4292,74,2025-06-16T03:56:54,"Hi, we have released a new paper that studies the underlying mechanism of artifacts in attention and feature maps from [Vision Transformers Need Registers](https://arxiv.org/abs/2309.16588), a phenomena that has also been observed in LLMs (e.g., [1](https://arxiv.org/abs/2402.17762), [2](https://arxiv.org/abs/2309.17453)). We propose a training-free method to mitigate this. As one of the authors, I am creating this post to kickstart any discussion. 

Paper: [https://arxiv.org/abs/2506.08010](https://arxiv.org/abs/2506.08010)

Project Page: [https://avdravid.github.io/test-time-registers/](https://avdravid.github.io/test-time-registers/)

Code: [https://github.com/nickjiang2378/test-time-registers/tree/main](https://github.com/nickjiang2378/test-time-registers/tree/main)",21,"Very cool paper! I liked this a lot when I saw it a few days ago. Did you guys explore if this emerges in in other transformer based models (i.e. DiT, MAR, Supervised ViT)? Maybe the reason these models previously were dismissed not to have nice attention maps was due to a similar register token. It would align nicely with your Rosetta work too :)"
1jgwjjk,[D] Are GNNs obsolete because of transformers?,Master_Jello3295,109,2025-03-22T00:56:04,"I’ve always been interested in Graph Neural Networks (GNNs) but haven’t had the chance to study them deeply. Now that transformers are prevalent, the attention mechanism—where each query interacts with all keys—feels conceptually similar to operations on densely connected graphs. This makes me wonder if transformers can be considered a type of GNN. Is there any truth to this? Can transformers actually replace GNNs?",31,Transformers are GNNs on a fully connected graph of tokens and with multi-head attention as neighbourhood aggregation. This is a nice post on it: [https://graphdeeplearning.github.io/post/transformers-are-gnns/](https://graphdeeplearning.github.io/post/transformers-are-gnns/)
1g0lnij,[R] nGPT: Normalized Transformer with Representation Learning on the Hypersphere,StartledWatermelon,126,2024-10-10T15:37:27,"**Paper:** [https://arxiv.org/pdf/2410.01131](https://arxiv.org/pdf/2410.01131)

**Abstract:**

&gt;We propose a novel neural network architecture, the normalized Transformer (nGPT) with representation learning on the hypersphere. In nGPT, all vectors forming the embeddings, MLP, attention matrices and hidden states are unit norm normalized. The input stream of tokens travels on the surface of a hypersphere, with each layer contributing a displacement towards the target output predictions. These displacements are defined by the MLP and attention blocks, whose vector components also reside on the same hypersphere. Experiments show that nGPT learns much faster, reducing the number of training steps required to achieve the same accuracy by a factor of 4 to 20, depending on the sequence length.

**Highlights:**

&gt;Our key contributions are as follows:   
  
*Optimization of network parameters on the hypersphere* We propose to normalize all vectors forming the embedding dimensions of network matrices to lie on a unit norm hypersphere. This allows us to view matrix-vector multiplications as dot products representing cosine similarities bounded in \[-1,1\]. The normalization renders weight decay unnecessary.   
  
*Normalized Transformer as a variable-metric optimizer on the hypersphere* The normalized Transformer itself performs a multi-step optimization (two steps per layer) on a hypersphere, where each step of the attention and MLP updates is controlled by eigen learning rates—the diagonal elements of a learnable variable-metric matrix. For each token t\_i in the input sequence, the optimization path of the normalized Transformer begins at a point on the hypersphere corresponding to its input embedding vector and moves to a point on the hypersphere that best predicts the embedding vector of the next token t\_i+1 .   
  
*Faster convergence* We demonstrate that the normalized Transformer reduces the number of training steps required to achieve the same accuracy by a factor of 4 to 20.

**Visual Highlights:**

https://preview.redd.it/0jdj23ew6ytd1.png?width=1313&amp;format=png&amp;auto=webp&amp;s=144f4fa881d05bd1bc90faa2a0bb2c74e58c71df

[Not sure about the difference between 20k and 200k budgets; probably the best result from runs with different initial learning rates is plotted](https://preview.redd.it/8tf5tw0x6ytd1.png?width=1187&amp;format=png&amp;auto=webp&amp;s=4f9dfbe1f49bdc8aed6fa953dc9220556d7dc947)

https://preview.redd.it/waof2llr7ytd1.png?width=1337&amp;format=png&amp;auto=webp&amp;s=3f82cee29c5fe753e219edf55ab16460fcf9a11a

https://preview.redd.it/a5vburms7ytd1.png?width=859&amp;format=png&amp;auto=webp&amp;s=a3f34b73a580a5798bd5e10e9a4cc950b93fa691

",57,What is the intuition on why this performs better and whether it will scale?
1fz0pya,[R] Differential Transformer (Microsoft Research),Decent_Action2959,201,2024-10-08T14:10:25,"Abstract: Transformer tends to overallocate attention to irrelevant context. In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps. The subtraction cancels noise, promoting the emergence of sparse attention patterns. Experimental results on language modeling show that Diff Transformer outperforms Transformer in various settings of scaling up model size and training tokens. More intriguingly, it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. By being less distracted by irrelevant context, Diff Transformer can mitigate hallucination in question answering and text summarization. For in-context learning, Diff Transformer not only enhances accuracy but is also more robust to order permutation, which was considered as a chronic robustness issue. The results position Diff Transformer as a highly effective and promising architecture to advance large language models.",41,"Didn't really understand how you were able to differentiate the original query, key and value terms in important and noise terms. 

The changes to the actual attention calculation by subtracting the noise was clear."
1bit2f9,[D] Why do transformers use embeddings with the same dimensionality in each layer?,timtom85,126,2024-03-19T19:35:46,"My intuition is that tokens get gradually enriched as we move through the layers, but that would mean we need to store a lot less information per token in the early layers than in the later ones.

Wouldn't it make sense to start out with (relatively) low-dimensional embeddings, and then project or extend these onto higher dimensions, until they reach their final size?",86,"Maybe, but I think this would:
  - Remove the empirically very useful residual stream (is very good for training stability – it also allows [information transfer directly between non-adjacent layers](https://transformer-circuits.pub/2021/framework/index.html) mostly without affecting the intermediate layers)
  - Probably remove the model's ability to organize its embedding space into something stable across layers, removing things like [emergent outliers](https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/)
  - Not increase the amount of information at any layer – it would either stay the same as it currently is or get worse

So what would the potential benefits be? It could *maybe* reduce latency, because the matrix multiplications in early layers would be smaller, and the memory bandwidth usage would be slightly lower, but I'm not sure how significant that would be. It would also slightly reduce parameter count, as long as you didn't increase the feed-forward width of earlier layers to compensate."
11okrni,[Discussion] Compare OpenAI and SentenceTransformer Sentence Embeddings,Simusid,546,2023-03-11T13:54:22,,58,"Two quick tips for finding the best embedding models:

Sentence Transformers documentation compares models: [https://www.sbert.net/docs/pretrained\_models.html](https://www.sbert.net/docs/pretrained_models.html)

Massive Text Embedding Benchmark (MTEB) Leaderboard has 47 different models: [https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

These will help you compare different models across a lot of benchmark datasets so you can figure out the best one for your use case."
1eu3auv,[D] HuggingFace transformers - Bad Design?,duffano,142,2024-08-16T23:30:30,"Hi,

I am currently working with HuggingFace's transformers library. The library is somewhat convenient to load models and it seems to be the only reasonable platform for sharing and loading models. But the deeper I go, the more difficulties arise and I got the impression that the api is not well designed and suffers a lot of serious problems.

The library allows for setting the same options at various places, and it is not documented how they interplay. For instance, it seems there is no uniform way to handle special tokens such as EOS. One can set these tokens 1. in the model, 2. in the tokenizer, and 3. in the pipeline. It is unclear to me how exactly these options interplay, and also the documentation does not say anything about it. Sometimes parameters are just ignored, and the library does not warn you about it. For instance, the parameter ""add\_eos\_token"" of the tokenizer seems to have no effect in some cases, and I am not the only one with this issue (https://github.com/huggingface/transformers/issues/30947). Even worse is that it seems the exact behavior often depends on the model, while the library pretends to provide a uniform interface. A look into the sourcecode confirms that they actually distingish depending on the currently loaded model.

Very similar observations concern the startup scripts for multi-threading, in particular: accelerate. I specify the number of cores, but this is just ignored. Without notification, without any obvious reason. I see in the system monitor that it still runs single-threaded. Even the samples taken from the website do not always work.

In summary, there seems to be an uncontrolled growth of configuration settings. Without a clear structure and so many effects influencing the library that large parts of its behavior are in fact undocumented. One could also say, it looks a bit unstable and experimental. Even the parts that work for me worry me as I have doubts if everything will work on another machine after deployment.

Anyone having thoughts like this?",57,"If you ever find yourself writing a constructor with [119 keyword arguments](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments), it's time to rethink your approach IMO."
1jn0ha9,[R] [D] My (Mostly Failed) Attempt to Improve Transformers by Enriching Embeddings with the Last Hidden State – Why It Didn't Scale,Academic_Sleep1118,166,2025-03-30T00:29:36,"Hi guys!

I [recently posted](https://www.reddit.com/r/MachineLearning/comments/1iu4ymf/d_enriching_token_embedding_with_last_hidden_state/) on this sub about what I believed was a sub-optimal feature of Decoder Transformers: namely the fact that the last hidden state, which has the potential to carry a lot of information (32 bits \* embedding dim), is collapsed into a single token (assuming temperature is 0), that can only carry log2(vocab\_size) bits of information.

I tested a new architecture where the last hidden state of the transformer is used to enrich the embedding of the token that was generated using it (it = the last hidden state).

And, would you believe it? It failed. 

The worst thing about it is that it worked well enough for very small (100K params) transformers to give me hope and feed my self delusional grandiosity. I had even given this architecture a name. But when I scaled it up (a whopping 1M params!!), the compute overhead stopped being worth the improvement.

The high-level idea of why it failed is that every hidden state of every previous token, up to the penultimate one (the input of the last decoder block) are available when predicting the next token, thanks to the token-mixing property of the attention mechanism. Only the last couple of hidden states (the input of the last decoder block's FFN, and final linear layer + softmax) are unavailable, as there are no token-mixing steps left. So this hidden state injection idea is merely about not discarding the work done by the last couple layers, which is not that important when there are a lot of decoder layers (the marginal importance of each layer decreases).

Anyway, I wrote a [5,000 words post](https://www.eloidereynal.com/p/a-partially-failed-attempt-at-improving?r=4ksqg3&amp;utm_campaign=post&amp;utm_medium=web&amp;showWelcomeOnShare=false) about why it failed, with a bit of nice math and some cattle pictures, just in case you like cows. 

Honestly, the post is quite long and technical, but you might find one or two interesting things, especially if you like to read about the failures of other people.",17,"Thanks for the detailed write up! I think some of the LaTeX is broken, particularly the equation above:

&gt; Anyway, two good reasons to believe, at first sight, that the Stateful"
1li43eh,"[R] [ClsToken, AvgPool] can be a poor choice for transformer embedding models",agbrothers,29,2025-06-23T01:22:32,"This paper started with the following question: why do some approaches choose ClsToken vs AvgPool vs MaxPool for Transformer-based embedding models like BERT or ViT, and what are the consequences? Often, these summarization techniques seem like convenient methods for aligning dimensions that just happen to work well enough, and the decision comes down to empirical performance rather than being motivated mathematically. This then evolved into the question — what is the best possible way to summarize embeddings?

We address this question by introducing a framework to evaluate pooling methods as lossy compressors, taking inspiration from vector quantization. For a given task, only a subset of the embeddings matter (signal) while the rest should be treated as noise by the compressor and ignored. The goal of any such pooling method should thus be to aggregate the embeddings in a way that minimizes signal loss.

This reframing reveals failure modes for common methods like ClsToken, AvgPool, and MaxPool as signal-to-noise ratios vary. This result led us to investigate an adaptive attention-based pooling formulation and show that it can both theoretically and empirically lead to better performance and robustness of Transformer embedding models in a variety of applications.

📃 Paper: [https://www.arxiv.org/abs/2506.09215](https://www.arxiv.org/abs/2506.09215)   
👾 Code: [https://github.com/agbrothers/pooling](https://github.com/agbrothers/pooling)

Side note — this is my first main-track conference paper and I’m excited, but also a bit intimidated by the poster session (I’m only a Master’s student). I don’t have an advisor to lean on, so if anyone has any feedback or advice I would really appreciate it!",18,"Nice work! In my opinion there should be a little more highlight on the problem of choosing a good starting query for the AdaPool method, although you indeed discuss it.


Moreover, I'd suggest you check the paper ""Keep It SimPool"", which basically proposes AdaPool with AvgPool as the starting query. They do a greater theoretical work in unifying pooling methods, but they don't do the work you did for SNR and robustness analysis.


Cheers!"
1l74fv7,[P][R] Sparse Transformers: Run 2x faster LLM with 30% lesser memory,Economy-Mud-6626,73,2025-06-09T13:11:57,"We have built fused operator kernels for structured contextual sparsity based on the amazing works of LLM in a Flash (Apple) and Deja Vu (Zichang et al). We avoid loading and computing activations with feed forward layer weights whose outputs will eventually be zeroed out.

The result? We are seeing **5X faster MLP layer performance** in transformers with 50% lesser memory consumption avoiding the sleeping nodes in every token prediction. For Llama 3.2, Feed forward layers accounted for **30% of total weights** and forward pass computation resulting in **1.6-1.8x increase** in throughput:

    Sparse LLaMA 3.2 3B vs LLaMA 3.2 3B (on HuggingFace Implementation):
    - Time to First Token (TTFT):  1.51× faster (1.209s → 0.803s)
    - Output Generation Speed:     1.79× faster (0.7 → 1.2 tokens/sec)  
    - Total Throughput:           1.78× faster (0.7 → 1.3 tokens/sec)
    - Memory Usage:               26.4% reduction (6.125GB → 4.15GB)

Please find the operator kernels with differential weight caching open sourced (Github link in the comment).

PS: We will be actively adding kernels for int8, CUDA and sparse attention.

Update: We also opened a [discord server](https://discord.gg/CxzDDffR) to have deeper discussions around sparsity and on-device inferencing.",14,Github project link: [https://github.com/NimbleEdge/sparse\_transformers](https://github.com/NimbleEdge/sparse_transformers)
13i43n0,[R] MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers,redpnd,274,2023-05-15T10:17:25,,86,"&gt; Autoregressive transformers are spectacular models for short sequences but scale poorly to long sequences such as high-resolution images, podcasts, code, or books. We proposed Megabyte, a multi-scale decoder architecture that enables end-to-end differentiable modeling of sequences of over one million bytes. Megabyte segments sequences into patches and uses a local submodel within patches and a global model between patches. This enables sub-quadratic self-attention, much larger feedforward layers for the same compute, and improved parallelism during decoding -- unlocking better performance at reduced cost for both training and generation. Extensive experiments show that Megabyte allows byte-level models to perform competitively with subword models on long context language modeling, achieve state-of-the-art density estimation on ImageNet, and model audio from raw files. Together, these results establish the viability of tokenization-free autoregressive sequence modeling at scale."
18zie7z,Transformer-Based LLMs Are Not General Learners: A Universal Circuit Perspective [R],we_are_mammals,273,2024-01-05T21:39:40,"https://openreview.net/forum?id=tGM7rOmJzV

&gt; (LLMs') remarkable success triggers a notable shift in the research priorities of the artificial intelligence community. These impressive empirical achievements fuel an expectation that LLMs are “sparks of Artificial General Intelligence (AGI)"". However, some evaluation results have also presented confusing instances of LLM failures, including some in seemingly trivial tasks. For example, GPT-4 is able to solve some mathematical problems in IMO that could be challenging for graduate students, while it could make errors on arithmetic problems at an elementary school level in some cases.

&gt; ...

&gt; Our theoretical results indicate that T-LLMs fail to be general learners. However, the T-LLMs achieve great empirical success in various tasks. We provide a possible explanation for this inconsistency: while T-LLMs are not general learners, they can partially solve complex tasks by memorizing a number of instances, leading to an illusion that the T-LLMs have genuine problem-solving ability for these tasks.",59,"Reminds me of [""What algorithms can transformers learn?""](https://arxiv.org/abs/2310.16028) from earlier this year. 

They created a programming language called RASP-L that can implement all the same operations as transformers. They found that algorithmic tasks that transformers are bad at (like addition or parity) are also difficult to implement in RASP-L. 

They also found that reformatting these tasks in such a way that they could be easily solved with RASP-L also allowed transformers to learn them."
1k63r4a,[D] Is my take on transformers in time series reasonable / where is it wrong?,ReinforcedKnowledge,39,2025-04-23T16:37:52,"Hi everyone!

For a bit of context, I'm giving some lectures in time series to an engineering class and the first course I just introduced the main concepts in time series (stationarity, ergodicity, autocorrelations, seasonality/cyclicity and a small window on its study through frequency analysis).

I wanted this course to invite students to think throughout the course about various topics and one of the open questions I asked them was to think whether natural language data can be considered non-stationary and if it is the case, why transformers do so well on it but not in other fields where data is non-stationary time series.

I gave them other lectures about different deep learning models, I tried to talk about inductive biases, the role of the architecture etc. And now comes the final lecture about transformers and I'd like to tackle that question I gave them.

And here's my take, I'd love it if you can confirm if some parts of it are correct, and correct the parts that are wrong, and maybe add some details that I might have missed.

This is not a post to say that actual foundational models in time series are good. I do not think that is the case, we have tried many time at work, whether using them out of the shelf, fine-tuning them, training our own smaller ""foundational"" models it never worked. They always got beaten by simpler methods, sometimes even naive methods. And many times just working on the data, reformulating the problem, adding some features or maybe understanding that it is this other data that we should care about etc., led to better results.

My ""worst"" experience with time series is not being able to beat my AR(2) model on a dataset we had for predicting when EV stations will break down. The dataset was sampled from a bunch of EV stations around the city, every hour or so if I remember correctly. There was a lot of messy and incoherent data though, sometimes sampled at irregular time intervals etc. And no matter what I did and tried, I couldn't beat it.

I just want to give a reasonable answer to my students. And I think the question is very complex and it is very much related to the field of question, its practices and the nature of its data, as much as of the transformer architecture itself. I do not claim I am an expert in time series or an expert in transformers. I'm not a researcher. I do not claim this is the truth or what I say is a fact. This is why I'd like you to criticize as much as possible whatever I think. This would be helpful to me to improve and will also be helpful to me students. Thank you.

I think we can all agree, to some extent at least, that transformers have the ability to learn very an AR function, or whatever ""traditional"" / ""naive"" method. At least in theory. Well it's hard to prove I think, we have to prove that our data lives in a compact space (correct me if I'm wrong please) but we can just agree upon it.  But in practice we don't notice that. I think it's mainly due to the architecture. Again, I might be wrong, but in general in machine learning it's better to use these types of architectures with low constraining inductive biases (like transformers) when you have very large datasets, huge compute power and scaling capability and let the model learn everything by itself. Otherwise, it's better to use some architecture with stronger inductive biases. It's like injecting some kind of prelearned knowledge about the dataset or the task to bridge that gap of scale. I might be wrong and again I'd love to be corrected on this take. And I think we don't always have that for time series data, *or*, we have it but are not using it properly. And by the way if you allow me this mini-rant within this overly huge thread, I think a lot of foundational model papers are dishonest. I don't want to mention specific ones because I do not want any drama here, but many papers inflate their perceived performance, in general through misleading data practices. If you are interested about this we can talk about it in private and I can refer you to some of those papers and why I think it is the case. 

So I think the issue is multi-faceted, like it is always the case in science, and most probably I'm not covering anything. But I think it's reasonable to start with: 1/ the field and its data, 2/ how we formulate the forecasting task (window, loss function), 3/ data itself when everything else is good.

Some fields like finance are just extremely hard to predict. I don't want to venture into unknown waters, I have never worked in finance, but from what a quant friend of mine explained to me, is that, if you agree with the efficient market hypothesis, predicting the stock price is almost impossible to achieve and that most gains come from predicting volatility instead. To be honest, I don't really understand what he told me but from what I gather is that the prediction task itself is hard, and that is independent of the model. Like some kind of Bayes limit. Maybe it'd be better to focus on volatility instead in the research papers. 

The other thing that I think might cause issues is the forecast window. I wouldn't trust the weather forecast in 6 months. Maybe its a model issue, but I think the problem is inherent to non-stationary data. 

Why do transformers work so well on natural language data then? I think its due to many things, two of them would be large scale data and having correlations repeated through it. If you take a novel from the 19th century from a British author, I think it'd be hard to learn a ""good"" model of what that language is, but having many different authors gives you a set of data that *probably* contain enough repeating correlations, though each author is unique, there are *probably* some kind of common or basis of language mastery, for the model to be able to learn a ""good enough"" model. This is without taking into account the redundant data, code for example. Asking an LLM to sort a list in place in Python will always result in the same *correct* answer because it is repeated through the training set. The other thing would be our metric of what a good model is or our expectation of what a good model is. A weather forecasting model is measured by the difference of its output with respect to the actual measurements. But if I ask a language model how to sort a list in Python, whether it gives me directly the answer or it talks a little bit before doesn't change much my judgment of the model. The loss functions during training are different as well, and some might argue its easier to fit cross-entropy for the NLP task than fitting some regression functions on some time series data.

That's why I think transformers in most cases of time series do not work well and we're better off with traditional approaches. And maybe this whole thread gives an idea of when we can apply time series (in a field where we can predict well, like weather forecasting, using shorter horizons, and using very large scale data). Maybe to extend the data we can include context from other data sources as well but I don't have enough experience with that to talk about it.

Sorry for this very huge thread, and if you happen to read it I'd like to thank you and I'd love to hear what you think about this :)

Thank you again!

",25,"IMO transformers work well on natural language because:
1) natural language is auto-correlated at *multiple scales*,
2) tokens, in language, have very rich embedding spaces,
3) we have a fuckton of language data.


And most time series problems just don’t have those interesting properties. Therefore simpler models with high inductive biases do great.

In particular, I think that the multi-scale autocorrelation with long time-horizon dependencies makes next-token-prediction work super well in language. Transformers with big context windows do a really great job at finding and exploiting text that’s separated by thousands of tokens.

Language has structure at the word-level, at the sentence-level, at the paragraph-level, at the chapter level. And they have really subtle interactions.

Many time series decompose to like, cyclic + trend. Or basically just act like a state-transition function.

Also we have way more text data and it’s super diverse."
1lieh3l,[D] Is it possible to convert music audio to guitar tabs or sheet music with transformers?,No-Score712,22,2025-06-23T11:46:53,"Hey folks,

I'm a guitarist who can't sing, so I play full song melodies on my guitar (fingerstyle guitar). I admire those who can transcribe music into tabs or sheet music, but I can't do this myself.

I just had an interesting thought - the process of transcribing music to sheets sounds a lot like language translation, which is a task that the transformer model is originally built for. If we could somehow come up with a system that represents sheet music as tokens, would it be possible to train such a transformer to take audio tokens as input and the sheet music as output?

Any input or thoughts would be greatly appreciated.",16,"Yeah this is its own research field (Automatic Music Transcription). [Here's a relevant blog from Magenta](https://magenta.tensorflow.org/transcription-with-transformers) (i.e. Google's music AI research lab). There's plenty of recent research, also some more user-friendly software like AnthemScore as the other commenter said"
1kj7ylw,[D] Best Way to Incorporate Edge Scores into Transformer After GNN?,AdInevitable1362,16,2025-05-10T11:21:35,"Hi everyone, 

I’m working on a social recommendation system using GNNs for link prediction. I want to add a Transformer after the GNN to refine embeddings and include score ratings (edge features). 

I haven’t found papers that show how to pass score ratings into the Transformer. Some mention projecting the scalar into an embedding. Does adding the score rating or the relation scalar is not recommended ? 

Has anyone dealt with this before please?",22,"There are plenty of existing GNN blocks that incorporate edge features into node embeddings (My favourite is [TransformerConv](https://pytorch-geometric.readthedocs.io/en/2.6.1/generated/torch_geometric.nn.conv.TransformerConv.html)) so when you put your node features through your non-GNN transformer, they should include relevant edge embeddings mixed in to the node embeddings.

Can I ask why you are putting the node features through a transformer? I'm pretty sure you would get just as good a result using just a TransformerConv in your GNN, and nothing afterwards."
i49jf8,"[D] Biggest roadblock in making ""GPT-4"", a ~20 trillion parameter transformer",AxeLond,350,2020-08-05T17:21:59,"So I found this paper, [https://arxiv.org/abs/1910.02054](https://arxiv.org/abs/1910.02054) which pretty much describes how the GPT-3 over GPT-2 gain was achieved, 1.5B -&gt; 175 billion parameters

# Memory

&gt;Basic data parallelism (DP) does not reduce memory per device, and runs out of memory for models with more than 1.4B parameters on current generation of GPUs with 32 GB memory

The paper also talks about memory optimizations by clever partitioning of Optimizer State, Gradient between GPUs to reduce need for communication between nodes. Even without using Model Parallelism (MP), so still running 1 copy of the model on 1 GPU.

&gt;ZeRO-100B can train models with up to 13B parameters without MP on 128 GPUs, achieving throughput over 40 TFlops per GPU on average. In comparison, without ZeRO, the largest trainable model with DP alone has 1.4B parameters with throughput less than 20 TFlops per GPU.

Add 16-way Model Parallelism in a DGX-2 cluster of Nvidia V100s and 128 nodes and you got capacity for around 200 billion parameters. From MP = 16 they could run a 15.4x bigger model without any real loss in performance, 30% less than peak performance when running 16-way model parallelism and 64-way data parallelism (1024 GPUs).

This was all from Gradient and Optimizer state Partitioning, they then start talking about parameter partitioning and say it should offer a linear reduction in memory proportional to number of GPUs used, so 64 GPUs could run a 64x bigger model, at a 50% communication bandwidth increase. But they don't actually do any implementation or testing of this.

# Compute

Instead they start complaining about a compute power gap, their calculation of this is pretty rudimentary. But if you redo it with the method cited by GPT-3 and using the empirically derived values by GPT-3 and the cited paper,   [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361) 

Loss (L) as a function of model parameters (N) should scale,

L = (N/8.8 \* 10\^13)\^-0.076

Provided compute (C) in petaFLOP/s-days is,

L = (C/2.3\*10\^8)\^-0.05  ⇔ L = 2.62 \* C\^-0.05

GPT-3 was able to fit this function as 2.57 \* C\^-0.048

So if you just solve C from that,

[C = 2.89407×10\^-14 N\^(19/12)](https://www.wolframalpha.com/input/?i=%28N%2F8.8*10%5E13%29%5E-0.076+%3D+2.57*C%5E-0.048+solve+C)

If you do that for the same increase in parameters as GPT-2 to GPT-3, then you get

C≈3.43×10\^7 for [20 trillion](https://www.wolframalpha.com/input/?i=C+%3D+2.89407%C3%9710%5E-14+N%5E%2819%2F12%29+and+N+%3D+175+billion+%2F+1.5+billion+*+175+billion) parameters, vs 18,300 for 175 billion. 10\^4.25 PetaFLOP/s-days looks around what they used for GPT-3, they say several thousands, not twenty thousand, but it was also slightly off the trend line in the graph and probably would have improved for training on more compute.

You should also need around 16 trillion tokens, GPT-3 trained on 300 billion tokens (function says 370 billion ideally). English Wikipedia was 3 billion. 570GB of webcrawl was 400 billion tokens, so 23TB of tokens seems relatively easy in comparison with compute.

With GPT-3 costing around [$4.6 million](https://lambdalabs.com/blog/demystifying-gpt-3/) in compute, than would put a price of [$8.6 billion](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7%2F18%2C300+*+%244.6M+) for the compute to train ""GPT-4"".

If making bigger models was so easy with parameter partitioning from a memory point of view then this seems like the hardest challenge, but you do need to solve the memory issue to actually get it to load at all.

However, if you're lucky you can get 3-6x compute increase from Nvidia A100s over V100s,  [https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/](https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/)

But even a 6x compute gain would still put the cost at $1.4 billion.

Nvidia only reported $1.15 billion in revenue from ""Data Center"" in 2020 Q1, so just to train ""GPT-4"" you would pretty much need the entire world's supply of graphic cards for 1 quarter (3 months), at least on that order of magnitude.

The Department of Energy is paying AMD $600 million to build the 2 Exaflop El Capitan supercomputer. That supercomputer could crank it out in [47 years](https://www.wolframalpha.com/input/?i=3.43%C3%9710%5E7+petaFLOPS*+days++%2F+%282+EXAFLOPS%29).

To vastly improve Google search, and everything else it could potentially do, $1.4 billion or even $10 billion doesn't really seem impossibly bad within the next 1-3 years though.",138,If you have 8.6 billion to spend on building a language model I suggest put $5 billion into research grants. You could probably train a pretty good model with the remaining 3.6 billion and thirty thousand new research papers on language modeling.
10kdeex,"H3 - a new generative language models that outperforms GPT-Neo-2.7B with only *2* attention layers! In H3, the researchers replace attention with a new layer based on state space models (SSMs). With the right modifications, it can outperform transformers. Also has no fixed context length.",MysteryInc152,480,2023-01-24T19:11:08,,52,"Code and model weights were released

https://github.com/HazyResearch/H3"
1j7bozz,[P] Guys did my model absolutely blew Transformer?,TwoSunnySideUp,0,2025-03-09T16:45:39,"Transformer (standard):
batch = 64, block_size = 256, learning rate = 0.0003, embedding_dimension = 384, layer = 6, heads = 6, dataset = Tiny Shakespeare, max_iters = 5000, character level tokenisation

My model (standard):
same as transformer except for learning rate = 0.0032 with lr scheduler, embedding_dimension = 64, heads don't apply atleast as of now

Why nan happened during end of training, will experiment tomorrow but have some clues.

Will upload the source code after I have fixed nan issue and optimised it further.",34,This says absolutely nothing about anything 
1l68rlb,[P] BERT-Emotion: Lightweight Transformer Model (~20MB) for Real-Time Emotion Detection,boltuix_dev,27,2025-06-08T10:12:13,"Hi all,

I am sharing **BERT-Emotion**, a compact and efficient transformer model fine-tuned for short-text emotion classification. It supports **13 distinct emotions** such as Happiness, Sadness, Anger, and Love.

**Key details:**

* **Architecture**: 4-layer BERT with hidden size 128 and 4 attention heads
* **Size**: \~20MB (quantized), suitable for mobile, IoT, and edge devices
* **Parameters**: \~6 million
* Designed for offline, real-time inference with low latency
* **Licensed** under Apache-2.0, free for personal and commercial use

The model has been downloaded over **11,900 times last month**, reflecting active interest in lightweight NLP for emotion detection.

**Use cases** include mental health monitoring, social media sentiment analysis, chatbot tone analysis, and smart replies on resource constrained devices.

Model and details are available here:  
[https://huggingface.co/boltuix/bert-emotion](https://huggingface.co/boltuix/bert-emotion)

*I welcome any feedback or questions!*

For those interested, **full source code &amp; dataset are available** in a detailed walkthrough on YouTube.",13,"I think the biggest problem of such models is that they dont work for mixed emotions related to different subjects. For example how will it handle the following text review?

""I had so much trouble with other service providers that I lost all my hope for finding a reliable service provider. Luckily I found ABC XYZ LTD and they exceeded all my expectations. Of course nobody is perfect, they also have room to grow but they were pretty good for my use case."""
1lcqcd6,[D] Time series Transformers- Autogressive or all at once?,Sufficient_Sir_4730,3,2025-06-16T11:28:17,"One question I need help with, what would you recommend - predicting all 7 days (my predict length) at once or in an autoregressive manner? Which one would be more suitable for time series transformers.",12,"This is honestly one of the most debated design choices in time series transformers and the answer depends heavily on your specific use case. I work at a consulting firm that helps companies optimize their forecasting systems, and we see teams make the wrong choice on this constantly.

For 7-day forecasting, here's what actually works in practice:

All-at-once (direct multi-step) is usually better for time series transformers because:

Error accumulation kills autoregressive approaches. Each prediction becomes input for the next, so errors compound exponentially over 7 steps. Your day 7 forecast ends up being garbage.

Training efficiency is way better. You can parallelize the entire prediction sequence instead of doing sequential forward passes.

The attention mechanism in transformers is designed to capture long-range dependencies across the entire sequence, which works better when predicting all steps simultaneously.

Autoregressive only makes sense when:

You have very strong sequential dependencies where each day's prediction critically depends on the previous day's actual outcome.

Your prediction horizon is really short (1-2 steps) where error accumulation isn't a huge problem.

You're doing online learning where you can incorporate actual observations as you get them.

For 7-day forecasting specifically, go with all-at-once. The attention mechanism will capture the weekly patterns better than trying to chain predictions together.

Most successful production time series transformers use direct multi-step prediction. The only exception is when you're doing really long horizons (30+ days) where you might use a hybrid approach.

What's your specific domain? That might affect the recommendation since some industries have stronger sequential dependencies than others."
1i1l8d4,[R] Transformer²: Self-Adaptive LLMs,hardmaru,188,2025-01-15T00:41:09,"Paper: https://arxiv.org/abs/2501.06252

**Abstract**

Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer², a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer² employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific ""expert"" vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Transformer² demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer² represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems. 

Blog Summary: https://sakana.ai/transformer-squared/

GitHub: https://github.com/SakanaAI/self-adaptive-llms",13,I think this is the first Sakana paper I've seen that didn't list you as an author. I'm interpreting that as a sign that your lab is getting bigger. Congrats!
1ib2vtx,[D] Why did DeepSeek open-source their work?,we_are_mammals,960,2025-01-27T07:48:28,"If their training is 45x more efficient, they could have dominated the LLM market. Why do you think they chose to open-source their work? How is this a net gain for their company? Now the big labs in the US can say: ""we'll take their *excellent* ideas and we'll just combine them with our *secret* ideas, and we'll still be ahead""

---

*Edit:* `DeepSeek-R1` is now ranked #1 in the LLM Arena (with `StyleCtrl`). They share this rank with 3 other models: `Gemini-Exp-1206`, `4o-latest` and `o1-2024-12-17`.",330,"Look for posts that ask, why did meta open-source their llama models?"
1ev32c0,[D] Normalization in Transformers,Collegesniffer,132,2024-08-18T06:52:16,"Why isn't BatchNorm used in transformers, and why is LayerNorm preferred instead? Additionally, why do current state-of-the-art transformer models use RMSNorm? I've typically observed that LayerNorm is used in language models, while BatchNorm is common in CNNs for vision tasks. However, why do vision-based transformer models still use LayerNorm or RMSNorm rather than BatchNorm?

",34,"In LayerNorm, for a (B, T, C) tensor, the mean and variance is computed across the channel/embedding (C) dimension for each position (T) and for each sample in batch (B). This results in (B \* T) different means and variances. The normalization is applied independently to each sample across all the channels/embeddings (C). RMSNorm operates similarly to LayerNorm but only computes the root mean square (RMS) across the channel/embedding (C) dimension for each position (T) and for each sample in batch (B). This results in (B \* T) different RMS values. The normalization is applied by dividing each sample's activations by its RMS value, without subtracting the mean, making it computationally more efficient than LayerNorm.

Since BatchNorm computes the mean and variance across the batch dimension and depends on batch size, it is not used in transformers due to variable sequence lengths in NLP. It requires storing the running mean and variance for each feature, which is memory-intensive for large models. Also, during distributed training, batch statistics need to be synced across multiple GPUs. LayerNorm is preferred not just in NLP but even in vision based transformers because it normalizes each sample independently, making it invariant to sequence length and batch size. RMSNorm operates in a very similar manner to LayerNorm but is more computationally efficient (since, unlike LayerNorm, mean subtraction is not performed and only RMS values are calculated) and can potentially lead to quicker convergence during training."
1ad1o11,[D] What are the OUTPUT embeddings in transformer? Where does it come from? (not the input embeddings),ShlomiRex,227,2024-01-28T12:31:51,,42,"Because the original architecture was designed for translation. Input Embeddings of a language to Output embeddings of another. 

Later the encoder was dropped for decoder only translation where the model simply learns to autocomplete by corrupting the training data and expecting the model to learn how to uncorrupt it. 

Surprisingly as more and more data is thrown at the network and it's scaled up, it learns interesting correlations and abilities. Even translation to the point of a context aware translation. 

You're looking at the unadulterated architecture: now we just link them together. In other words, different steps of the model but they share the same weights."
1ayog60,[D] What Are the Fundamental Drawbacks of Mamba Compared to Transformers?,Alarmed-Profile5736,111,2024-02-24T07:11:08,"Hello!

I've been pondering this question for some time. To clarify, I'm not referring to aspects like ""it hasn't been tested extensively,"" ""its scalability is uncertain,"" or ""there's a lack of industry infrastructure."" Instead, I'm interested in understanding the core differences between the transformer and Mamba architectures, specifically how these differences may place Mamba at a disadvantage compared to Transformers.

Best regards!

**Edit:**

From what I can understand from your answers, Transformers are ""better"" in the following sense compared to Mamba in that:

* Transformers does not compress the input.
* Transformers can handle non-sequential data.
* Transformer might be better to handle instructions that is located at the end of an input.

**Edit 2:**

To sum things up:

* **Transformers:** More compute for larger contexts but access to more information albeit possibly some useless information
* **Mamba:** Less compute for larger contexts but access to less information and therefore risks missing out on information.",58,Attention being O(ctx_len^2 ) has problems but is also really useful -- you can always retrieve exact sequence contents at any point in the context and can retrieve totally different values for every token. With SSMs in general and Mamba in particular you have to compress the sequence and so can do only a limited amount of retrieval on past values.
1h4urpr,[R] Simplified RNNs Achieve Transformer-Like Performance with Parallel Training and Reduced Parameters,Successful-Western27,116,2024-12-02T13:19:02,"This paper systematically examines whether RNNs might have been sufficient for many NLP tasks that are now dominated by transformers. The researchers conduct controlled experiments comparing RNNs and transformers while keeping model size, training data, and other variables constant.

Key technical points:
- Tested both architectures on language modeling and seq2seq tasks using matched parameters (70M-1.5B)
- Introduced ""RNN with Parallel Generation"" (RPG) allowing RNNs to generate tokens in parallel like transformers
- Evaluated on standard benchmarks including WikiText-103 and WMT14 En-De translation
- Analyzed representation capacity through probing tasks and attention pattern analysis

Main results:
- RNNs matched or outperformed similarly-sized transformers on WikiText-103 language modeling
- Transformers showed 1-2 BLEU score advantage on translation tasks
- RPG achieved 95% of transformer generation speed with minimal accuracy loss
- RNNs showed stronger local context modeling while transformers excelled at long-range dependencies

I think this work raises important questions about architecture choice in modern NLP. While transformers have become the default, RNNs may still be viable for many applications, especially those focused on local context. The parallel generation technique could make RNNs more practical for production deployment.

I think the results suggest we should reconsider RNNs for specific use cases rather than assuming transformers are always optimal. The computational efficiency of RNNs could be particularly valuable for resource-constrained applications.

TLDR: Comprehensive comparison shows RNNs can match transformers on some NLP tasks when controlling for model size and training. Introduces parallel generation technique for RNNs. Results suggest architecture choice should depend on specific application needs.

[Full summary is here](https://aimodels.fyi/papers/arxiv/were-rnns-all-we-needed). Paper [here](https://arxiv.org/abs/2410.01201)",22,There are some spicy comments on this paper on [openreview](https://openreview.net/forum?id=GrmFFxGnOR).
1fbavdv,[R] Adam Optimizer Causes Privileged Basis in Transformer Language Models,rrenaud,66,2024-09-07T16:26:05,,39,"I'm not sure that this blog post qualifies as research per se. It seems like cargo cult science; like, it mimics some of the aesthetics of science but lacks the corresponding substance.

The motivating statement is strange, and also wrong:

&gt; Mathematical theories of the transformer architecture do not predict this. They expect rotational equivariance within a model, where one dimension is no more important than any other. Is there something wrong with our reasonably informed intuitions of how transformers work?

Wait, what? A hypothetical mathematical theory that predicts rotational equivariance is not an intuition, it's a theorem about whose accuracy we can have no doubts. Whereas if you're operating based on intuition then that means that you don't already have a mathematical theory to support your beliefs. You have to pick one of these, it can't be both.

Also, there are no citations for this statement, presumably because it is incorrect. Mathematical theory does not predict transformers to have rotational equivariance; in fact AFAIK it predicts the opposite.

There's a good paper on this topic: [Scalars are universal: Equivariant machine learning, structured like classical physics](https://arxiv.org/abs/2106.06610). They *prove* that if a model with a bunch of vector inputs v_n has orthogonal group equivariance respect to these vectors (which is what this blog post means to say) then that model can be written as a function of only the inner products of the v_n. That's not true of transformers, which is why they're not orthogonal group equivariant.

Indeed there is a very large number of *peer reviewed* papers about the general topic of model equivariance. This blog post cites none of them, and does not seem to be aware of them. It does recommend reading this other blog post, though, which seems to be the inspiration for its content: https://transformer-circuits.pub/2023/privileged-basis/index.html

That blog post similarly appears to be cargo cult science. It cites no papers to back up its premise and provides very little mathematics to support what it's talking about; the contents are mostly hand waving. It also seems to be confused about the difference between rotational equivariance and equivariance with respect to the general linear group.

For people who are interested in this kind of stuff with respect to transformers you should take a look at this document: https://johnthickstun.com/docs/transformers.pdf . It provides a concise summary of the standard transformer model in terms of equations. It's really difficult to do any kind of meaningful reasoning about transformers without framing it in these terms.

TLDR random arxiv posts are already a pretty sketchy resource for info on ML research and that's doubly true of random blog posts."
1lb2eah,[P] Non Diverse predictions for Time Series Custom Transformer using global Zscore and RevIn,Sufficient_Sir_4730,0,2025-06-14T06:56:19,"Hi. Im currently building a custom transformer for time series forecasting ( percentage deltas) for an index. I added RevIn along with global Zscore but have this issue that predictions are almost constant (variation after 4-5 decimals for all samples). Added revin the solve the problem of index shift, but facing this issue. Any suggestions?",9,how are you sampling?
1g13gkd,[R] Differential Transformer,fliiiiiiip,231,2024-10-11T06:26:11,"### [Paper](https://arxiv.org/abs/2410.05258)

### Abstract

&gt; Transformer tends to overallocate attention to irrelevant context. 
&gt; In this work, we introduce Diff Transformer, which amplifies attention to the relevant context while canceling noise. 
&gt; Specifically, the differential attention mechanism calculates attention scores as the difference between two separate softmax attention maps.
&gt; The subtraction cancels noise, promoting the emergence of sparse attention patterns. [...]
&gt; [...] it offers notable advantages in practical applications, such as long-context modeling, key information retrieval, hallucination mitigation, in-context learning, and reduction of activation outliers. [...]",16,original post/discussion: https://www.reddit.com/r/MachineLearning/comments/1fz0pya/r_differential_transformer_microsoft_research
17cmzcz,[D] Transformers are basically CNNs?,Veson,186,2023-10-20T22:11:27,"I've watched an interesting video: [Deriving the Ultimate Neural Network Architecture from Scratch](https://www.youtube.com/watch?v=kWLed8o5M2Y). It's about how to come up to the transformer architecture when you have an understanding of CNNs.

The crux of it is an idea of pairwise convolutional layers. The first layer applies not to the sequence of words itself, but to all pairs of words in the sentence. This ensures that each relation of words that are far from each other is taken into account.

The next convolutional layer applies to all pairs of results of the previous one. This way longer subsequences of words are factored in.

[pairs of words](https://preview.redd.it/mn9yma2jfevb1.png?width=649&amp;format=png&amp;auto=webp&amp;s=15376a9db4749f437c46bf414381a34fec3806d0)

A couple of questions:

* Transformers are basically CNNs?
* Are there any articles on how transformers were invented? I see a lot of explanations of the original paper, but at best they all answer the question *how* transformers work. But *why* is the architecture the way it is? Was it discovered like the video describes? Or the path was more convoluted? I'd like to know more about this connection.

Anyway, it would be great to figure out in all details how these pairwise layers are related to the concepts of query, key, and value. Here's what the author of the video wrote on youtube:

&gt;Yeah it's a term I made up so you won't find it in any sources, sorry  about that. Usually sources will just talk about self attention in terms  of key, query and value lookups, so you can look at those to get a more  detailed understanding of the transformer. The value transform is  equivalent to the linear representation function I use in the pairwise  convolution, the key and query attention scores are equivalent to the  bi-linear form scoring function I use (with the bi-linear form weight  matrix given by Q\^TK). I chose to use this unusual terminology because,  personally, I feel the key, query and value terminology comes out of  nowhere, and I wanted to connect the transformer more directly to its  predecessor (the CNN).

Update: the author gave a deeper explanation of the math [here in the comments](https://www.reddit.com/r/MachineLearning/comments/17cmzcz/comment/k5t7g70/?context=2).",56,"The ""[attention is all you need](https://arxiv.org/abs/1706.03762)"" paper is a machine translation paper. [There's](https://arxiv.org/pdf/1702.00887.pdf) [a thread](https://arxiv.org/pdf/1508.04025.pdf) [of prior](https://arxiv.org/pdf/1409.0473.pdf) work that suggests that CNNs had very little, if at all, to do with its inspiration, and rather that it was devised to directly represent word correlations between source and target text."
svb5bx,[D] What are the biggest architectural innovations since Transformers?,MikeFent0n,224,2022-02-18T06:32:52,"Title. Transformers, to my knowledge, seems to have been the last big innovation in architecture (following from CNNs, LSTMS, graphs) that has really propagated into every day use. What paper(s) from the last few years are the ""next"" Transformers, in your opinion?",101,"It looks like diffusion models are going to continue to grow in importance and are still actively expanding right now. idk

Edit: and transformers still have a lot of viable usecases where they have not really been applied yet"
1hli20i,"[D] In Byte Latent Transformer, how is the decoded patch boundary determined?",TommyX12,40,2024-12-24T17:19:39,"In Meta’s recent paper Byte Latent Transformer, I understand that the local encoder model uses the patch segmentation method (e.g. the entropy based method) to cut patches first and then for each patch, cross attention will attend to the bytes in that batch (since the patch boundaries are already determined). However, how does decoding work in this case? Is it that when each byte is being decoded, it is assumed to be in the latest patch, and if the new output byte is detected as a new patch boundary (e.g. using the entropy based method), it cuts a new patch and future bytes now belong to this patch? If this is the case, won’t the starting byte of each output patch be effectively decoded using the previous patch? Or is it that, when the new boundary is found, this byte is discarded, a new patch is started, and its starting byte is decoded again using this new patch? I am not sure if the author explicitly mentioned this in the paper.",27,"The latent transformer is autoregressive, works the same way as standard decoder only transformer. It just works on patches instead of tokens. 

The local encoder and local decoder are 2 encoder-decoder transformers with cross-attention (like the original transformer paper) that replaces the tokenizer encoder and decoder. 

So essentially 

LLM = latent transformer

Tokenizer training = entropy transformer 

Tokenizer encoding = encoder-decoder transformer

Tokenizer decoding = encoder-decoder transformer"
1esteqd,"[R] I've devised a potential transformer-like architecture with O(n) time complexity, reducible to O(log n) when parallelized.",Conscious-Gazelle-91,89,2024-08-15T12:03:51,"\[R\] I've attempted to build an architecture that uses plain divide and compute methods. From what I can see and understand, it seems to work, at least in my eyes. While there's a possibility of mistakes in my code, I've checked and tested it without finding any errors.

I'd like to know if this approach is anything new. If so, I'm interested in collaborating with you to write a research paper about it. Additionally, I'd appreciate your help in reviewing my code for any potential mistakes.

But most most importantly I want to know about the architecture ,is it new, has anyone has tried this or something similar ,

I've written a Medium article that includes the code. The article is available at: [https://medium.com/@DakshishSingh/equinox-architecture-divide-compute-775a8ff698fe](https://medium.com/@DakshishSingh/equinox-architecture-divide-compute-775a8ff698fe)

Your assistance and thoughts on this matter would be greatly appreciated. If you have any questions or need clarification, please feel free to ask.",36,"Just skimmed through your article, looks interesting but I'd question the result that ""It almost achieves perplexity near zero and 100% accuracy in predicting the next token"". Is your architecture meant to be a causal LM? If so, I don't see any ""masking"" mechanism, which could be a reason why the result is so suspicious. I might be wrong, since I haven't read your code yet. I will take a closer look later."
s252wb,[R] ConvNets vs Transformers,AdelSexy,323,2022-01-12T12:24:53," [A ConvNet for the 2020s](https://arxiv.org/pdf/2201.03545.pdf) \- nice read to start 2022. The authors explore modernizations of Resnets and adopt some tricks from transformers training design to make ConvNets great again.

There is a lot to reflect and thing about.

 Code is [here](https://github.com/facebookresearch/ConvNeXt?fbclid=IwAR3l75JSoSW_MKKXgshjB7BHgHfwS-2rfFeQjpAH3yk-KOqnKTjv-hjHnuU). 

https://preview.redd.it/kqnqe86729b81.png?width=2696&amp;format=png&amp;auto=webp&amp;s=ac0a4f045c61c34756cfcce3073792ace8f64301",75,[deleted]
1ktenon,[D] Improving VQVAE+Transformer Text-to-Image Model in TensorFlow – Balancing Codebook Usage and Transformer Learning,TubaiTheMenace,3,2025-05-23T08:45:22,"Hello everyone,

I'm currently working on a VQVAE + Transformer model for a text-to-image task, implemented entirely in TensorFlow. I'm using the Flickr8k dataset, limited to the first 4000 images (reshaped to 128x128x3) and their first captions due to notebook constraints (Kaggle).

The VQVAE uses residual blocks, a single attention block on both encoder and decoder, and incorporates commitment loss, entropy loss, and L2 loss. When downsampled to 32x32, the upsampled image quality is fairly good (L2 ~2), but codebook usage remains low (~20%) regardless of whether the codebook shape is 512×128 or 1024×128.

My goal is to use the latent image representation (shape: batch_size x 1024) as a token prediction task for the transformer, using only the captions (length 40) as input. However, the transformer ends up predicting a single repeated token.

To improve this, I tried adding another downsampling and upsampling block to reduce the latent size to 256 tokens, which helps the transformer produce varied outputs. However, this results in blurry and incoherent images when decoded.

I’m avoiding more complex methods like EMA for now and looking for a balance between good image reconstruction and useful transformer conditioning. Has anyone here faced similar trade-offs? Any suggestions on improving codebook usage or sequence alignment strategies for the transformer?

Appreciate any insights!",9,So you train VQVAE and transformer at the same time?
11sboh1,[D] Our community must get serious about opposing OpenAI,SOCSChamp,3049,2023-03-15T22:34:01,"OpenAI was founded for the explicit purpose of democratizing access to AI and acting as a counterbalance to the closed off world of big tech by developing open source tools.

They have abandoned this idea entirely.

Today, with the release of GPT4 and their direct statement that they will not release details of the model creation due to ""safety concerns"" and the competitive environment, they have created a precedent worse than those that existed before they entered the field. We're at risk now of other major players, who previously at least published their work and contributed to open source tools, close themselves off as well.

AI alignment is a serious issue that we definitely have not solved. Its a huge field with a dizzying array of ideas, beliefs and approaches. We're talking about trying to capture the interests and goals of all humanity, after all. In this space, the one approach that is horrifying (and the one that OpenAI was LITERALLY created to prevent) is a singular or oligarchy of for profit corporations making this decision for us. This is exactly what OpenAI plans to do.

I get it, GPT4 is incredible. However, we are talking about the single most transformative technology and societal change that humanity has ever made. It needs to be for everyone or else the average person is going to be left behind.

We need to unify around open source development; choose companies that contribute to science, and condemn the ones that don't.

This conversation will only ever get more important.",449,The biggest issue is that they've started a trend and now most probably all the other AI/ML major forces will stop releasing their findings or at least restrict what gets published. It would probably happen sooner or later but it's pretty ironic it started with **Open**AI
18apkw6,[D] Which architecture could substitute the transformer?,NoIdeaAbaout,183,2023-12-04T17:44:29," I recently read this perspective discussing that the transformer could be replaced.  


[https://towardsdatascience.com/a-requiem-for-the-transformer-297e6f14e189](https://towardsdatascience.com/a-requiem-for-the-transformer-297e6f14e189)

In general, articles have been published in the past few months showing the limitations of the transformer:

[https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)

[https://arxiv.org/abs/2304.15004](https://arxiv.org/abs/2304.15004)  


in computer vision, ConvNets with the same budget would seem to have similar performance:

[https://arxiv.org/abs/2310.19909](https://arxiv.org/abs/2310.19909)

[https://arxiv.org/abs/2310.16764](https://arxiv.org/abs/2310.16764)  


DeepMind shows that the transformer would not be able to generalize beyond the training set distribution:  
[https://arxiv.org/abs/2311.00871](https://arxiv.org/abs/2311.00871)

Models such as liquid NN, Hyena, spiking NN, and others show an active search for a new architecture:

[https://arxiv.org/pdf/2006.04439.pdf](https://arxiv.org/pdf/2006.04439.pdf)

[https://www.together.ai/blog/monarch-mixer](https://www.together.ai/blog/monarch-mixer)  


Not that the transformer is likely to be supplanted any time soon, though what I wonder is at the present time what might be the next dominant architecture?  


None of the proposed architectures seem to have a competitive advantage over the transformer ",47,[error fetching comment: 'body']
1hhhcu7,[D] Are LSTMs faster than transformers during inference?,Complex-Media-8074,67,2024-12-19T01:24:20,"Transformers have an O(n\*\*2) parallel attention computation which makes me think that they would be slower than an O(n) LSTM during inference but there has also been a lot of work in speeding up and parallelizing transformers. 

How do they compare for single data point and batch data inference?",22,"During inference both models operate autoregressively. Transformers perform more computations than an RNN over the sequence length due to the attention mechanism which attends to each previous timestep during generation. Even with tricks like KV-cache, attention still requires O(N) computations per timestep during inference. The RNN on the other hand compresses the sequence history into its hidden state and thus is constant-time at inference."
19eemq2,[D] Vision Mamba Strikes Again! Is the Transformer Throne Crumbling?,Instantinopaul,88,2024-01-24T11:06:31,"Remember Mamba, the state-space model that rocked NLP? Well, hold onto your pixels, because they're crushing it in computer vision now too!

Their new model, Vision Mamba, ditches the self-attention craze and leans on state space magic. The result? Performance on par with top vision transformers (DeiT) like, but with better efficiency!

This might be a game-changer, folks. We're talking faster, lighter models that can run on your grandma's laptop, but still see like a hawk.

Any thoughts? I am excited to see some competition in the transformers space. Can we expect a chatgpt v2 on this new architecture. Apologies! Might sound crazy and too early to comment on.

Check out the paper: [https://paperswithcode.com/paper/vision-mamba-efficient-visual-representation](https://paperswithcode.com/paper/vision-mamba-efficient-visual-representation)",59,this reads like an ad
1gys51e,[D] Emergent Cognitive Pathways In Transformer Models. Addressing Fundamental Flaws About Limits.,ipassthebutteromg,14,2024-11-24T14:27:12,"**TLDR:**

Cognitive functions like reasoning and creativity emerge as models scale and train on better data. Common objections crumble when we consider humans with unusual cognitive or sensory differences—or those with limited exposure to the world—who still reason, formulate novel thoughts, and build internal models of the world.

EDIT: It looks like I hallucinated the convex hull metric as a requirement for out of distribution tests. I thought I heard it in a Lex Fridman podcast with either LeCun or Chollet, but while both advocate for systems that can generalize beyond their training data, neither actually uses the convex hull metric as a distribution test. Apologies for the mischaracterization.

**OOD Myths and the Elegance of Function Composition**

Critics like LeCun and Chollet argue that LLMs can't extrapolate beyond their training data, ~~often citing convex hull measurements~~. This view misses a fundamental mathematical reality: novel distributions emerge naturally through function composition. When non-linear functions f and g combine as f(g(x)), they create outputs beyond the original training distributions. This is not a limitation but a feature of how neural networks generalize knowledge.

Consider a simple example: training on {poems, cat poems, Shakespeare} allows a model to generate ""poems about cats in Shakespeare's style""—a novel computational function blending distributions. Scale this up, and f and g could represent Bayesian statistics and geopolitical analysis, yielding insights neither domain alone could produce. Generalizing this principle reveals capabilities like reasoning, creativity, theory of mind, and other high-level cognitive functions.

**The Training Data Paradox**

We can see an LLM's training data but not our own experiential limits, leading to the illusion that human knowledge is boundless. Consider someone in 1600: their 'training data' consisted of their local environment and perhaps a few dozen books. Yet they could reason about unseen phenomena and create new ideas. The key isn't the size of the training set - it's how information is transformed and recombined.

**Persistent Memory Isn't Essential**

A common objection is that LLMs lack persistent memory and therefore can’t perform causal inference, reasoning, or creativity. Yet people with anterograde amnesia, who cannot form new memories, regularly demonstrate all these abilities using only their working memory. Similarly, LLMs use context windows as working memory analogs, enabling reasoning and creative synthesis without long-term memory.

**Lack of a World Model**

The subfield of mechanistic interpretation strongly implies by its existence alone, that transformers and neural networks do create models of the world. One claim is that words are not a proper sensory mechanism and so text-only LLMs can't possibly form a 3D model of the world.

Let's take the case of a blind and deaf person with limited proprioception who can read in Braille. It would be absurd to claim that because their main window into the world is just text from Braille, that they can't reason, be creative or build an internal model of the world. We know that's not true.

Just as a blind person constructs valid world models from Braille through learned transformations, LLMs build functional models through composition of learned patterns. What critics call 'hallucinations' are often valid explorations of these composed spaces - low probability regions that emerge from combining transformations in novel ways.

**Real Limitations**

While these analogies are compelling, true reflective reasoning might require recursive feedback loops or temporal encoding, which LLMs lack, though attention mechanisms and context windows provide partial alternatives. While LLMs currently lack true recursive reasoning or human-like planning, these reflect architectural constraints that future designs may address.

**Final Thoughts**

The non-linearity of feedforward networks and their high-dimensional spaces enables genuine novel outputs, verifiable through embedding analysis and distribution testing. Experiments like Golden Gate Claude, where researchers amplified specific neural pathways to explore novel cognitive spaces, demonstrate these principles in action. We don't say planes can't fly simply because they're not birds - likewise, LLMs can reason and create despite using different cognitive architectures than humans. We can probably approximate and identify other emergent cognitive features like Theory of Mind, Metacognition, Reflection as well as a few that humans may not possess.",33,"I think this is skipping over some legitimate empirical and theoretical objections to the hyper scaling paradigm:

*  While it's true that scaling &amp; training models increases correct answers on hard problems, scaling &amp; training both also leads to increasingly confident, wrong answers, and that area of increased confident wrongness is proportionately higher (Zhou et al., 2024)
* Early research on LLMs found emergent abilities as models scaled (Woodside, 2024). Subsequent research has shown however that emergence may be a mirage caused by faulty metrics. Early benchmarks use in the emergence studies were all-or-nothing measures and so steady, partial improvement towards problem solving hid smooth improvement.  When metrics are adjusted to measure progress and partial solving, improvements smooth out, with the apparent emergence of new abilities vanishing (Schaeffer et al., 2024). 
* While LLMs have improved on problem-solving reasoning benchmarks as they scale, this may be a result of pattern memorization.  One example of this is the “reversal curse”, where models can memorize a relationship unidirectionally but not bi-directionally (Berglund et al., 2023; Golovneva et al., 2024). That is, LLMs can memorize that “A has feature B,” but not that “B is a feature A,” unless the model is double trained to separately memorize this relationship. 
* Recent research on mathematical reasoning also highlights the issue of LLM performance as memorization (Mirzadeh, 2024). If benchmarks are abstracted to symbols (e.g instead of “If Tony has four apples and Janet has six,” the question has “If {name} has {x} apples and {name} has {y}”) not only does accuracy drop dramatically (up to 65%), but this fragility also increases with the length of the benchmark question.  Further, if linguistically similar but irrelevant information (“five of the kiwis are smaller than average”), LLMs tend to naively incorporate this irrelevant information, e.g. subtracting the smaller kiwis.
* Theoretically, there is no model that explains how LLMs can model physics or causality.  The weighted association of words around ""blade,"" ""knife"" edge"" etc. don't model how sharp steel affects flesh under force, nor is there a theoretical understanding of how an LLM could accurately model causality, like how bad getting stabbed can be.
* Again, in addition to the empirical evidence that LLMs cannot do symbolic work (math, logical reasoning), there is no theoretical explanation of how they could. 

There's good reasons to think transformers have inherent limits that cannot be bypassed by hyperscaling, and it's not crazy to suggest tat LLMs are important but partial: that real intelligence while require hybrids systems, e.g. physics inspired neural networks (PINNs), information lattice learning, causal models, neurosymbolic models, and LLMs together."
ydqmjp,"[P] Up to 12X faster GPU inference on Bert, T5 and other transformers with OpenAI Triton kernels",pommedeterresautee,369,2022-10-26T06:10:48,"We are releasing [Kernl](https://github.com/ELS-RD/kernl/) under Apache 2 license, a library to make PyTorch models inference significantly faster. With 1 line of code we applied the optimizations and made Bert up to 12X faster than Hugging Face baseline. T5 is also covered in this first release (&gt; 6X speed up generation and we are still halfway in the optimizations!). This has been possible because we wrote custom GPU kernels with the new OpenAI programming language Triton and leveraged TorchDynamo.

**Project link**: [https://github.com/ELS-RD/kernl/](https://github.com/ELS-RD/kernl/)

**E2E demo notebooks**: [XNLI classification](https://github.com/ELS-RD/kernl/blob/main/tutorial/bert%20e2e.ipynb), [T5 generation](https://github.com/ELS-RD/kernl/blob/main/tutorial/t5%20e2e.ipynb)

[Benchmarks ran on a 3090 RTX GPU, 12 cores Intel CPU, more info below](https://preview.redd.it/mlo3wvn0d3w91.png?width=2738&amp;format=png&amp;auto=webp&amp;s=1b9dce736ee4c0e371b54b9ef796310f9728660d)

On long sequence length inputs, [Kernl](https://github.com/ELS-RD/kernl/) is most of the time the fastest inference engine, and close to Nvidia TensorRT on shortest ones. Keep in mind that Bert is one of the most optimized models out there and most of the tools listed above are very mature.

What is interesting is not that [Kernl](https://github.com/ELS-RD/kernl/) is the fastest engine (or not), but that the code of the kernels is short and easy to understand and modify. We have even added a Triton debugger and a tool (based on Fx) to ease kernel replacement so there is no need to modify PyTorch model source code.

Staying in the comfort of PyTorch / Python maintains dynamic behaviors, debugging and iteration speed. Teams designing/training a transformer model (even custom) can take care of the deployment without relying on advanced GPU knowledge (eg. CUDA programming, dedicated inference engine API, etc.).

Recently released models relying on slightly modified transformer architectures are rarely accelerated in traditional inference engines, we need to wait months to years for someone (usually inference engine maintainers) to write required custom CUDA kernels. Because here custom kernels are written in OpenAI Triton language, **anyone without CUDA experience** can easily modify them: OpenAI Triton API is simple and close to Numpy one. Kernels source code is significantly shorter than equivalent implementation in CUDA (&lt; 200 LoC per kernel). Basic knowledge of how GPU works is enough. We are also releasing a few tutorials we initially wrote for onboarding colleagues on the project. We hope you will find them useful: [https://github.com/ELS-RD/kernl/tree/main/tutorial](https://github.com/ELS-RD/kernl/tree/main/tutorial). In particular, there is:

* Tiled matmul, the GPU way to perform matmul: [https://github.com/ELS-RD/kernl/blob/main/tutorial/1%20-%20tiled%20matmul.ipynb](https://github.com/ELS-RD/kernl/blob/main/tutorial/1%20-%20tiled%20matmul.ipynb)
* Simple explanation of what Flash attention is and how it works, a fused attention making long sequences much faster: [https://github.com/ELS-RD/kernl/blob/main/tutorial/4%20-%20flash%20attention.ipynb](https://github.com/ELS-RD/kernl/blob/main/tutorial/4%20-%20flash%20attention.ipynb)

And best of the best, because we stay in the PyTorch / Python ecosystem, we plan in our roadmap to also enable **training** with those custom kernels. In particular [Flash attention](https://github.com/HazyResearch/flash-attention) kernel should bring a 2-4X speed up and the support of very long sequences on single GPU (paper authors went as far as 16K tokens instead of traditional 512 or 2048 limits)! See below for more info.

**IMPORTANT**: Benchmarking is a difficult art, we tried to be as fair as possible. Please note that:

* Timings are based on wall-clock times and we show speedup over baseline as they are easier to compare between input shapes,
* When we need to choose between speed and output precision, we always choose precision
* HF baseline, CUDA graphs, Inductor and [Kernl](https://github.com/ELS-RD/kernl/) are in mixed precision, AITemplate, ONNX Runtime, DeepSpeed and TensorRT have their weights converted to FP16.
* Accumulation is done in FP32 for AITemplate and [Kernl](https://github.com/ELS-RD/kernl/). TensorRT is likely doing it in FP16.
* CUDA graphs is enabled for all engines except baseline, Nvfuser and ONNX Runtime which [has a limited support of it](https://github.com/microsoft/onnxruntime/issues/12977#issuecomment-1258406358).
* For [Kernl](https://github.com/ELS-RD/kernl/) and AITemplate, fast GELU has been manually disabled (TensorRT is likely using Fast GELU).
* AITemplate measures are to be taken with a grain of salt, it [doesn’t manage attention mask](https://github.com/facebookincubator/AITemplate/issues/46#issuecomment-1279975463) which means 1/ batch inference can’t be used in most scenarios (no padding support), 2/ it misses few operations on a kernel that can be compute-bounded (depends of sequence length), said otherwise it may make it slower to support attention mask, in particular on long sequences. AITemplate attention mask support will come in a future release.
* For TensorRT for best perf, we built 3 models, one per batch size. AITemplate will support dynamic shapes in a future release, so we made a model per input shape.
* Inductor is in prototype stage, performances may be improved when released, none of the disabled by default optimizations worked during our tests.

As you can see, CUDA graphs erase all CPU overhead (Python related for instance), sometimes there is no need to rely on C++/Rust to be fast! Fused kernels (in CUDA or Triton) are mostly important for longer input sequence lengths. We are aware that there are still some low hanging fruits to improve [Kernl](https://github.com/ELS-RD/kernl/) performance without sacrificing output precision, it’s just the first release. More info about how it works [here](https://github.com/ELS-RD/kernl#how).

**Why?**

We work for Lefebvre Sarrut, a leading European legal publisher. Several of our products include transformer models in latency sensitive scenarios (search, content recommendation). So far, ONNX Runtime and TensorRT served us well, and we learned interesting patterns along the way that we shared with the community through an open-source library called [transformer-deploy](https://github.com/ELS-RD/transformer-deploy). However, recent changes in our environment made our needs evolve:

* New teams in the group are deploying transformer models in prod directly with PyTorch. ONNX Runtime poses them too many challenges (like debugging precision issues in fp16). With its inference expert-oriented API, TensorRT was not even an option;
* We are exploring applications of large generative language models in legal industry, and we need easier dynamic behavior support plus more efficient quantization, our creative approaches for that purpose we shared [here on Reddit](https://www.reddit.com/r/MachineLearning/comments/uwkpmt/p_what_we_learned_by_making_t5large_2x_faster/) proved to be more fragile than we initially thought;
* New business opportunities if we were able to train models supporting large contexts (&gt;5K tokens)

On a more personal note, I enjoyed much more writing kernels and understanding low level computation of transformers than mastering multiple complicated tools API and their environments. It really changed my intuitions and understanding about how the model works, scales, etc. It’s not just OpenAI Triton, we also did some prototyping on C++ / CUDA / Cutlass and the effect was the same, it’s all about digging to a lower level. And still the effort is IMO quite limited regarding the benefits. If you have some interest in machine learning engineering, you should probably give those tools a try.

**Future?**

Our road map includes the following elements (in no particular order):

* Faster warmup
* Ragged inference (no computation lost in padding)
* Training support (with long sequences support)
* Multi GPU (multiple parallelization schemas support)
* Quantization (PTQ)
* New batch of Cutlass kernels tests
* Improve hardware support (&gt;= Ampere for now)
* More tuto

Regarding training, if you want to help, we have written an issue with all the required pointers, it should be very doable: [https://github.com/ELS-RD/kernl/issues/93](https://github.com/ELS-RD/kernl/issues/93)

On top of speed, one of the main benefits is the support of very long sequences (16K tokens without changing attention formula) as it’s based on [Flash Attention](https://github.com/HazyResearch/flash-attention).

Also, note that future version of PyTorch will include [Inductor](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747). It means that all PyTorch users will have the option to compile to Triton to get around [1.7X faster training](https://dev-discuss.pytorch.org/t/torchinductor-update-3-e2e-model-training-with-torchdynamo-inductor-gets-1-67x-2-1x-speedup/793).

A big thank you to Nvidia people who advised us during this project.",49,"As the creator/maintainer of Triton, I find this very exciting! Thanks for putting in all that work, and sorry for all the bugs you may have faced along the way -- we are working hard on re-designing the whole thing to make it more stable in the long run!

&gt;On a more personal note, I enjoyed much more writing kernels andunderstanding low level computation of transformers than masteringmultiple complicated tools API and their environments.

This is exactly why I started the project in the first place, and it is very rewarding to read this. Really glad that this project has helped people gain a deeper understanding of how neural networks computations get parallelized for execution on GPUs. :-)"
1l1azju,[D] How to train a model for Speech Emotion Recognition without a transformer?,Defiant_Strike823,3,2025-06-02T06:31:55,"(I'm sorry if this is the wrong tag for the post, or if the post is not supposed to be here, I just need some help with this)

Hey guys, I'm building a speech analyzer and I'd like to extract the emotion from the speech for that. But the thing is, I'll be deploying it online so I'll have very limited resources when the model will be in inference mode so I can't use a Transformer like wav2vec for this, as the inference time will be through the roof with transformers so I need to use Classical ML or Deep Learning models for this only.

So far, I've been using the CREMA-D dataset and have extracted audio features using Librosa (first extracted ZCR, Pitch, Energy, Chroma and MFCC, then added Deltas and Spectrogram), along with a custom scaler for all the different features, and then fed those into multiple classifiers (SVM, 1D CNN, XGB) but it seems that the accuracy is around 50% for all of them (and it decreased when I added more features). I also tried feeding in raw audio to an LSTM to get the emotion but that didn't work as well.

Can someone please please suggest what I should do for this, or give some resources as to where I can learn to do this from? It would be really really helpful as this is my first time working with audio with ML and I'm very confused as to what to here.

(P.S.: Mods I agree this is noob's question, but I've tried my best to make it non-low-effort)",6,"Hey! I am pursing my PhD in Foundational Audio AI, and from my experience I'd say that a small CNN architecture with dilated convolutions should do the job. Check the paper that introduced it to the audio field to understand the architecture a bit.   
  
Instead of generating audio, you can pull the embeddings using mean/max/sum aggregation and pass it to the linear layer to classify emotions. Also, from my understanding you will not be doing real-time detection so you can drop the casuality constraint and use non-casual convolutions.

[https://arxiv.org/pdf/1609.03499](https://arxiv.org/pdf/1609.03499)

PS: You can also try normal convolutions, but dilated convolutions give you a higher resolution with lower number of parameters."
1aj9swv,Repeat After Me: Transformers are Better than State Space Models at Copying [R],we_are_mammals,137,2024-02-05T06:16:25,"https://arxiv.org/abs/2402.01032

**Abstract:**

Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as ""generalized state space models"" (GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.",46,Seems a little obvious? Of course a transformer that literally looks at the entire context at every token has an easier time copying it compared to a system that compresses the input sequence into a fixed-size memory and then uncompresses it back.
1jn11wq,[N] [P] Transformer model made with PHP,yuichiis,11,2025-03-30T00:58:42,"**New Release**

Rindow Neural Networks Version 2.2 has been released.

This release includes samples of transformer models.

We have published **a tutorial on creating transformer models** supported in the new version.

* [Neural Machine Translation with Transformer Models in PHP](https://rindow.github.io/neuralnetworks/tutorials/neural-machine-translation-with-transformer.html)

Rindow Neural Networks is a high-level neural network library for PHP.

It enables powerful machine learning in PHP.

* [Rindow Neural Networks](https://rindow.github.io/neuralnetworks/)

**Overview**

* Rindow Neural Networks is a high-level neural network library for PHP. It enables powerful machine learning in PHP.
* You can build machine learning models such as DNN, CNN, RNN, (multi-head) attention, etc.
* You can leverage your knowledge of Python and Keras.
* Popular computer vision and natural language processing samples are available.
* By calling high-speed calculation libraries, you can process data at speeds comparable to the CPU version of TensorFlow.
* No dedicated machine learning environment is required. It can run on an inexpensive laptop.
* NVIDIA GPU is not required. You can utilize the GPU of your laptop.

**What Rindow Neural Networks is not:**

* It is not an inference-only library.
* It is not a PHP binding for other machine learning frameworks.
* It is not a library for calling AI web services.",14,Why.
1i6os2n,[R] Multivariate Time Series Prediction with Transformers,Chroma-Crash,22,2025-01-21T18:01:49,"I am working on a model that I want to be able to take in a multivariate time series of weather and river height data, and output a series of predictions for one of the river gauge heights (Essentially, I feed in timesteps 20-40 and expect to receive timesteps 41-61). I have previously been using an LSTM for this, but I got pretty subpar results with several different architectures. I'm now looking at using a transformer encoder network, and I have this recurring issue I can't seem to figure out.

For almost any context length, model size, positional encoding, training time, etc.; the model seems to be incapable of distinguishing between timesteps on the outputs. It always learns to predict a good average for the gauge height across the timesteps, but there's no variation in its outputs. On an example case where the target gauge height is \[0.2, 0.3, 0.7, 0.8, 0.6\] it would output something like \[0.4, 0.45, 0.4, 0.45, 0.5\].

In fact, the model performs almost exactly the same without any positional encoding at all.

Here's an example of what an output might look like from several continuous tests:

[Graph showing monotonous predictions regardless of actual position on graph.](https://preview.redd.it/vkcemz918see1.jpg?width=904&amp;format=pjpg&amp;auto=webp&amp;s=3adefcefde09109cce9e3a13ac267689ae667212)

I have tried both relative positional encoding and absolute positional encoding and adjusting the loss function to add a term that focuses on the slope between timesteps, but I can't seem to enforce differentiation between timesteps.

The extra loss term:

    class TemporalDeregularization(nn.Module):
        def __init__(self, epsilon):     
            super().__init__() 
            self.epsilon = epsilon 
            self.mse = nn.MSELoss()
    
        def forward(self, yPred, yTrue):
            predDiff = yPred[:, 1:] - yPred[:, :-1]
            targetDiff = yTrue[:, 1:] - yTrue[:, :-1]
            return self.epsilon * self.mse(predDiff, targetDiff)

My positional encoding scheme:

    class PositionalEncoding(nn.Module):
        def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000, batch_first=False):
            super().__init__()
            self.batch_first = batch_first
            self.dropout = nn.Dropout(p=dropout)
    
            position = torch.arange(max_len).unsqueeze(1)
            div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
            pe = torch.zeros(max_len, 1, d_model)
            pe[:, 0, 0::2] = torch.sin(position * div_term)
            pe[:, 0, 1::2] = torch.cos(position * div_term)
            self.register_buffer('pe', pe)
    
        def forward(self, x: Tensor) -&gt; Tensor:
            if self.batch_first:
                x = x + self.pe[:x.size(1)].permute(1, 0, 2)
            else:
                x = x + self.pe[:x.size(0)]
            return self.dropout(x)

Here's a diagram of my architecture that's more explicit:

[Image containing transformer network architecture, including a linear projection, positional encoding, transformer encoder, and another projection in series.](https://preview.redd.it/emh3s8ozvdee1.png?width=521&amp;format=png&amp;auto=webp&amp;s=ba586df018bdfa653747e98bf03d88a871e0a9ce)

I understand that this isn't exactly a common use case or architecture for this use case, but I'm not sure why the model isn't capable of making the distinction between timesteps. I've considered adding a bidirectional LSTM before the final projection to force time differentiation.

For reference, I have found that this model performs well with a dModel of 64, feedForward of 128, 6 layers, and 8 heads. The other term in the loss function is a standard MSE. Also, I don't apply masking as all of the inputs should be used to calculate the outputs in my case.

I can't post much code as this is related to my job, but I would like to learn more about what is wrong with my approach.

Any help or advice is appreciated, I'm getting my master's currently but I have yet to encounter any machine learning classes despite years of work experience with it, so I may just be missing something. (Also sorry for the dog ass Google drawings)

Edit: Solved! At least for now. The generative approach fixed monotonicity problems, and viewing the problem as a distribution predictor helped with stabilizing generation. For those curious, I changed the model architecture to include a second and separate linear layer for the final outputs to produce a variance score alongside the mean score, and use nn.GaussianNLLLoss for training. Thanks to u/BreakingBalls u/radarsat1 and u/Technical-Seesaw9383",21,"Am I understanding you correctly, that you're using an encoder transformer? May I ask why you're not using a decoder transformer? A decoder does future masking in the attention weights so that past tokens cannot attend to future tokens. That makes it more appropriate for time series predictions.

Also, maybe I'm misunderstanding from your diagram, but it seems like you're inputting tokens 1:n as input and using (n+1):2n as output. Idk if that works or not in theory, but that's not what LLMs do with text tokens. They use tokens 1:n as input and tokens 2:(n+1) as output (just shifting over by 1) so that each token is predicting the following token. If that's not what you're doing, then I would recommend it. But if it is, then I guess I just misunderstood what you're showing."
1izs7c8,[R] Belief State Transformers,RajonRondoIsTurtle,51,2025-02-27T22:19:57,,12,"At this point I've seen so many ""transformers, but better"" papers that went nowhere, that I have no clue how to judge if this is meaningful or interesting."
umq908,"[R] RWKV-v2-RNN : A parallelizable RNN with transformer-level LM performance, and without using attention",bo_peng,376,2022-05-10T19:11:43,"Hi guys. I am an independent researcher and you might know me (BlinkDL) if you are in the EleutherAI discord.

I have built a RNN with transformer-level performance, without using attention. Moreover it supports both sequential &amp; parallel mode in inference and training. So it's combining the best of RNN and transformer - great performance, fast inference, saves VRAM, fast training, ""infinite"" ctx\_len, and free sentence embedding.

[https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM)

I am training a L24-D1024 RWKV-v2-RNN LM (430M params) on the Pile with very promising results:

https://preview.redd.it/xqtkadp5pf191.png?width=946&amp;format=png&amp;auto=webp&amp;s=5fd2f98978dea01e07ded77ed6b5e57b9b7645eb

**All of the trained models will be open-source.** Inference is very fast (only matrix-vector multiplications, no matrix-matrix multiplications) even on CPUs, and **I believe you can run a 1B params RWKV-v2-RNN with reasonable speed on your phone.**

It is inspired by Apple's AFT ([https://arxiv.org/abs/2105.14103](https://arxiv.org/abs/2105.14103)) with a number of my own tricks, such as:

* RNNify it (via a particular nice form of w\_{t, t\^\\prime}), and use my CUDA kernel to speedup training ([https://github.com/BlinkDL/RWKV-CUDA](https://github.com/BlinkDL/RWKV-CUDA))
* Token-shift ([https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing](https://github.com/BlinkDL/RWKV-LM#token-shift-time-shift-mixing))
* SmallInitEmb ([https://github.com/BlinkDL/SmallInitEmb](https://github.com/BlinkDL/SmallInitEmb)) which helps the embedding quality, and stabilizes Post-LN (which is what I am using).

I also transferred some time-related parameters from a small model to a large model, to speed up the convergence. Basically the model learns to focus more on short-distance interactions in early layers, and long-distance interactions in later layers.

https://preview.redd.it/ibk4ic0b6py81.png?width=865&amp;format=png&amp;auto=webp&amp;s=78e4f794abd0fe25c8af8fd6634836a472e4120a

The maths behind RWKV-2:

https://preview.redd.it/j1qg47ypb5691.png?width=662&amp;format=png&amp;auto=webp&amp;s=6cf8eb4ba5f591d807ace347059cf210a6dc1f90

Please feel free to ask questions :)

And let me know if you'd like to test it in other domains (music / speech / protein / ViT / etc.)",54,"*Schmidhuber:* 😏         
Jokes aside, if this field of research finally competes with LMs while being less compute intensive - it could be a game-changer..."
1hnnl6s,[D] The Parallelism Tradeoff: Understanding Transformer Expressivity Through Circuit Complexity,currentscurrents,161,2024-12-27T20:05:15,"Talk: https://www.youtube.com/watch?v=7GVesfXD6_Q

Paper: https://aclanthology.org/2023.tacl-1.31/

TL;DR the author (Will Merrill) looks at transformers from a circuit complexity perspective and places them in the TC^0 complexity class - threshold circuits of constant depth. This is a relatively restricted complexity class that cannot solve many inherently sequential problems.

Their main point is that the expressive limitations of transformers come from their parallel nature, rather details of their architecture. Adding chain of thought allows transformers to solve problems from additional complexity classes, but at the cost of sacrificing parallelism and efficient training.

They suggest that this tradeoff between parallel and sequential computation cannot be avoided, and future architectures should be designed with the tradeoff in mind. They also look at an extension to state space models that makes the tradeoff more efficiently than transformers+CoT.",8,Awesome paper--thanks for sharing.
152u5va,[R] Retentive Network: A Successor to Transformer for Large Language Models,Balance-,171,2023-07-18T10:01:44,"Paper: [**https://arxiv.org/abs/2307.08621**](https://arxiv.org/abs/2307.08621)

# Retentive Network: A Successor to Transformer for Large Language Models

[Yutao Sun](https://arxiv.org/search/cs?searchtype=author&amp;query=Sun%2C+Y), [Li Dong](https://arxiv.org/search/cs?searchtype=author&amp;query=Dong%2C+L), [Shaohan Huang](https://arxiv.org/search/cs?searchtype=author&amp;query=Huang%2C+S), [Shuming Ma](https://arxiv.org/search/cs?searchtype=author&amp;query=Ma%2C+S), [Yuqing Xia](https://arxiv.org/search/cs?searchtype=author&amp;query=Xia%2C+Y), [Jilong Xue](https://arxiv.org/search/cs?searchtype=author&amp;query=Xue%2C+J), [Jianyong Wang](https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+J), [Furu Wei](https://arxiv.org/search/cs?searchtype=author&amp;query=Wei%2C+F)

&gt;In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost O(1) inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at [this https URL](https://aka.ms/retnet).

[ Figure 1: Retentive network \(RetNet\) achieves low-cost inference \(i.e., GPU memory, throughput, and latency\), training parallelism, and favorable scaling curves compared with Transformer. Results of inference cost are reported with 8k as input length. Figure 6 shows more results on different sequence lengths. ](https://preview.redd.it/4brsylxn5pcb1.png?width=2689&amp;format=png&amp;auto=webp&amp;s=1532db7f7e63e9e6d5af5130e4dc0f13f7118503)

[ Figure 2: RetNet makes the “impossible triangle” possible, which achieves training parallelism, good performance, and low inference cost simultaneously ](https://preview.redd.it/b290sjxn5pcb1.png?width=1547&amp;format=png&amp;auto=webp&amp;s=d4487633f154321764bf8ee5739617547dad0f1a)

[ Figure 5: Perplexity decreases along with scaling up the model size. We empirically observe that RetNet tends to outperform Transformer when the model size is larger than 2B. ](https://preview.redd.it/gamd2lxn5pcb1.png?width=960&amp;format=png&amp;auto=webp&amp;s=2db66b779e328dcbb9e52b41b40e7484fcad917e)

[ Figure 6: Inference cost of Transformer and RetNet with a model size of 6.7B. RetNet outperforms Transformers in terms of memory consumption, throughput, and latency ](https://preview.redd.it/e7ukzjxn5pcb1.png?width=1865&amp;format=png&amp;auto=webp&amp;s=5131f1bd8191aab891c48e02582eea87c06741c6)

**GTP 3.5 16k summary (slightly edited):**

&gt;The research paper titled ""Retentive Network: A Successor to Transformer for Large Language Models"" proposes a new architecture called Retentive Network (RetNet) as a successor to the Transformer model for large language models. The paper addresses the limitations of Transformer models in terms of inefficient inference, high memory consumption, and limited scalability.  
&gt;  
&gt;The authors introduce the concept of retention, which combines the benefits of recurrence and parallelism. The retention mechanism supports three computation paradigms: parallel, recurrent, and chunkwise recurrent.  The parallel representation enables training parallelism, the recurrent representation allows for low-cost O(1) inference, and the chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity. The RetNet architecture consists of multi-scale retention modules and feed-forward network modules.  
&gt;  
&gt;The retention mechanism is formulated as a dual form of recurrence and parallelism. It employs content-aware projections to compute contextualized vector representations and utilizes a parallel or recurrent formulation for training and inference. The chunkwise recurrent representation further enhances training efficiency by dividing input sequences into chunks, enabling parallel encoding within each chunk and recurrent encoding across chunks.  
&gt;  
&gt;The authors describe the overall architecture of RetNet, which consists of multiple blocks, each containing a multi-scale retention (MSR) module and a feed-forward network (FFN) module. The MSR module performs the retention operation, while the FFN module handles the feed-forward computation. The architecture is designed to optimize training parallelism, inference efficiency, and memory consumption.  
&gt;  
&gt;The paper compares RetNet with various existing models, including Transformers, Linear Transformers, recurrent neural networks, and other Transformer variants. Experimental results show that RetNet achieves comparable performance to Transformers in language modeling tasks while providing more efficient training and inference. RetNet exhibits favorable scaling properties, parallel training, low-cost deployment, and efficient inference. It outperforms other models in terms of memory consumption, throughput, and latency during inference.  
&gt;  
&gt;The authors also conduct ablation studies to analyze the impact of different components and design choices in RetNet. They demonstrate that the swish gate, GroupNorm, multi-scale decay rates, and larger head dimensions contribute to improved performance.  
&gt;  
&gt;Overall, the paper presents RetNet as a strong successor to Transformer models for large language models. Its retention mechanism combines the benefits of recurrence and parallelism, enabling efficient training and inference while maintaining competitive performance. The proposed architecture addresses the limitations of Transformers and offers advantages in terms of memory consumption, speed, and scalability.

Paper: [**https://arxiv.org/abs/2307.08621**](https://arxiv.org/abs/2307.08621)",54,You can't just proclaim being a successor to transformers lmao
1lgpskb,[D] Batch shuffle in time series transformer,Sufficient_Sir_4730,0,2025-06-21T06:21:07,"Im building a custom time series transformer for stock price prediction, wanted to know if for training dataset batches, Shuffle=True should be done or not? The data within the sample is chronologically arranged, but should I shuffle the samples within the batch or not.

It is a stock market index that im working on, using shuffle true gives more stable training and getting good results. But im worried the regime shift info might be discarded. ",1,"In my opinion, you generally want to avoid shuffling when working with time series forecasting because temporal continuity is a big part of what gives the data meaning. If your training samples are sequential windows taken from a continuous timeline, shuffling them can break the natural order and make it harder for the model to learn trends or transitions like regime shifts. That temporal structure is often what the model needs to capture.

However, if you're using fixed-length windows that are self-contained and don't overlap, and you're confident there's no leakage between them, then shuffling might be fine. In that case, it can help stabilize training and reduce overfitting to local patterns.

Personally, I prefer to keep the training data in chronological order to make sure the model learns in a way that reflects how the data would be used in practice. I usually go with careful windowing, no shuffle during training, and validation on a continuous, ordered slice of the timeline to measure realistic performance."
13pbgef,[R] RWKV: Reinventing RNNs for the Transformer Era,MysteryInc152,276,2023-05-23T02:34:35,Paper - https://arxiv.org/abs/2305.13048,42,"&gt;Transformers have revolutionized almost all natural language processing (NLP) tasks but suffer from memory and computational complexity that scales quadratically with sequence length. In contrast, recurrent neural networks (RNNs) exhibit linear scaling in memory and computational requirements but struggle to match the same performance as Transformers due to limitations in parallelization and scalability. We propose a novel model architecture, Receptance Weighted Key Value (RWKV), that combines the efficient parallelizable training of Transformers with the efficient inference of RNNs. Our approach leverages a linear attention mechanism and allows us to formulate the model as either a Transformer or an RNN, which parallelizes computations during training and maintains constant computational and memory complexity during inference, leading to the first non-transformer architecture to be scaled to tens of billions of parameters. Our experiments reveal that RWKV performs on par with similarly sized Transformers, suggesting that future work can leverage this architecture to create more efficient models. This work presents a significant step towards reconciling the trade-offs between computational efficiency and model performance in sequence processing tasks."
1la59kh,[2506.06105] Text-to-LoRA: Instant Transformer Adaption,51616,10,2025-06-13T02:49:50,,1,"Abstract:

**Text-to-LoRA: Instant Transformer Adaption**

While Foundation Models provide a general tool for rapid content creation, they regularly require task-specific adaptation. Traditionally, this exercise involves careful curation of datasets and repeated fine-tuning of the underlying model. Fine-tuning techniques enable practitioners to adapt foundation models for many new applications but require expensive and lengthy training while being notably sensitive to hyperparameter choices. To overcome these limitations, we introduce Text-to-LoRA (T2L), a model capable of adapting large language models (LLMs) on the fly solely based on a natural language description of the target task. T2L is a hypernetwork trained to construct LoRAs in a single inexpensive forward pass. After training T2L on a suite of 9 pre-trained LoRA adapters (GSM8K, Arc, etc.), we show that the ad-hoc reconstructed LoRA instances match the performance of task-specific adapters across the corresponding test sets. Furthermore, T2L can compress hundreds of LoRA instances and zero-shot generalize to entirely unseen tasks. This approach provides a significant step towards democratizing the specialization of foundation models and enables language-based adaptation with minimal compute requirements.

Code: https://github.com/SakanaAI/text-to-lora"
1ksb9pj,[P] Datatune: Transform data with LLMs using natural language,metalvendetta,7,2025-05-21T22:36:47,"#  

Hey everyone,

At Vitalops, we've been working on a problem many of us face with transforming and filtering data with LLMs without hitting context length limits or insanely high API costs.

We just open-sourced Datatune, which lets you process datasets of any size using natural language instructions. 

Key features:

* Map and Filter operations - transform or filter data with simple prompts
* Support multiple LLM providers (OpenAI, Azure, Ollama for local models) or use your custom class

* Dask DataFrames that support partitioning and parallel processing

Example usage:

    import dask.dataframe as dd
    df =  dd.read_csv('products.csv')
    # Transform data with a simple prompt
    mapped = Map(
        prompt=""Extract categories from the description."",
        output_fields=[""Category"", ""Subcategory""]
    )(llm, df)
    
    # Filter data based on natural language criteria
    filtered = Filter(
        prompt=""Keep only electronics products""
    )(llm, mapped)

We find it especially useful for data cleaning/enrichment tasks that would normally require complex regex or custom code.

Check it out here: [https://github.com/vitalops/datatune](https://github.com/vitalops/datatune)

Would love feedback, especially on performance and API design. What other operations would you find useful?",4,"It's a neat idea but your claims didn't match the source code.

Fundamentally, building a prompt PER ROW of the dataframe and then running inference on it is a strategy that I really got a kick out of. It's funny/creative. But it's not fast, cheap, or scalable. Those claims are overblown.

This is a very small (600 lines, half docstring), fun, hobby grade project. I hope you had fun building it. There's nothing of any commercial value here, though. The basic chat apps will do this more accurately (won't introduce nondeterministic behavior PER ROW) much faster for free with a python interpreter."
19er4pp,[R] Are Vision Transformers More Data Hungry Than Newborn Visual Systems?,currentscurrents,115,2024-01-24T20:59:24,,45,"In my view, I would consider most animal brains to have been heavily ""pretrained"" already at birth through natural selection. Having fostered some cat and puppy litters before, it is wild to see how much of their intelligent behavior seems to be already baked into their minds."
vo2br1,[D] Why are transformers still being used?,DickMan64,311,2022-06-30T07:20:05,"We already have architecture(s) which are supposed to fix one of the biggest issues with transformers, namely that they scale quadratically with input size. The performer scales linearly, which should allow for much bigger context windows, yet looking at recent large language models from major players, all of them seem to be using the old transformer save for some minor improvements. The only exception was Flamingo which had to use a Perceiver because images are huge.

So why haven't we ditched the transformer yet?",55,"Attention cost for large models is dwarfed by matmuls  

People writing 50th version of fancy attention pattern don’t tell you that"
ryw53x,[D] Fourier transform vs NNs as function approximators,Hazalem,354,2022-01-08T09:33:30,"So this is probably a basic question. If the main premise of neural networks is that they are global function approximators, what advantage do they have against other approximators such Fourier transform, which is also proven to be able to approximate any function. Why does not the whole supervised learning field become one of calculating Fourier coefficients",57,"[Fourier series](https://en.wikipedia.org/wiki/Fourier_series) are universal approximators of continuous functions and that's something proven in most analysis courses. (Fast) Fourier transformations can be used to quickly compute Fourier series from uniformly spaced data, though non-uniform FFTs do exist. They have [extremely good properties](https://en.wikipedia.org/wiki/Convergence_of_Fourier_series): on smooth enough models they get spectral convergence, which means the error decreases exponentially (you can see this by the Holder condition on the coefficients). While the Fourier series assumes periodicity, extensions of the model include the [Chebyshev transform](https://en.wikipedia.org/wiki/Discrete_Chebyshev_transform) / [Chebyshev polynomials](https://en.wikipedia.org/wiki/Chebyshev_polynomials) which have similar spectral convergence but on [-1,1] non-periodic functions (you basically just do a cosine transform to the space). 

Neural networks don't converge anything near exponential (it's barely even linear in the best case), so why isn't everything using these methods? Well first of all, if you look at computational science, a lot of things are using pseudospectral methods, spectral elements, etc. Hell, even polynomials are universal approximators to a large set of functions (see the [Stone–Weierstrass theorem](https://en.wikipedia.org/wiki/Stone%E2%80%93Weierstrass_theorem)). So again, why neural networks?

The answer is because all of those universal approximators are one dimensional (there are some specifically designed ones for low dimensions too, such as spherical harmonics, but they are for very specific cases). You can make a one-dimensional universal approximator into a multi-dimensional via tensor products, but if you write it out you'll see what happens. 

`a0 + a1*sin(x) + b1*cos(x) + a2*sin(2x) + b2*cos(2x) + ...` 

is one dimension, so then in two dimensions you have 

`a0 + a1*sin(x) + b1*cos(x) + c1*sin(y) + d1*cos(y) + a2*sin(2x) + b2*cos(2x) + c2*sin(2y) + d2*cos(2y) + e2*sin(x)*cos(y) + ...` 

So the expansion is ""one in the x, one in the y, two in the x, two in the y, one in both, three in ..."". So as you go to higher dimensions, you have to add terms for every combination of the higher order terms. [Combinations grow factorially or approximately exponentially](https://www.mathsisfun.com/combinatorics/combinations-permutations.html). You're talking about [161,700 terms](https://www.wolframalpha.com/input/?i=100+choose+3) to represent just the cross terms of the third order of an expansion of a 100 dimensional input! Fully representing large images with thousands of pixels will never happen with this approximator. 

This exponential growth with respect to the input size is what is referred to as the ""curse of dimensionality"". Methods which ""overcome the curse of dimensionality"" are methods which do not demonstrate exponential cost growth in memory and compute time with respect to growing input sizes. Neural networks have empirically demonstrated polynomial cost growth with input size ([and some theoretical results as well](https://arxiv.org/abs/1809.07321) or [others](https://cbmm.mit.edu/sites/default/files/publications/02_761-774_00966_Bpast.No_.66-6_28.12.18_K1.pdf)), and that is why they are used for these ""big data"" problems.

But doesn't that mean that Fourier series can be better for sufficiently small and smooth problems? Oh yes! That's why Physics-Informed Neural Networks and Fourier Neural Operators are not competitive against good PDE solvers in 3-dimensional cases (how papers can say the opposite is a long story that will be elucidated later). In fact, in [this paper we showcase how mixing a CNN + a universal approximator in a specific way into ODEs (universal differential equations) can be used to automatically discover PDE discretizations](https://arxiv.org/abs/2001.04385), and we show in that paper that a Fourier universal approximator works better than a neural network for that specific case. [DiffEqFlux.jl includes classical basis layers](https://diffeqflux.sciml.ai/dev/layers/BasisLayers/) and [tensor product tools](https://diffeqflux.sciml.ai/dev/layers/TensorLayer/) for this reason. That said, they have to be used in the right context. Remember, spectral convergence requires that the function being approximated is smooth, and when that's violated you can still get convergence but it's slow. The issues other posts refer to as ""global"" vs ""local"" properties are mostly manifestations of this issue, as the mentioned ""ringing"" is a property of the [Gibbs phonomenon](https://en.wikipedia.org/wiki/Gibbs_phenomenon) which only exists when the approximated function has discontinuities. 

Neural networks are a tool. Fourier series are a tool. Chebyshev series are a tool. Etc. When they are used in ways that match their theoretical properties you can improve your performance. And as a parting note, what the the properties for polynomial expansions you should be aware of? Legendre polynomials? Sparse grids? Radial basis functions? What are the pros and cons vs neural networks? I believe every practitioner of machine learning should know the answer to those questions.

[Edit: in a re-read I realized I forgot to mention why global and Gibbs phenomenon are related. It's because they are both the assumption of smoothness. If you assume a function is smooth, then every point influences everywhere else in the domain. You can think about this by looking at the [convergence of Taylor series](https://en.wikipedia.org/wiki/Taylor_series#/media/File:Sintay_SVG.svg), where as you get more and more derivatives correct the approximation is ""close"" to the original function for longer and longer. When you assume infinitely many derivatives, then the effect of each piece of data is effectively global. This is no longer true when you have a discontinuity, and so the Gibbs phenomenon is a kind of aberration introduced near the point where this assumption is broken. That's a very high level description but you can follow it into the spectral analysis because it's where the error bounds need to make the smoothness assumptions.]"
1c1l16l,[R] Infinite context Transformers,Dyoakom,113,2024-04-11T17:35:05,"I took a look and didn't see any discussion thread here on this paper which looks perhaps promising.  


[https://arxiv.org/abs/2404.07143](https://arxiv.org/abs/2404.07143)  


What are your thoughts? Could it be one of the techniques behind the Gemini 1.5 reported 10m token context length? ",36,"Any know how it compares to Based attention(linear Taylor series approximation of exp(QK^T) + full sliding attention) released recently that also promised higher quality recall for longer context than mamba and without any drawbacks as it still has full attention for recent tokens. 

https://assets-global.website-files.com/650c3b59079d92475f37b68f/65e557e1f75f13323664294a_blogpost-01.png

 https://www.together.ai/blog/based"
1fyb9jj,[P] Model2Vec: Distill a Small Fast Model from any Sentence Transformer,Pringled101,83,2024-10-07T16:02:31,"Hey 👋!

I wanted to share a project we've been working on for the past couple of months called [Model2Vec](https://github.com/MinishLab/model2vec) that we recently open-sourced. It's a technique to distill Sentence Transformer models and create very small static embedding models (30mb on disk) that are up to 500x faster than the original model, making them very easy to use on CPU. Distillation takes about 30 seconds on a CPU.

These embeddings outperform similar methods such as GloVE and BPEmb by a large margin on MTEB while being much faster to create, and no dataset is needed. It's designed as an eco-friendly alternative to (Large) Language Models and particularly useful for situations where you are time-constrained (e.g. search engines), or don't have access to fancy hardware.

The idea is pretty straightforward, but works surprisingly well:

1: Take the token output embeddings of any Sentence Transformer.

2: Reduce the dimensionality using PCA. This reduces the model size, but also normalizes the output space.

3: Apply zipf weighting to the embeddings based on the word/token frequencies. This essentially downweights frequent words, meaning you don't need to remove stopwords for example.

We've created a couple of easy to use methods that can be used after installing the package with `pip install model2vec`:

**Inference:**

    from model2vec import StaticModel
    
    # Load a model from the HuggingFace hub (in this case the M2V_base_output model)
    model_name = ""minishlab/M2V_base_output""
    model = StaticModel.from_pretrained(model_name)
    
    # Make embeddings
    embeddings = model.encode([""It's dangerous to go alone!"", ""It's a secret to everybody.""])

**Distillation:**

    from model2vec.distill import distill
    
    # Choose a Sentence Transformer model
    model_name = ""BAAI/bge-base-en-v1.5""
    
    # Distill the model
    m2v_model = distill(model_name=model_name, pca_dims=256)
    
    # Save the model
    m2v_model.save_pretrained(""m2v_model"")

I'm curious to hear your thoughts on this, and happy to answer any questions!

Links:

* [Repo link](https://github.com/MinishLab/model2vec)
* [Results link](https://github.com/MinishLab/model2vec?tab=readme-ov-file#results)",21,Could this work for other embedding models? Like image embeddings?
1kz2lqb,[R] A transformer inspired architecture capable of  imagination and higher-level human mental states,Leading_Health2642,0,2025-05-30T12:10:08,"What are your comments on this? imo this can change the whole AI industry.  
**Abstract:** *Attending to what is relevant is fundamental to both the mammalian brain and modern machine learning models such as Transformers. Yet, determining relevance remains a core challenge, traditionally offloaded to learning algorithms like backpropagation. Inspired by recent cellular neurobiological evidence linking neocortical pyramidal cells to distinct mental states, this work shows how models (e.g., Transformers) can emulate high-level perceptual processing and awake thought (imagination) states to pre-select relevant information before applying attention. Triadic neuronal-level modulation loops among questions (Q), clues (keys, K), and hypotheses (values, V) enable diverse, deep, parallel reasoning chains at the representation level and allow a rapid shift from initial biases to refined understanding. This leads to orders-of-magnitude faster learning with significantly reduced computational demand (e.g., fewer heads, layers, and tokens), at an approximate cost of \\mathcal{O}(N), where N is the number of input tokens. Results span reinforcement learning (e.g., CarRacing in a high-dimensional visual setup), computer vision, and natural language question answering.*",2,"The results have nothing to do with  mental states, let alone of higher order"
17hy6lj,[R] ConvNets Match Vision Transformers at Scale,psyyduck,162,2023-10-27T21:36:16,"**PAPER**: https://arxiv.org/abs/2310.16764

**SUMMARY**

The paper ""ConvNets Match Vision Transformers at Scale"" from Google DeepMind aims to debunk the prevalent notion that Vision Transformers (ViTs) are inherently superior to ConvNets for large-scale image classification. Using the NFNet model family as a representative ConvNet architecture, the authors pre-train various models on the extensive JFT-4B dataset under different compute budgets, ranging from 0.4k to 110k TPU-v4 core hours. Through this empirical analysis, they observe a log-log scaling law between held-out loss and compute budget. Importantly, when these NFNets are fine-tuned on ImageNet, they match the performance metrics of ViTs trained under comparable computational constraints. Their most resource-intensive model even achieves a Top-1 ImageNet accuracy of 90.4%.

The crux of the paper's argument is that the supposed performance gap between ConvNets and ViTs largely vanishes under a fair comparison, which accounts for compute and data scale. In other words, the efficacy of a machine learning model in large-scale image classification is more dependent on the available data and computational resources than on the choice between ConvNet and Vision Transformer architectures. This challenges the community's leaning towards ViTs and emphasizes the importance of equitable benchmarking when evaluating different neural network architectures.",42,"[The “it” in AI models is the dataset.](https://nonint.com/2023/06/10/the-it-in-ai-models-is-the-dataset/)

&gt;... trained on the same dataset for long enough, pretty much every model with enough weights and training time converges to the same point. Sufficiently large diffusion conv-unets produce the same images as ViT generators. AR sampling produces the same images as diffusion."
1l3cws1,[P] Metadata-Augmented Transformers: Early Results &amp; Call for Collaboration,FaithlessnessEast838,0,2025-06-04T18:19:56,"Transformers typically process sequences of plain tokens. We're exploring **metadata augmentation** to create semantically richer and more structured contexts. We introduce a **Metadata-Enhanced Transformer** that layers metadata on top of raw data. Early experiments show that this augmentation:

* Accelerates training convergence
* Lowers training loss
* Improves generalization
* Amplifies scaling benefits

Code, datasets, and test results: [GitHub – Metadata\_Enhanced\_Transformer](https://github.com/iliaMalinovskii/Metadata_Enhanced_Transformer)

This is a work in progress, and I’m looking for both feedback and collaborators interested in joint research.

Would love to hear your thoughts. Happy to dive deeper in replies or DMs.",1,DM’d
1i40viz,[D] Dynamic Neuron-Controller-Based Transformer Architecture: Feedback Wanted,bunny5544,13,2025-01-18T05:45:20,"**Dynamic Neuron-Controller-Based Transformer Architecture by Shanmukh Ram**

# Abstract

This white paper presents an innovative architecture that integrates dynamic neuron-controller systems with transformer models to create a continuously adaptive and resource-efficient AI framework. The proposed architecture utilizes neuron or batch controllers to dynamically adjust the weights and operations of a shared transformer architecture in real time.

By responding to signals generated by individual or grouped neurons, the system continuously adapts to changing demands. This adaptability enables efficient multi-tasking and optimizes resource sharing, ensuring high performance across diverse contexts. These features establish the architecture as a groundbreaking innovation in AI, unlocking advancements in applications such as general intelligence, personalized systems, and multi-agent collaboration.

# 1. Introduction

# 1.1 Background

Transformer architectures have revolutionized natural language processing and other domains, owing to their scalability, attention mechanisms, and ability to model long-range dependencies. However, transformers remain largely static post-training, with fine-tuning or retraining required to adapt to new tasks or shifting environments.

# 1.2 Motivation

Real-world applications often involve dynamic and unpredictable environments. Traditional transformer models, though powerful, are inefficient in adapting to real-time changes without significant retraining. This gap motivates the design of a system where neurons act as adaptive controllers, dynamically modifying the transformer’s behavior to optimize performance across varying tasks and inputs.

# 2. Proposed Architecture

# 2.1 Core Components

The architecture consists of the following core components:

1. **Neuron-Controllers**:
   * Independent neurons or batches of neurons act as dynamic agents within the system, controlling and optimizing the transformer’s performance. These controllers receive input signals from various sources, including real-time environmental data, user feedback, or task-specific objectives. Upon processing these inputs, the controllers generate precise control signals to dynamically modify transformer parameters such as attention weights, layer activations, or embeddings. For instance, in a natural language processing task, the controllers might adjust attention weights to focus on critical phrases in a document, ensuring more accurate summarization. Similarly, in image recognition tasks, layer activations could be optimized to emphasize edges or textures, improving classification accuracy.
   * These targeted adjustments significantly enhance the system’s ability to adapt to diverse tasks while maintaining high performance and efficiency. This dynamic adjustment ensures the system remains highly adaptive, continuously optimizing its responses to suit specific tasks or contexts.
2. **Shared Transformer Framework**:
   * A modular transformer architecture forms the backbone of the system, meticulously crafted to support real-time adjustments to its operational parameters. This modularity allows each core component, such as attention heads, transformer layers, or embeddings to be dynamically reconfigured based on control signals generated by neuron-controller batches. By enabling real-time adaptability, the system ensures that computational resources can be scaled efficiently or concentrated on specific areas of importance, depending on the complexity and requirements of the task. For instance, attention heads may be activated selectively for high-priority inputs, while layers or embeddings can be modified dynamically to fine-tune task-specific outputs. This approach not only enhances scalability but also optimizes performance, making the architecture capable of handling both simple and complex tasks with remarkable efficiency.
3. **Feedback Loop**:
   * The architecture integrates a continuous feedback mechanism wherein the transformer's outputs are systematically analyzed and fed back to the neuron-controllers. This iterative process allows the neuron-controllers to refine their strategies based on real-time performance metrics and contextual outcomes. By dynamically adjusting control parameters, the system ensures alignment with evolving task objectives and operational efficiency. This feedback loop not only enhances adaptability but also fosters a robust learning environment where both controllers and the transformer progressively improve in tandem.
   * This loop refines the controllers’ strategies in real time, ensuring constant performance improvement and alignment with task objectives.
   * By iteratively optimizing both the controllers and the transformer, the system achieves a closed-loop learning environment.
4. **Coordinator Mechanism**:
   * A centralized or decentralized coordinator mechanism is designed to ensure seamless interactions among multiple neuron-controller batches. This mechanism prioritizes resource allocation and balances task assignments, mitigating potential conflicts that may arise when neuron batches manage separate transformers or collaborate on shared tasks. By enabling effective coordination, the architecture prevents inefficiencies and ensures that all tasks are executed optimally, maintaining synergy across the entire system.

# 2.2 Key Features

1. **Dynamic Weight Adjustment**:

Dynamic weight adjustment represents the core capability of the system where controllers fine-tune specific transformer weights in real time. These adjustments are informed by contextual signals, which include environmental data, user feedback, and task-specific objectives. For example, in autonomous driving, the controllers can adjust attention weights to prioritize critical inputs like pedestrian detection over less immediate data, such as road signage in clear weather. In healthcare applications, layer activations might be fine-tuned dynamically to focus on anomalies in medical imaging, ensuring accurate diagnostics. When an input signal is received, the neuron-controllers analyze it and generate precise commands to recalibrate the transformer's internal parameters, such as attention weights or activation thresholds. This process ensures that the architecture adapts seamlessly to the demands of diverse tasks and dynamic environments. The ability to perform these real-time optimizations not only enhances task-specific performance but also maximizes resource efficiency, as only the necessary components of the transformer are engaged at any given time. This dynamic adaptability is crucial for handling complex, real-world scenarios where static models would fail to perform optimally, thereby positioning this system as a significant advancement in AI adaptability and responsiveness.

1. **Batch-Based Control**:
   * Groups of neurons manage different tasks or modules, each acting as specialized agents to oversee specific functionalities within the system. This allows simultaneous optimization across multiple frameworks by dynamically distributing computational resources and responsibilities. For example, one group of neurons may control language modeling tasks while another focuses on vision-based analysis, enabling these processes to run concurrently without interfering with each other. This approach enhances efficiency and ensures that the transformer system remains scalable and adaptable, bringing the value of multitasking without compromising performance.
2. **Task-Specific Adaptation**:
   * Each neuron batch can specialize in controlling a subset of the transformer for task-specific performance by dynamically focusing on the specific layers, attention mechanisms, or embeddings that are most relevant to the task. For example, in a multi-task learning setup, one neuron batch could fine-tune the transformer’s attention weights for language modeling, while another batch might adjust embedding layers for visual data processing. This specialization ensures that the system can effectively handle diverse tasks in parallel without sacrificing efficiency or performance. By leveraging this dynamic specialization, the architecture optimizes resource utilization, minimizes interference between tasks, and enhances the accuracy and responsiveness of each transformer subset to its assigned task.
3. **Multi-Agent Collaboration**:
   * Neuron batches play a pivotal role in enhancing the system's overall performance by engaging in collaborative or competitive dynamics tailored to complex, multi-dimensional tasks. For example, in a multi-modal AI system, one neuron batch could specialize in processing textual data, while another focuses on visual inputs. Collaboration between these batches ensures that insights from both modalities are integrated effectively, leading to more accurate and coherent outcomes, such as in video summarization or multimedia content analysis. Similarly, competition among neuron batches could prioritize critical tasks, ensuring time-sensitive objectives like anomaly detection in real-time surveillance are addressed promptly. These batches act as specialized agents, dynamically adjusting their behaviors to maximize task outcomes based on the broader system’s objectives. For instance, collaboration between neuron batches may involve sharing insights or control signals to optimize resource allocation across different sections of the transformer. In contrast, competitive dynamics could arise in scenarios where distinct neuron batches vie to prioritize their assigned tasks, ensuring critical objectives receive adequate focus.
   * By allowing both collaboration and competition, the architecture fosters a balance between efficiency and task-specific precision. This mechanism integrates seamlessly with the feedback and coordination systems, ensuring that neuron batches remain aligned with the overarching goals of the system while dynamically optimizing their strategies. The value of this approach lies in its ability to handle multi-tasking demands with enhanced adaptability and responsiveness, making it an essential component of the architecture's design.

# 3. Implementation

# 3.1 Input Signals

Neuron-controllers process a variety of inputs, such as:

* **Environmental Data**: Real-time data streams from external sensors or APIs.
* **Feedback Signals**: Outputs from transformers or user interaction data.
* **Predefined Objectives**: Task-specific goals encoded during training.

# 3.2 Dynamic Controllers

Neuron-controllers utilize advanced reinforcement learning (RL) techniques and optimization algorithms to determine the most effective adjustments for the transformer. These adjustments include recalibrating attention weights to focus on the most relevant features of the input, selectively activating or deactivating layers to optimize computational efficiency, and dynamically modifying positional encodings or embeddings to enhance the transformer's contextual understanding. By analyzing input signals and system feedback in real-time, neuron-controllers ensure that the architecture remains highly adaptive and aligned with task-specific objectives, enabling superior performance across diverse and complex tasks.

# 3.3 Transformer Modularity

The transformer is designed with modularity in mind:

* **Adapters**: Lightweight modules inserted into transformer layers to enable task-specific adjustments.
* **Sparse Activation**: Only parts of the transformer are activated based on control signals.
* **Mixture of Experts (MoE)**: Controllers determine which expert modules to activate for a given input.

# 3.4 Feedback Mechanism

A feedback loop evaluates the transformer’s output and updates the neuron-controllers’ strategies, creating a continuous learning environment.

# 4. Applications

# 4.1 Multi-Task Learning

Dynamic controllers empower a single transformer architecture to manage multiple tasks simultaneously by dynamically redistributing resources to optimize for each task's specific requirements. These controllers act as task-specialized agents, analyzing the contextual demands of each input and directing computational focus to the most relevant sections of the transformer such as attention heads, embeddings, or specific layers. For example, when handling a combination of natural language processing and vision-based tasks, the dynamic controllers can assign priority resources to textual embeddings for language inputs while activating vision-specific modules for image data.

This simultaneous multi-task optimization ensures that each task benefits from the transformer's shared architecture without compromising performance. The ability to dynamically allocate resources not only reduces computational redundancy but also enhances scalability, allowing the system to adapt seamlessly to complex, real-world scenarios. By maintaining task-specific precision while sharing computational infrastructure, this architecture represents a significant step forward in creating efficient and robust AI systems capable of managing diverse workloads.

# 4.2 Personalized Systems

Dynamic controllers allow the transformer to adapt its behavior to individual users or specific contexts, enabling highly tailored and responsive applications. By analyzing real-time user data, such as preferences, historical interactions, or contextual inputs, these controllers dynamically modify the transformer's parameters to deliver personalized outputs. For example, in a virtual assistant application, the controller might adjust the transformer's attention mechanisms to prioritize the user's current needs or focus on topics of interest based on prior interactions. This capability ensures that the system evolves alongside the user, providing a more engaging and effective experience. The ability to personalize outputs in real-time is critical for applications in education, healthcare, and customer service, where individualized solutions add significant value.

# 4.3 Collaborative AI

Neuron-controller batches enhance the system's ability to handle complex, multi-dimensional problems by fostering collaboration among multiple transformers. For instance, in a multi-modal AI system integrating text, images, and audio, one batch of neuron-controllers could process and extract key textual information, another batch could analyze visual data, and a third could handle audio signals. Collaboration ensures that insights from each modality are synthesized into a unified understanding, significantly improving outcomes such as multimedia content analysis or real-time event summarization.

This collaborative potential enables the system to leverage diverse data types effectively, ensuring comprehensive and accurate results. These controllers dynamically allocate resources and share insights between transformers, enabling them to work together seamlessly. For instance, in multi-modal AI applications that integrate text, images, and audio, one transformer might specialize in processing textual data while another focuses on visual analysis.

Through real-time communication and coordination, the system ensures that insights from each modality contribute to a cohesive and accurate result. This collaborative approach not only improves task performance but also enables the system to tackle problems that require integrated knowledge from multiple domains.

# 4.4 General Intelligence

The architecture's dynamic adaptability, real-time resource allocation, and collaborative mechanisms represent a significant step toward achieving general artificial intelligence. By allowing neuron-controller batches to manage diverse tasks and contexts dynamically, the system creates a foundation for cross-domain learning and decision-making. Unlike traditional AI systems that require retraining for new tasks, this architecture can rapidly adapt to novel scenarios, demonstrating a level of flexibility and generalization that closely mirrors human intelligence. The ability to integrate knowledge across tasks and respond effectively to unforeseen challenges positions this architecture as a cornerstone in the pursuit of general AI.

# 5. Societal Impacts

# 5.1 Positive Outcomes

* **Efficiency**: Reduced computational costs through dynamic resource sharing.
* **Adaptability**: Better handling of real-world variability and user-specific needs.
* **Innovation**: New AI applications and use cases become feasible.

# 5.2 Risks

* **Unpredictability**: Dynamic systems may produce unforeseen behaviors.
* **Security**: Systems must be robust against adversarial inputs or misuse.
* **Ethical Concerns**: Continuous learning raises questions about accountability and transparency.

# 6. Future Directions

The dynamic neuron-controller-based transformer architecture opens up several avenues for research and practical advancements. The focus must be on refining the foundational mechanisms to further enhance scalability, adaptability, and safety.

# 6.1 Enhancing Controller Intelligence

Research should prioritize the development of neuron-controllers capable of understanding higher-level abstractions, contextual nuances, and complex task hierarchies. By integrating advanced algorithms such as meta-learning and neural architecture search, these controllers can evolve into highly intelligent agents that adapt seamlessly to diverse and unforeseen challenges. This advancement will make the system more robust in managing a wider array of applications.

# 6.2 Scaling to Larger Architectures

Efforts must be directed toward designing and managing larger systems that integrate multiple controllers and transformers. However, scaling such architectures presents significant challenges, including increased computational overhead, potential bottlenecks in communication between controllers, and the risk of degraded performance in highly complex systems. Addressing these limitations is crucial to unlock the full potential of this approach and ensure seamless scalability in real-world applications. Techniques such as distributed computing, modular design, and sparse activations will be critical to maintain performance and efficiency at scale. This scaling capability will empower the architecture to handle increasingly complex tasks across industries, from healthcare diagnostics to autonomous systems.

# 6.3 Safety and Robustness

Ensuring the safety and reliability of dynamically adaptive systems is paramount. Specific strategies to achieve this include the integration of robust adversarial defense mechanisms to counter malicious inputs, the development of fail-safe protocols to handle unexpected failures, and the implementation of comprehensive ethical oversight frameworks. Additionally, employing techniques such as explainability in AI and real-time monitoring systems can ensure transparency and accountability, further reinforcing the trustworthiness of these architectures. This requires the implementation of fail-safes, ethical oversight mechanisms, and robust adversarial defenses.

By addressing these concerns, the architecture can operate confidently in critical applications, including finance, defense, and public safety. For example, in finance, the system could dynamically adapt to market changes by prioritizing critical data streams for fraud detection or risk assessment. In defense, collaborative neuron-controller batches could integrate intelligence from multiple data modalities such as satellite imagery, intercepted communications, and real-time ground reports to provide actionable insights for decision-makers. Similarly, in public safety, the architecture could manage resources dynamically during emergencies, such as optimizing response times for disaster management or ensuring accurate predictions for crowd control. Safety-focused research will also ensure that the system remains compliant with evolving regulations and ethical standards.

# 8. Conclusion

The proposed dynamic neuron-controller-based transformer architecture represents a paradigm shift in AI development. By enabling real-time adaptability, efficient resource sharing, and multi-tasking capabilities, this system has the potential to revolutionize AI applications across industries. While challenges remain, the opportunities for innovation and societal benefit are immense, making this a promising direction for future research and development.",17,"It reads like something that won't answer questions arising from further reading.


I got the feeling I won't find any technical detail, which gives the feeling that some algorithms have been put to code and developed a lot, maybe with testing the lack of coding error, as in ""instantiate a network and a random tensor and do a forward pass on the tensor"", but without any actual training on actual tasks with actual data.


This is bad, especially if training was actually done; since it's the most important part, it's very bad not being able to convey that the most important part has been done."
175ep6x,[R] Tsinghua University: Inverting Transformers Significantly Improves Time Series Forecasting,Successful-Western27,169,2023-10-11T14:00:19,"Transformers are great at NLP and computer vision tasks, but I was surprised to learn they still lag behind simple linear models at time series forecasting.

The issue is how most Transformer architectures treat each timestamp as a token and fuse all the variable data from that moment. This makes two big problems:

* **Variables recorded at slightly different times get blurred together,** losing important timing info
* **Each token can only see a single moment,** no long-term dependencies

So Transformers struggle to extract useful patterns and correlations from the data.

Some researchers from Tsinghua University took a fresh look at this and realized the Transformer components themselves are solid, they just need to flip the architecture for time series data.

Their ""Inverted Transformer"" (or iTransformer):

* **Makes each variable's full history into a token,** instead of each timestamp
* Uses self-attention over variables to capture relationships
* Processes time dependencies per variable with feedforward layers

This simple tweak gives all the benefits we want:

* State-of-the-art forecasting accuracy, beating both linear models and standard Transformers
* Better generalization to unseen variables
* Increased interpretability
* Ability to leverage longer historical context

**TLDR: Inverting Transformers to align with time series structure allows them to outperform alternatives in working with time series data.**

[Full summary](https://open.substack.com/pub/aimodels/p/tsinghua-university-inverting-transformers?r=2apyaf&amp;utm_campaign=post&amp;utm_medium=web). Paper is [here](https://arxiv.org/pdf/2310.06625.pdf). EDIT: One of the authors commented below!",41,How is that inverted? Transposed more like it. And why not 2d attention over both time and feature dimensions?
1jurarc,[P] Reducing Transformer Training Time Without Sacrificing Accuracy — A Dynamic Architecture Update Approach,suparshwa1,8,2025-04-08T23:06:39,"Hey everyone!

I’ve been working on a research project focused on optimizing transformer models to **reduce training time without compromising accuracy**. 🚀

Through this work, I developed a novel method where the model **dynamically updates its architecture during training**, allowing it to converge faster while still maintaining performance. Think of it like adaptive scaling, but smarter — we’re not just reducing size arbitrarily, we're making informed structural updates *on the fly*.

I recently published a Medium article explaining one part of the approach: **how I managed to keep the model’s accuracy stable even after reducing the training time**. If you're interested in the technical details or just want to nerd out on optimization strategies, I'd love for you to check it out!

🔗 **Medium article**: [https://medium.com/@patil311299/my-journey-with-dynamic-transformers-parallel-encoders-in-action-e7449c3d7ccf](https://medium.com/@patil311299/my-journey-with-dynamic-transformers-parallel-encoders-in-action-e7449c3d7ccf)  
🔗 **GitHub repo**: [https://github.com/suparshwa31/Dynamic\_Transformer](https://github.com/suparshwa31/Dynamic_Transformer)

Would love feedback, ideas, or even collaborators — feel free to open a PR or drop your thoughts. Always happy to discuss!",7,"When I tried to access the [medium.com](https://medium.com/me/stats/post/e7449c3d7ccf) link, I got

&gt;Error 403   
You don’t have access to this page."
1d7a6l6,[R] Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality,floppy_llama,134,2024-06-03T17:30:21,,25,New “[insert trendy thing] is just [insert another trendy thing]” paper just dropped
1kwfusu,[D] Audio Spectrogram Transformer,_ajing,1,2025-05-27T05:54:30,Hi. Does the model Audio Spectrogram Transformer (AST) automatically generate a spectrogram? or do i still need to generate it beforehand using methods like STFT then input it on the AST model?,1,The model itself needs a spectrogram. Depending on if you are loading up the model yourself or if you are using someone else's code (which would likely have the preprocessing implemented) will depend on if you need to preprocess the spectrogram or not
1l3837k,[D] Latest Work in Transformation-based Models?,ChiliPepperHott,0,2025-06-04T15:12:13,"It seems like there was a short period of time in the '90s where transformation-based models (like those from Eric Brill) were state-of-the-art. What's happened since then?

Since they're so human-readable, I would imagine they are quite good for non-generative, classification tasks.",0,
z7rabn,[r] The Singular Value Decompositions of Transformer Weight Matrices are Highly Interpretable - LessWrong,visarga,299,2022-11-29T11:20:56,"https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight

&gt; If we take the SVD of the weight matrices of the OV circuit and of MLP layers of GPT models, and project them to token embedding space, we notice this results in highly interpretable semantic clusters. This means that the network learns to align the principal directions of each MLP weight matrix or attention head to read from or write to semantically interpretable directions in the residual stream.

&gt; We can use this to both improve our understanding of transformer language models and edit their representations. We use this finding to design both a natural language query locator, where you can write a set of natural language concepts and find all weight directions in the network which correspond to it, and also to edit the network's representations by deleting specific singular vectors, which results in relatively large effects on the logits related to the semantics of that vector and relatively small effects on semantically different clusters

Looks like a thoughtful article and it has nice visuals.",43,"This is very interesting, if somewhat dense and hard to follow if you don't have some of the background.

I recommend reading an article they reference:  A Mathematical Framework for Transformer Circuits [https://transformer-circuits.pub/2021/framework/index.html](https://transformer-circuits.pub/2021/framework/index.html)  

If nothing else, that paper will explain that OV means output-value:

&gt;Attention heads can be understood as having two largely independent   
computations: a QK (“query-key”) circuit which computes the attention   
pattern, and an OV (“output-value”) circuit which computes how each   
token affects the output if attended to."
1iicsz0,[R] Transformer-Squared: Self-adaptive LLMs,Jind0sh,45,2025-02-05T15:39:02,"A framework by Sakana AI that allows LLMs to **adjust some of their weights at inference.**

[Paper](https://arxiv.org/html/2501.06252v3) | [GitHub](https://github.com/SakanaAI/self-adaptive-llms) | [Blog Summary](https://sakana.ai/transformer-squared/)

https://preview.redd.it/61pd7me6jche1.png?width=915&amp;format=png&amp;auto=webp&amp;s=b223fcb9369dc461c0b933669b1026f5eb46d351

Abstract:

&gt;""Self-adaptive large language models (LLMs) aim to solve the challenges posed by traditional fine-tuning methods, which are often computationally intensive and static in their ability to handle diverse tasks. We introduce Transformer-Squared, a novel self-adaptation framework that adapts LLMs for unseen tasks in real-time by selectively adjusting only the singular components of their weight matrices. During inference, Transformer-Squared employs a two-pass mechanism: first, a dispatch system identifies the task properties, and then task-specific 'expert' vectors, trained using reinforcement learning, are dynamically mixed to obtain targeted behavior for the incoming prompt. Our method consistently outperforms ubiquitous approaches such as LoRA, with fewer parameters and greater efficiency. Furthermore, Transformer-Squared demonstrates versatility across different LLM architectures and modalities, including vision-language tasks. Transformer-Squared represents a significant leap forward, offering a scalable, efficient solution for enhancing the adaptability and task-specific performance of LLMs, paving the way for truly dynamic, self-organizing AI systems.""

https://preview.redd.it/w5tey3kebche1.png?width=907&amp;format=png&amp;auto=webp&amp;s=15550138bac56f881d8981d5e45022c4cbf6c278

https://preview.redd.it/nb3rdwagbche1.png?width=962&amp;format=png&amp;auto=webp&amp;s=df98f74dea04365eefba9bf4004ba1c3c50a3359

Conclusion:

&gt;In this paper, we introduced Transformer2, providing a novel blueprint toward realizing self-adaptive LLMs. Within this framework, we first proposed SVF, offering superior performance than prior fine-tuning recipes, together with reduced costs, high compositionality, and overfitting regularization – all crucial properties to achieve scalable self-adaptation. Leveraging a set of SVF experts as building blocks, we developed three effective strategies for self-adaptation, each offering unique benefits and monotonic performance benefits with increasing access to the test-time conditions.

&gt;While Transformer2 demonstrates promising results, there remain exciting opportunities for future work. One limitation is that the capabilities of SVF experts are tied to the latent components of the base model. To address this, model merging offers a promising direction (Yu et al., [2024](https://arxiv.org/html/2501.06252v3#bib.bib44); Goddard et al., [2024](https://arxiv.org/html/2501.06252v3#bib.bib12); Akiba et al., [2024](https://arxiv.org/html/2501.06252v3#bib.bib1)), enabling specialized models to be combined into a single, more capable model. Additionally, while our CEM-based adaptation effectively balances performance and efficiency, scaling to a large number of specialized domains may introduce increased one-time computational costs. However, this trade-off is offset by the benefits of improved performance and enhanced self-adaptation capabilities. Advances in model merging and efficient adaptation techniques have produced models dominating open leaderboards, making them strong candidates as base models for Transformer2 and opening new possibilities for adaptive LLMs.",9,"oh cool, hypernetworks are back"
1fec2jq,[D] Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise,Commercial_Carrot460,25,2024-09-11T14:52:01,"Hi everyone, 

The point of this post is not to blame the authors, I'm just very surprised by the review process.

I just stumbled upon this paper. While I find the ideas somewhat interesting, I found the overall results and justifications to be very weak.   
It was a clear reject from ICLR2022, mainly for a lack of any theoretical justifications. [https://openreview.net/forum?id=slHNW9yRie0](https://openreview.net/forum?id=slHNW9yRie0)  
The exact same paper is resubmitted at NeurIPS2023 and I kid you not, the thing is accepted for a poster. [https://openreview.net/forum?id=XH3ArccntI](https://openreview.net/forum?id=XH3ArccntI)

I don't really get how it could have made it through the review process of NeurIPS. The whole thing is very preliminary and is basically just consisting of experiments.  
It even llack citations of other very closely related work such as *Generative Modelling With Inverse Heat Dissipation* [https://arxiv.org/abs/2206.13397](https://arxiv.org/abs/2206.13397) which is basically their ""blurring diffusion"" but with theoretical background and better results (which was accepted to ICLR2023)...

I thought NeurIPS was on the same level as ICLR, but now it seems to me sometimes papers just get randomly accepted.

So I was wondering, if anyone had an opinion on this, or if you have encountered other similar cases ? ",29,"Yes. The review process is very noisy. I seen the same paper get accepted to ICML but heavily rejected from ICLR with a title change.

In the case of Cold Diffusion, another factor is its popularity. Cold Diffusion was a well cited paper even with ICLR reject. So it is possible the reviewers already knew about the paper. That year in ICLR also have a similar paper [Soft Diffusion](https://arxiv.org/abs/2209.05442)"
17h3tgx,"[R] QMoE: Practical Sub-1-Bit Compression of Trillion-Parameter Models - Institute of Science and Technology Austria (ISTA) 2023 - Can compress the 1.6 trillion parameter SwitchTransformer-c2048 model to less than 160GB (20x compression, 0.8 bits per parameter) at only minor accuracy loss!",Singularian2501,127,2023-10-26T19:01:24,"Paper: [https://arxiv.org/abs/2310.16795](https://arxiv.org/abs/2310.16795)

Github: [https://github.com/ist-daslab/qmoe](https://github.com/ist-daslab/qmoe)

Abstract:

&gt;Mixture-of-Experts (MoE) architectures offer a general solution to the high inference costs of large language models (LLMs) via sparse routing, bringing faster and more accurate models, at the cost of massive parameter counts. For example, the SwitchTransformer-c2048 model has 1.6 trillion parameters, requiring 3.2TB of accelerator memory to run efficiently, which makes practical deployment challenging and expensive. In this paper, we present a solution to this memory problem, in form of a new compression and execution framework called QMoE. Specifically, **QMoE** consists of a **scalable algorithm** which **accurately compresses trillion-parameter MoEs to less than 1 bit per parameter**, in a custom format co-designed with bespoke GPU decoding kernels to facilitate efficient end-to-end compressed inference, with minor runtime overheads relative to uncompressed execution. Concretely, **QMoE can compress the 1.6 trillion parameter SwitchTransformer-c2048 model to less than 160GB (20x compression, 0.8 bits per parameter) at only minor accuracy loss, in less than a day on a single GPU.** This enables, for the first time, the execution of a trillion-parameter model on affordable commodity hardware, like a single server with 4x NVIDIA A6000 or 8x NVIDIA 3090 GPUs, at less than 5% runtime overhead relative to ideal uncompressed inference. 

https://preview.redd.it/wka92keqelwb1.jpg?width=1843&amp;format=pjpg&amp;auto=webp&amp;s=10cf67b344d3c776049da6b78244fc140b2d4142

https://preview.redd.it/khxw1neqelwb1.jpg?width=898&amp;format=pjpg&amp;auto=webp&amp;s=83006dac6e03963f2443f4e4ba710dcf8166acc8

https://preview.redd.it/xsc2ykeqelwb1.jpg?width=796&amp;format=pjpg&amp;auto=webp&amp;s=c23d03956f4a9aa9d9f185592a5bfe45039698f4",44,.8 bits per parameter… whot?
rboh2r,[D] Are transformers overhyped?,The_deepest_learner,186,2021-12-08T10:33:00,"I've been wondering about this for a pretty long time since I've never seen anybody say anything bad about transformers, while to me, they seemed pretty flawed from the moment I've read the paper. I'm in no way an ML expert. I'm only an aspiring PhD student, who's not even specializing in NLP, so if I'm any way wrong I'd really like to hear it.

tl;dr: I believe that transformers are, in the long term, a pretty small contribution to the world of NLP, and may even be damaging due to shifting the focus of the research community in the wrong direction. Why? They don't address the long-term dependency problem.

Before transformers NLP used to be dominated by RNNs and specifically the encoder-decoder architecture. In the case of translation, the encoder would encode the input sentence in a fixed-length vector and the decoder would then decode this vector into an output translated sentence. Now transformers also use encoder-decoder architecture, but there is one big difference. For RNNs, encoding and decoding actually happens at every step of the way. Words (I know it's tokens but I'll call them words) are inserted sequentially into the RNN. For every single word, the encoder RNN had to look at the current encoding vector and the input word and then choose how to update the encoding vector in a meaningful way.

The problem with this approach, which I'll call long-term dependency, arises when the RNN has to look at a very long sequence of words. Humans can easily distill the information that they've read and remember only the important bits, for example, the name of a character that was mentioned 5 pages ago. But RNN models had trouble encoding what happened 5 sentences ago. The research community starts solving this problem with the original attention paper, but then transformers come out.

So out comes the transformer and starts dominating the NLP world. What does the transformer do? It is a huge model that, when encoding simply takes 512 input words (or some other arbitrary number) and looks at all of them simultaneously. And it works wonders. Look, the transformer can remember what happened 5 sentences ago because the previous 5 sentences combined have less than 512 words, hooray. Can it remember what happened 10 sentences ago though? Uh well... no. Can we improve it in some way to solve the long-term dependency problem? Well, we can be smart about which sentences we feed into it, but that means we still have to distill information from a large body of text so... we're back at the beginning.

It's obvious that we have to solve the long-term dependency problem if we ever hope to achieve human-like NLP models, and to me, it seems that transformers do nothing to solve this problem. So why are they dominating the field of NLP research? Maybe the optimal solution will include a combination of both the transformer and some other model for information distillation, but if we still need to solve the long-term dependency problem why are throwing out RNNs so quickly?",81,"Transformers are not popular because they solve long-term dependency problem. As you have correctly discovered, they have a hard limit on their memory. They are popular because they are faster to train. With RNNs you have O(N) time complexity. The N-th word needs to wait for all the previous words to compute before it can be processed. With transformers you can easily parallelize the computation, because you don't have to wait for the N-1 previous words. You can do the calculations for all the words at the same time. This is a critical speed-up when you are trying to process TBs of text data. There were RNN-based LMs used previously (e.g. ELMO), but they are not practical at the scale we use now."
1iv6o6n,[D] Elastic/Serverless GPU instances for transformer hyper-parameter search,elbiot,8,2025-02-22T00:46:37,"**too long; didn't read:** I want to spin up a bunch of GPU instances for an hour or two at a time on demand to grid search hyper-parameters for training a decoder transformer. What services/tools do people use for this?  
  
I'm learning about transformers by trying to train a small LLM using nano-GPT. My plan is basically:

1) Grid search learning rates, batch sizes, model width/depth/architecture (keeping parameter count roughly constant).  
2) scale up the number of parameters and again search a bunch of learning rates to see if I can leverage the Maximal Update Parametrization (muP) strategy  
3) Damn it, try again  
4) Train models of a few sizes to estimate the scaling laws for my situation and determine the target model size for my training resources (available tokens, compute budget, etc)  
5) train a ""big"" (not big) model 

Right now I'm playing with a tiny model and doing runs on my 3090-ti, tracking runs with Weights and Biases) but soon I'd like to distribute out this grid searching. I've used Runpod serverless instances for inference so I've started from their Dockerfile and deployed a model there, and I could see using that here. It seems natural to just send out a bunch of requests with my parameters and have Runpod scale it out, but I'm wondering if it's kind of a hack because it's pretty geared towards inference.

What do you use when you want to run a bunch of parallel single GPU trial training runs?",11,I recommend Modal if it's something to automate. I tried to automate runpod but it was full of subprocess calls. Maybe modal wraps it but their api is nice.
vuw77a,[N] First-Ever Course on Transformers: NOW PUBLIC,DragonLord9,369,2022-07-09T07:17:02,"**CS 25: Transformers United**

https://preview.redd.it/1st4o3tvtha91.png?width=350&amp;format=png&amp;auto=webp&amp;s=e4416da38001692989304e980dd4d61d23a74398

Did you grow up wanting to play with robots that could turn into cars? While we can't offer those kinds of transformers, we do have a course on the class of deep learning models that have taken the world by storm.

Announcing the public release of our lectures from the first-ever course on **Transformers: CS25 Transformers United** ([http://cs25.stanford.edu](http://cs25.stanford.edu/)) held at [Stanford University](https://www.linkedin.com/school/stanford-university/).

Our intro video is out and available to watch here 👉: [***YouTube Link***](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM&amp;fbclid=IwAR2mJd868IzGp8ChykBBRTxq7RQh-KICfnAg8rLQ-qsekbhnUcd_z4-4E7g)

Bookmark and spread the word 🤗!

[(Twitter Thread)](https://twitter.com/DivGarg9/status/1545541542235975682?s=20&amp;t=_Ed9dpjD9Qpx4svpMNDIKQ&amp;fbclid=IwAR2tnSQROnkOQl15aa6nkfNFaJdrnZQHDbidooDaQRJALlWsYMiQU_37dn4)

Speaker talks out starting Monday ...",40,"An entire course just on transformers? What's next, a web series on residual blocks?

Jokes aside, this looks more like a speaker series about transformer research rather than a ""course""."
lwysts,[N] Google Study Shows Transformer Modifications Fail To Transfer Across Implementations and Applications,Yuqing7,337,2021-03-03T17:04:59,"A team from Google Research explores why most transformer modifications have not transferred across implementation and applications, and surprisingly discovers that most modifications do not meaningfully improve performance.

Here is a quick read: [Google Study Shows Transformer Modifications Fail To Transfer Across Implementations and Applications](https://syncedreview.com/2021/03/03/google-study-shows-transformer-modifications-fail-to-transfer-across-implementations-and-applications/)

The paper *Do Transformer Modifications Transfer Across Implementations and Applications?* is on [arXiv](https://arxiv.org/pdf/2102.11972.pdf).",63,"&gt; Not tuning hyperparameters handicapped other
methods. While per-modification tuning might improve results (as verified in section 4.2), we argue that
truly useful improvements to the Transformer should
be reasonably hyperparameter-agnostic. Further, if
hyperparameter sensitivity was the issue, it would
be likely that a least a few of the compared methods
“got lucky” with the hyperparameter settings, but
very few modifications produced a boost.

This is a little rich, given the amount of hparam tuning (explicit and implicit) that goes in in some (but not all) Google papers."
1jkngt9,[D] Data for Cow segmentation for Vision Transformer,FederalDog9965,1,2025-03-26T21:38:17,"I am working on cow teeth segmentation, I have limited amount of data. I used CNN and the performance wasn't that good. I know Vision Transformers(ViT) will improve the performance but with the limited data how can I use ViT? Is there any way to generate more similar(cow teeth) data?",7,Do you know SAM2? Check it out. Maybe it already solves whatever you want to do.
1gn6sam,[D] Has anyone replaced Transformers with fully-connected layers and verified that it performs strictly worse (for training language models)?,Cybernetic1,0,2024-11-09T09:52:49,"Seems an obvious question but such a ""data point"" would be very helpful to clear our ignorance.",24,"There is MLPMixer though I think that model is primarily used in Computer Vision.

Performs pretty well but, not state of the art quality.

Not sure how it would do in NLP."
1kekxqg,[D] Unstable training curves for transformers?,Top-Influence-5529,1,2025-05-04T14:21:04,"I'm training a llama transformer (using huggingface library) model on a synthetic task:

given a sequence of permutations on 5 elements, calculate the sequence of compositions of permutations. so if the input is (p\_1,p\_2,p\_3) the output should be (p\_1, p\_1\*p\_2, p\_1\*p\_2\*p\_3). I manually assigned indices to each permutation, so I don't use a tokenizer.

  
I'm training my model, and when the performance is starting to saturate, sometimes the training accuracy collapses, but it recovers back to the previous level in 1 epoch (I train for a total of 30-40 epochs). Has anyone else experienced something similar? I decreased the learning rate and that seemed to help.

  
Another issue I noticed: If I generate a fresh synthetic training set and train on that, the initial training accuracy is a lot lower than before. It quickly converges to the previous accuracy and continues to improve. Maybe that is a sign of overfitting to the old training set? The strange thing is, the accuracy on a validation set is stable, so why would training accuracy drop on the new training set?

More generally, are there any resources that describe debugging tricks and heuristics when training neural networks?",2,[error fetching comment: 'body']
1ikj9ml,[D] Is it possible to fused different blocks even whole transformer to accelerate LLM train and reference by Triton?,Logical_Divide_3595,18,2025-02-08T09:04:53,"There will be less intermediate variable if we fused different blocks in transformer, like ""feed forward"" and ""Add &amp; Norm"", ""Linear"" and ""Softmax"", even the whole transformer layer. This can reduce much memory usage and computation.

Are there similar works or research?",10,"Yes there is a huge amount of work on optimizing transformers end to end. If you use any popular LLM serving engine SGLang, vLLM, TRT-LLM, and so on the transformers have been carefully optimized with hand written kernels to achieve the maximize performance. A common optimization is to combine the attention block into FMHA or fused multi-head-attention which combines the entire attention block into a single kernel. 

Whether fusion is valuable or not depends on the workload itself, for example in many cases you can’t eliminate the intermediate values and there for combining a large kernels together might only save a small amount of kernel launch overhead. In fact PyTorch’s inductor back end converts models to Triton and compiles them automatically even performing some fusion."
1amzb52,Faith and Fate: Limits of Transformers on Compositionality [R],we_are_mammals,34,2024-02-09T21:34:13,"Edit: Kevin Murphy,  Francois Chollet, Vitaly Kurin and others recommended this paper (some very highly)

https://arxiv.org/abs/2305.18654 (Presented at NeurIPS in December)

**Abstract:**

Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with increased task complexity.

---

Kevin Murphy's summary: ""I like this paper. They prove that transformers are guaranteed to suffer from compounding errors when doing long reasoning chains (as @ylecun has argued), and much apparent ""success"" is just due to unreliable pattern matching / shortcut learning.""",47,[deleted]
19cv8q6,[D] Beyond Transformers: Structured State Space Sequence Models,cnichkawde,46,2024-01-22T13:07:31,Wrote an article explaining the fundamentals of State Space Sequence Models. The purpose of this article is to present the foundational level concepts in a simplified manner. This field is rapidly evolving in the realm of artificial intelligence owing to the leap it gives over Transformer architecture in terms of speed and memory consumption. Here is the link to the article: [https://cnichkawde.github.io/statespacesequencemodels.html](https://cnichkawde.github.io/statespacesequencemodels.html),46,"&gt; We have learned that if we throw enough computing and large amounts of data then a form of intelligence spontaneously emerges by doing something as simple as autoregressive pretraining.

Really?"
u1jbr2,[R] Transformers replicate Hippocampal representations; notably place and grid cells in the brain,Competitive-Rub-1958,172,2022-04-11T22:19:32,"Paper: [https://arxiv.org/abs/2112.04035](https://arxiv.org/abs/2112.04035)

Yes, the paper is cautious about comparing the model one-to-one to the brain

&gt;“Note, we are not saying the brain is closely related to transformers  because it learns the same neural representations, instead we are saying  the relationship is close because we have shown a mathematical  relationship between transformers and carefully formulated neuroscience  models of the hippocampal formation.”

While objections like ""its just correlation/relation, its not exactly the same!!"" are true to an extend, its still a **very** unexpected observation that, they're even remotely similar. Needless to say, Transformers were not inspired from the brain - and as more evidence collates ([https://www.nature.com/articles/s42003-022-03036-1](https://www.nature.com/articles/s42003-022-03036-1) \--&gt; Activations are linearly correlatable) it does feel mysterious; perhaps atleast *some* of the systems used by the brain converge on an efficient pattern discovered by our backpropogated friends...

[\[](https://giphy.com/gifs/clonespiracy-ioR8R00S5SibK)insert 'coincidence? I think not!' meme\]",69,"One of the biggest ML criticisms is that backpropagation does not exist in the brain. But if the *results* of backprop can be found there, does it really matter?"
j4xmht,[D] Paper Explained - An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Full Video Analysis),ykilcher,375,2020-10-04T11:42:18,"[https://youtu.be/TrdevFK\_am4](https://youtu.be/TrdevFK_am4)

Transformers are Ruining Convolutions. This paper, under review at ICLR, shows that given enough data, a standard Transformer can outperform Convolutional Neural Networks in image recognition tasks, which are classically tasks where CNNs excel. In this Video, I explain the architecture of the Vision Transformer (ViT), the reason why it works better and rant about why double-bline peer review is broken.

&amp;#x200B;

OUTLINE:

0:00 - Introduction

0:30 - Double-Blind Review is Broken

5:20 - Overview

6:55 - Transformers for Images

10:40 - Vision Transformer Architecture

16:30 - Experimental Results

18:45 - What does the Model Learn?

21:00 - Why Transformers are Ruining Everything

27:45 - Inductive Biases in Transformers

29:05 - Conclusion &amp; Comments

&amp;#x200B;

Paper (Under Review): [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy)",59,Train your own dragon! Only 2.5K TPU-days required!
1k6i2c8,[D] Most widely used open-source decoder-only transformer?,Sea_Farmer5942,2,2025-04-24T03:03:39,"Hey guys,

So this question really stemmed from training a transformer and using GPT-2 as the backbone. Its just easy to use and isn't too large in architecture. How much better is something like Llama 3? How about in research, what transformers are typically used?

Many thanks!",2,"Any Llama is much more recent and better than GPT-2

Edit: maybe add Qwen and DeepSeek to your options. Read r/LocalLLaMA for ideas of what other models people are using"
1jizocl,[P] Efficient Language Model Built on WikiText-2: A Simpler Alternative to Transformers (Source Code &amp; Results Included),SetYourHeartAblaze_V,8,2025-03-24T19:36:57,"Hi all,

  
got GPT to draft the rest of this as I am not as good at explaining things. Would be great to hear some feedback on this work and whether it seems like it's worth continuing experimenting with? Please feel free to use and modify the source code for your own experiments but please credit me if you're doing anything cool with it? :-) the tl'dr is : Made a model that is vastly more efficient than transformers and has good eval metrics: Validation Loss: 2.2097 | Perplexity: 9.1127

Hey everyone,

I recently worked on a language model project and wanted to share it with you. The goal was to build an efficient model that can understand and generate text—similar to how Transformers work—but with less computational overhead. I'll explain what I did in simple terms and share both the code and the evaluation results.

# What Is This Project About?

**Traditional Transformers:**  
Transformers are a popular type of model for language tasks, but they perform something called “full self-attention,” which means every word in a sentence looks at every other word. This leads to high computational costs, especially for longer texts.

**My Approach:**  
I built a model that uses a method called **Hierarchical Snapshot Modeling**. Instead of having every word interact with every other word, the model compresses the sequence into a smaller set of “snapshot tokens.” Think of these snapshots as summary points that capture the key ideas of the text.

# Key Ideas Behind the Model

1. **Enhanced Positional Encoding:**
   * **What it means:** The model learns not only where each word is in a sentence but also how words relate to each other over distances.
   * **Why it's cool:** This helps the model understand long-range connections in text without extra heavy computations.
2. **Dynamic Snapshot Aggregation:**
   * **What it means:** Instead of simply averaging these snapshot tokens, the model uses an attention mechanism (a way to weight the importance of each snapshot) to decide which parts of the text are most important.
   * **Why it's cool:** This allows the model to focus on the most informative parts of the text and ignore less useful parts.
3. **Efficient Graph Layers:**
   * **What it means:** The model uses layers that only let words close to each other interact, rather than forcing all words to interact. It also combines local details with a global overview.
   * **Why it's cool:** This “sparse connectivity” significantly reduces the number of calculations required, making the model faster and more efficient.
4. **Hybrid &amp; Adaptive Techniques:**
   * **What it means:** The model includes options for experimenting with even more efficient attention methods (inspired by recent research) so that it can adaptively choose which words to pay attention to.
   * **Why it's cool:** It’s a flexible design that could potentially lead to even more improvements in the future.

# How Does It Compare to Traditional Transformers?

* **Efficiency:** Standard Transformers compute interactions between all pairs of words (quadratic complexity). My model reduces this by summarizing the sequence into snapshot tokens, making it more efficient, especially on longer texts.
* **Size &amp; Performance:** With about **17–18 million parameters**, this model is in the same ballpark as some small Transformer models (like certain configurations of Transformer-XL) that have been used on the WikiText-2 dataset. Our evaluation showed:
   * **Validation Loss:** \~2.21
   * **Perplexity:** \~9.11 These numbers indicate that the model is performing well on the task, even though it is more efficient.

# What’s Next?

I’ve made the full source code available below  along with detailed evaluation logs. This project is a proof-of-concept that efficient modeling is possible without the heavy computational cost of full self-attention. Whether you’re just curious about language models or looking to experiment with new ideas in NLP, I hope you find this work interesting.

    import os
    os.environ[""XLA_FLAGS""] = ""--xla_gpu_enable_command_buffer=""
    import tensorflow as tf
    
    import math
    import re
    import numpy as np
    from collections import Counter
    from tqdm import tqdm
    
    # Enable XLA JIT compilation.
    tf.config.optimizer.set_jit(True)
    
    # Hugging Face datasets, spaCy, and NLTK (assumed installed)
    from datasets import load_dataset
    import spacy
    import nltk
    nltk.download('punkt')
    from nltk.translate.bleu_score import sentence_bleu
    
    print(""TensorFlow version:"", tf.__version__)
    print(""GPU available?"", len(tf.config.list_physical_devices('GPU')) &gt; 0)
    
    # ========================
    # 1. Model Components
    # ========================
    
    def split_heads(x, num_heads):
        # x: (batch, seq_len, total_dim) -&gt; (batch, num_heads, seq_len, d)
        total_dim = tf.shape(x)[-1]
        d = total_dim // num_heads
        x = tf.reshape(x, (tf.shape(x)[0], tf.shape(x)[1], num_heads, d))
        return tf.transpose(x, perm=[0, 2, 1, 3])
    
    # --- Enhanced Positional Encoding: Relative Position Bias ---
    class RelativePositionBias(tf.keras.layers.Layer):
        def __init__(self, max_seq_len, num_snapshots, num_heads, max_distance=128):
            """"""
            max_seq_len: maximum sequence length
            num_snapshots: number of snapshot tokens (virtual query positions)
            num_heads: number of attention heads
            max_distance: maximum relative distance to consider (will be clipped)
            """"""
            super(RelativePositionBias, self).__init__()
            self.max_seq_len = max_seq_len
            self.num_snapshots = num_snapshots
            self.num_heads = num_heads
            self.max_distance = max_distance
            # Create an embedding table for relative distances in the range [-max_distance, max_distance]
            self.relative_embedding = tf.keras.layers.Embedding(2 * max_distance + 1, num_heads)
            # Precompute snapshot positions as evenly spaced indices (as integers in [0, max_seq_len-1])
            self.snapshot_positions = tf.cast(tf.linspace(0.0, max_seq_len - 1, num_snapshots), tf.int32)
    
        def call(self, token_positions):
            # token_positions: (B, seq_len) with integer positions.
            # Compute relative distances between each snapshot (query) and each token (key).
            # Expand snapshot positions to (1, num_snapshots, 1) and token_positions to (B, 1, seq_len)
            token_positions = tf.cast(token_positions, tf.int32)
            snapshot_positions = tf.reshape(self.snapshot_positions, (1, self.num_snapshots, 1))
            token_positions_expanded = tf.expand_dims(token_positions, axis=1)  # (B, 1, seq_len)
            relative_distance = token_positions_expanded - snapshot_positions  # (B, num_snapshots, seq_len)
            # Clip distances and shift to non-negative indices for embedding lookup.
            clipped_distance = tf.clip_by_value(relative_distance, -self.max_distance, self.max_distance)
            clipped_distance += self.max_distance  # now in [0, 2*max_distance]
            # Lookup the bias for each relative distance: output shape (B, num_snapshots, seq_len, num_heads)
            bias = self.relative_embedding(clipped_distance)
            # Transpose to (B, num_heads, num_snapshots, seq_len) so it can be added to attention scores.
            bias = tf.transpose(bias, perm=[0, 3, 1, 2])
            return bias
    
    # --- Multi-Head Snapshot Module with Dynamic Aggregation and Optional Linear Attention ---
    class MultiHeadSnapshotModule(tf.keras.layers.Layer):
        def __init__(self, embed_dim, num_heads, snapshot_dim, num_snapshots, max_seq_len, use_linear_attention=False):
            """"""
            embed_dim: final model embedding dimension
            num_heads: number of snapshot heads
            snapshot_dim: per-head dimension
            num_snapshots: fixed number of snapshot tokens
            max_seq_len: maximum sequence length (for relative positional bias)
            use_linear_attention: flag to optionally use an approximate linear attention mechanism
            """"""
            super(MultiHeadSnapshotModule, self).__init__()
            self.num_heads = num_heads
            self.snapshot_dim = snapshot_dim  # per-head dimension
            self.num_snapshots = num_snapshots
            total_snapshot_dim = num_heads * snapshot_dim
            # Trainable snapshot tokens: shape (num_snapshots, total_snapshot_dim)
            self.snapshot_tokens = self.add_weight(
                shape=(num_snapshots, total_snapshot_dim),
                initializer='random_normal',
                trainable=True
            )
            self.key_proj = tf.keras.layers.Dense(total_snapshot_dim)
            self.value_proj = tf.keras.layers.Dense(total_snapshot_dim)
            self.query_proj = tf.keras.layers.Dense(total_snapshot_dim)
            self.out_proj = tf.keras.layers.Dense(embed_dim)
            
            # Relative positional bias layer.
            self.rel_pos_bias = RelativePositionBias(max_seq_len, num_snapshots, num_heads)
            
            # Dynamic aggregation: instead of averaging snapshot tokens, learn to weight them.
            self.snapshot_agg = tf.keras.layers.Dense(1)
            
            # Flag for potential hybrid attention mechanisms.
            self.use_linear_attention = use_linear_attention
    
        def call(self, x, token_positions=None):
            # x: (B, seq_len, embed_dim)
            batch_size = tf.shape(x)[0]
            seq_len = tf.shape(x)[1]
            keys = self.key_proj(x)      # (B, seq_len, total_snapshot_dim)
            values = self.value_proj(x)  # (B, seq_len, total_snapshot_dim)
            # Expand snapshot tokens: (B, num_snapshots, total_snapshot_dim)
            snapshot = tf.expand_dims(self.snapshot_tokens, axis=0)
            snapshot = tf.tile(snapshot, [batch_size, 1, 1])
            queries = self.query_proj(snapshot)  # (B, num_snapshots, total_snapshot_dim)
            
            keys = split_heads(keys, self.num_heads)      # (B, num_heads, seq_len, snapshot_dim)
            values = split_heads(values, self.num_heads)  # (B, num_heads, seq_len, snapshot_dim)
            queries = split_heads(queries, self.num_heads)  # (B, num_heads, num_snapshots, snapshot_dim)
            
            d = tf.cast(self.snapshot_dim, tf.float32)
            scale = tf.math.sqrt(d)
            # Standard dot-product attention scores.
            attn_scores = tf.matmul(queries, keys, transpose_b=True) / scale  # (B, num_heads, num_snapshots, seq_len)
            
            # Integrate relative positional bias if token positions are provided.
            if token_positions is not None:
                rel_bias = self.rel_pos_bias(token_positions)  # (B, num_heads, num_snapshots, seq_len)
                attn_scores += rel_bias
            
            # Optionally, one could implement a linear attention variant here:
            if self.use_linear_attention:
                # [Placeholder] Implement linear attention approximations (e.g., using kernel feature maps)
                # For now, we continue with standard softmax attention.
                pass
    
            attn_weights = tf.nn.softmax(attn_scores, axis=-1)
            head_output = tf.matmul(attn_weights, values)  # (B, num_heads, num_snapshots, snapshot_dim)
            head_output = tf.transpose(head_output, perm=[0, 2, 1, 3])  # (B, num_snapshots, num_heads, snapshot_dim)
            combined = tf.reshape(head_output, (batch_size, self.num_snapshots, self.num_heads * self.snapshot_dim))
            
            # Dynamic snapshot aggregation using learned attention-based pooling.
            agg_weights = self.snapshot_agg(combined)  # (B, num_snapshots, 1)
            agg_weights = tf.nn.softmax(agg_weights, axis=1)  # (B, num_snapshots, 1)
            global_snapshot = tf.reduce_sum(combined * agg_weights, axis=1)  # (B, num_heads * snapshot_dim)
            
            output = self.out_proj(global_snapshot)  # (B, embed_dim)
            return output
    
    # --- Spatial Graph Layer with Sparse Connectivity, Hierarchical Aggregation, and Adaptive Gating ---
    class SpatialGraphLayer(tf.keras.layers.Layer):
        def __init__(self, embed_dim, sparse_threshold=None, use_hierarchical=False, residual_scale=1.0):
            """"""
            embed_dim: embedding dimension
            sparse_threshold: if provided, only tokens with distances below this threshold contribute to messages
            use_hierarchical: if True, incorporates a global context via a hierarchical connection
            residual_scale: scaling factor for the residual connection (improved stability)
            """"""
            super(SpatialGraphLayer, self).__init__()
            self.embed_dim = embed_dim
            self.sparse_threshold = sparse_threshold
            self.use_hierarchical = use_hierarchical
            self.residual_scale = residual_scale
            self.coord_proj = tf.keras.layers.Dense(3)
            self.message_proj = tf.keras.layers.Dense(embed_dim)
            self.update_proj = tf.keras.layers.Dense(embed_dim)
            self.norm = tf.keras.layers.LayerNormalization()
            if self.use_hierarchical:
                self.global_proj = tf.keras.layers.Dense(embed_dim)
            # Adaptive gating mechanism to allow tokens to dynamically control the update.
            self.gate_proj = tf.keras.layers.Dense(embed_dim, activation='sigmoid')
        
        def call(self, x):
            # x: (B, seq_len, embed_dim)
            coords = self.coord_proj(x)  # (B, seq_len, 3)
            coords_sq = tf.reduce_sum(tf.square(coords), axis=-1, keepdims=True)  # (B, seq_len, 1)
            distances = coords_sq + tf.transpose(coords_sq, perm=[0, 2, 1]) - 2 * tf.matmul(coords, coords, transpose_b=True)
            distances = tf.maximum(distances, 0.0)
            sigma = 1.0
            edge_weights = tf.exp(-distances / (2 * sigma**2))  # (B, seq_len, seq_len)
            
            # Apply sparse connectivity if a threshold is specified.
            if self.sparse_threshold is not None:
                mask = tf.cast(distances &lt; self.sparse_threshold, tf.float32)
                edge_weights = edge_weights * mask
                edge_weights = edge_weights / (tf.reduce_sum(edge_weights, axis=-1, keepdims=True) + 1e-6)
            else:
                edge_weights = edge_weights / (tf.reduce_sum(edge_weights, axis=-1, keepdims=True) + 1e-6)
            
            messages = self.message_proj(x)  # (B, seq_len, embed_dim)
            aggregated = tf.matmul(edge_weights, messages)  # (B, seq_len, embed_dim)
            update = self.update_proj(aggregated)
            # Adaptive gating: compute a gate from the input to modulate the update.
            gate = self.gate_proj(x)
            update = update * gate
            # Hierarchical connection: add global context if enabled.
            if self.use_hierarchical:
                global_context = tf.reduce_mean(x, axis=1, keepdims=True)
                global_context = self.global_proj(global_context)
                update += global_context  # Shape: (B, 1, embed_dim) broadcasts to (B, seq_len, embed_dim)
    
            updated = self.norm(x + update * self.residual_scale)
            return updated
    
    # --- Hierarchical Snapshot Model ---
    class HierarchicalSnapshotModel(tf.keras.Model):
        def __init__(self, vocab_size, max_seq_len, embed_dim, num_layers,
                     snapshot_dim, num_snapshots, group_size, num_snapshot_heads,
                     dropout_rate=0.2):
            super(HierarchicalSnapshotModel, self).__init__()
            self.vocab_size = vocab_size
            self.token_embed = tf.keras.layers.Embedding(vocab_size, embed_dim)
            self.abs_pos_embed = tf.keras.layers.Embedding(max_seq_len, embed_dim)
            self.grouped_pos_embed = GroupedPositionalEmbedding(max_seq_len, group_size, embed_dim)
            # Pass max_seq_len to the snapshot module for relative bias computation.
            self.multi_head_snapshot = MultiHeadSnapshotModule(
                embed_dim, num_snapshot_heads, snapshot_dim, num_snapshots, max_seq_len
            )
            # You can adjust the graph layer with sparse_threshold and hierarchical flags as needed.
            self.graph_layers = [
                SpatialGraphLayer(embed_dim, sparse_threshold=100.0, use_hierarchical=True, residual_scale=0.9)
                for _ in range(num_layers)
            ]
            self.out_proj = tf.keras.layers.Dense(vocab_size)
            self.dropout = tf.keras.layers.Dropout(dropout_rate)
        
        def call(self, inputs, training=False):
            # inputs: tuple (token_ids, positions, group_ids)
            token_ids, positions, group_ids = inputs
            x = self.token_embed(token_ids)
            abs_pos = self.abs_pos_embed(positions)
            grouped_pos = self.grouped_pos_embed(positions, group_ids)
            x = x + abs_pos + grouped_pos
            x = self.dropout(x, training=training)
            # Global context from multi-head snapshot attention.
            # Pass the token positions to enable relative positional bias.
            snapshot_vector = self.multi_head_snapshot(x, token_positions=positions)  # (B, embed_dim)
            snapshot_bias = tf.expand_dims(snapshot_vector, axis=1)  # (B, 1, embed_dim)
            x = x + snapshot_bias
            for layer in self.graph_layers:
                x = layer(x)
            logits = self.out_proj(x)
            return logits
    
    # ------------------------------
    # (Re)Defining the GroupedPositionalEmbedding for completeness.
    class GroupedPositionalEmbedding(tf.keras.layers.Layer):
        def __init__(self, max_position, group_size, embed_dim):
            super(GroupedPositionalEmbedding, self).__init__()
            self.abs_embedding = tf.keras.layers.Embedding(max_position, embed_dim)
            num_groups = (max_position + group_size - 1) // group_size
            self.group_embedding = tf.keras.layers.Embedding(num_groups, embed_dim)
        
        def call(self, positions, group_ids):
            pos_embed = self.abs_embedding(positions)
            group_embed = self.group_embedding(group_ids)
            return pos_embed + group_embed
    
    # ========================
    # 2. Data Loading &amp; Preprocessing (WikiText-2)
    # ========================
    
    print(""Loading WikiText2 dataset (English)..."")
    dataset = load_dataset(""wikitext"", ""wikitext-2-v1"")
    train_sentences = dataset[""train""][""text""]
    valid_sentences = dataset[""validation""][""text""]
    
    nlp_en = spacy.load(""en_core_web_sm"")
    def tokenize_en(text):
        return [token.text for token in nlp_en(text)]
    
    def build_vocab(sentences, tokenizer, min_freq=3):
        counter = Counter()
        for sentence in sentences:
            tokens = tokenizer(sentence)
            counter.update(tokens)
        specials = ['&lt;pad&gt;', '&lt;sos&gt;', '&lt;eos&gt;', '&lt;unk&gt;']
        vocab = {token: i for i, token in enumerate(specials)}
        for token, freq in counter.items():
            if freq &gt;= min_freq and token not in vocab:
                vocab[token] = len(vocab)
        return vocab
    
    print(""Building vocabulary..."")
    vocab = build_vocab(train_sentences, tokenize_en, min_freq=3)
    vocab_size = len(vocab)
    print(""Vocab size:"", vocab_size)
    
    def tokens_to_ids(tokens, vocab):
        return [vocab.get(token, vocab['&lt;unk&gt;']) for token in tokens]
    
    def collate_fn(sentences):
        batch_token_ids = []
        batch_positions = []
        batch_group_ids = []
        for sentence in sentences:
            tokens = tokenize_en(sentence)
            tokens = ['&lt;sos&gt;'] + tokens + ['&lt;eos&gt;']
            token_ids = tokens_to_ids(tokens, vocab)
            positions = list(range(len(token_ids)))
            group_ids = []
            group = 0
            punct = {""."", ""!"", ""?"", "";"", "":""}
            for token in tokens:
                group_ids.append(group)
                if token in punct:
                    group += 1
            batch_token_ids.append(token_ids)
            batch_positions.append(positions)
            batch_group_ids.append(group_ids)
        max_len = max(len(seq) for seq in batch_token_ids)
        for i in range(len(batch_token_ids)):
            pad_len = max_len - len(batch_token_ids[i])
            batch_token_ids[i] += [vocab['&lt;pad&gt;']] * pad_len
            batch_positions[i] += [0] * pad_len
            batch_group_ids[i] += [0] * pad_len
        inputs = [seq[:-1] for seq in batch_token_ids]
        targets = [seq[1:] for seq in batch_token_ids]
        positions = [seq[:-1] for seq in batch_positions]
        group_ids = [seq[:-1] for seq in batch_group_ids]
        return (np.array(inputs, dtype=np.int32),
                np.array(positions, dtype=np.int32),
                np.array(group_ids, dtype=np.int32),
                np.array(targets, dtype=np.int32))
    
    def generator(sentences, batch_size=16):
        batch = []
        for sentence in sentences:
            if sentence.strip():
                batch.append(sentence)
                if len(batch) == batch_size:
                    yield collate_fn(batch)
                    batch = []
        if batch:
            yield collate_fn(batch)
    
    BATCH_SIZE = 16
    train_dataset = tf.data.Dataset.from_generator(
        lambda: generator(train_sentences, batch_size=BATCH_SIZE),
        output_signature=(
            tf.TensorSpec(shape=(None, None), dtype=tf.int32),
            tf.TensorSpec(shape=(None, None), dtype=tf.int32),
            tf.TensorSpec(shape=(None, None), dtype=tf.int32),
            tf.TensorSpec(shape=(None, None), dtype=tf.int32)
        )
    )
    valid_dataset = tf.data.Dataset.from_generator(
        lambda: generator(valid_sentences, batch_size=BATCH_SIZE),
        output_signature=(
            tf.TensorSpec(shape=(None, None), dtype=tf.int32),
            tf.TensorSpec(shape=(None, None), dtype=tf.int32),
            tf.TensorSpec(shape=(None, None), dtype=tf.int32),
            tf.TensorSpec(shape=(None, None), dtype=tf.int32)
        )
    )
    # Map dataset elements to ((inputs, positions, group_ids), targets)
    train_dataset = train_dataset.map(lambda a, b, c, d: ((a, b, c), d),
                                      num_parallel_calls=tf.data.AUTOTUNE)
    valid_dataset = valid_dataset.map(lambda a, b, c, d: ((a, b, c), d),
                                      num_parallel_calls=tf.data.AUTOTUNE)
    # Repeat training dataset so model.fit doesn't run out of data; compute steps_per_epoch.
    train_dataset = train_dataset.repeat().prefetch(tf.data.AUTOTUNE)
    valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)
    
    # Build inverse vocabulary for decoding.
    inv_vocab = {i: token for token, i in vocab.items()}
    
    # ========================
    # 3. Training Setup
    # ========================
    
    device = ""/gpu:0"" if len(tf.config.list_physical_devices('GPU')) &gt; 0 else ""/cpu:0""
    print(""Training on device:"", device)
    
    # Updated hyperparameters for increased capacity.
    max_seq_len = 256
    embed_dim = 256          # Increased embedding dimension.
    num_layers = 6           # More layers.
    snapshot_dim = 64        # Per-head dimension (can be tuned).
    num_snapshots = 4
    group_size = 8
    num_snapshot_heads = 8   # More snapshot heads.
    NUM_EPOCHS = 10          # More epochs.
    learning_rate = 1e-4      # Lower learning rate for more stable training.
    
    # Define masked loss and accuracy functions to ignore pad tokens.
    def masked_loss_fn(pad_token_id):
        def loss_fn(y_true, y_pred):
            loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)
            mask = tf.cast(tf.not_equal(y_true, pad_token_id), tf.float32)
            loss *= mask
            return tf.reduce_sum(loss) / tf.reduce_sum(mask)
        return loss_fn
    
    def masked_accuracy_fn(pad_token_id):
        def accuracy_fn(y_true, y_pred):
            y_pred_ids = tf.argmax(y_pred, axis=-1, output_type=tf.int32)
            mask = tf.cast(tf.not_equal(y_true, pad_token_id), tf.float32)
            correct = tf.cast(tf.equal(y_true, y_pred_ids), tf.float32) * mask
            return tf.reduce_sum(correct) / tf.reduce_sum(mask)
        return accuracy_fn
    
    pad_token_id = vocab['&lt;pad&gt;']
    
    with tf.device(device):
        model = HierarchicalSnapshotModel(
            vocab_size, max_seq_len, embed_dim, num_layers,
            snapshot_dim, num_snapshots, group_size, num_snapshot_heads, dropout_rate=0.2
        )
        model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate),
            loss=masked_loss_fn(pad_token_id),
            metrics=[masked_accuracy_fn(pad_token_id)]
        )
    
    # Compute steps per epoch based on training examples.
    steps_per_epoch = math.ceil(len([s for s in train_sentences if s.strip()]) / BATCH_SIZE)
    validation_steps = math.ceil(len([s for s in valid_sentences if s.strip()]) / BATCH_SIZE)
    
    # Add a learning rate scheduler callback.
    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,
                                                        patience=2, min_lr=1e-6, verbose=1)
    
    checkpoint_dir = ""./kaggle/working/checkpoints""
    os.makedirs(checkpoint_dir, exist_ok=True)
    checkpoint_path = os.path.join(checkpoint_dir, ""cp-{epoch:04d}.weights.h5"")
    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_path,
        save_weights_only=True,
        verbose=1,
        save_freq='epoch'
    )
    
    history = model.fit(
        train_dataset,
        epochs=NUM_EPOCHS,
        steps_per_epoch=steps_per_epoch,
        validation_data=valid_dataset,
        validation_steps=validation_steps,
        callbacks=[checkpoint_callback, lr_scheduler]
    )
    print(""Training complete!"")
    
    # ========================
    # 4. Evaluation Functions
    # ========================
    
    def evaluate_perplexity(model, dataset):
        total_loss = 0.0
        total_tokens = 0.0
        for (inputs, positions, group_ids), targets in tqdm(dataset, desc=""Evaluating Perplexity""):
            logits = model((inputs, positions, group_ids), training=False)
            loss = tf.keras.losses.sparse_categorical_crossentropy(targets, logits, from_logits=True)
            mask = tf.cast(tf.not_equal(targets, pad_token_id), tf.float32)
            loss *= mask
            total_loss += tf.reduce_sum(loss).numpy()
            total_tokens += tf.reduce_sum(mask).numpy()
        avg_loss = total_loss / total_tokens
        perplexity = math.exp(avg_loss)
        return avg_loss, perplexity
    
    avg_loss, perplexity = evaluate_perplexity(model, valid_dataset)
    print(f""Validation Loss: {avg_loss:.4f} | Perplexity: {perplexity:.4f}"")
    
    def generate_text(model, prompt_tokens, max_length=50, temperature=1.0):
        generated = prompt_tokens.copy()
        for _ in range(max_length):
            input_seq = tf.expand_dims(generated, axis=0)  # (1, current_length)
            positions = tf.expand_dims(tf.range(len(generated)), axis=0)
            group_ids = tf.zeros_like(input_seq, dtype=tf.int32)
            logits = model((input_seq, positions, group_ids), training=False)
            # Temperature sampling instead of pure greedy:
            last_logits = logits[0, -1, :] / temperature
            next_token = tf.random.categorical(tf.expand_dims(last_logits, 0), num_samples=1)[0, 0].numpy().item()
            generated.append(next_token)
            if next_token == vocab['&lt;eos&gt;']:
                break
        return generated
    
    def decode_tokens(token_list, inv_vocab):
        words = [inv_vocab.get(token, '&lt;unk&gt;') for token in token_list if token not in (vocab['&lt;sos&gt;'], vocab['&lt;eos&gt;'], vocab['&lt;pad&gt;'])]
        return "" "".join(words)
    
    def evaluate_bleu(model, sentences, num_examples=50, max_gen_length=50, temperature=1.0):
        scores = []
        for sentence in sentences[:num_examples]:
            tokens = tokenize_en(sentence)
            tokens = ['&lt;sos&gt;'] + tokens + ['&lt;eos&gt;']
            token_ids = tokens_to_ids(tokens, vocab)
            prompt = [vocab['&lt;sos&gt;']]
            generated_ids = generate_text(model, prompt, max_length=max_gen_length, temperature=temperature)
            generated_text = decode_tokens(generated_ids, inv_vocab)
            reference_text = decode_tokens(token_ids, inv_vocab)
            bleu = sentence_bleu([reference_text.split()], generated_text.split())
            scores.append(bleu)
        return np.mean(scores)
    
    bleu_score = evaluate_bleu(model, valid_sentences, num_examples=50, max_gen_length=50, temperature=0.8)
    print(""Average BLEU score on validation examples:"", bleu_score)
    

**Evaluation Logs:**

    Epoch 10/10
    1486/1486 ━━━━━━━━━━━━━━━━━━━━ 471s 317ms/step - accuracy_fn: 0.5753 - loss: 2.7553 - val_accuracy_fn: 0.6579 - val_loss: 2.4391 - learning_rate: 1.0000e-04
    ...
    Validation Loss: 2.2097 | Perplexity: 9.1127
    

# Final Thoughts

This project is an experiment in making language models more efficient without sacrificing performance. I’m excited to see how these ideas could be expanded and improved in the future. If you have any questions, suggestions, or just want to chat about language models, please feel free to comment!

Cheers, and happy coding!

  
",5,OK how has no one else replied to this yet!? This is amazing. Would you be open to my reaching out about it? I have an immediate and very exciting use case.
