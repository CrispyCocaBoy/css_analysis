title,score,author,created_utc,url,permalink
[P] Illustrated book to learn about Transformers &amp; LLMs,312,shervinea,2024-08-19T13:14:52,https://www.reddit.com/r/MachineLearning/comments/1ew1hws/p_illustrated_book_to_learn_about_transformers/,https://www.reddit.com/r/MachineLearning/comments/1ew1hws/p_illustrated_book_to_learn_about_transformers/
"[R] Transformers without Normalization (FAIR Meta, New York University, MIT, Princeton University)",271,Nunki08,2025-03-15T10:30:09,https://www.reddit.com/r/MachineLearning/comments/1jbs7xg/r_transformers_without_normalization_fair_meta/,https://www.reddit.com/r/MachineLearning/comments/1jbs7xg/r_transformers_without_normalization_fair_meta/
[D] Have transformers won in Computer Vision?,192,Amgadoz,2025-01-12T13:47:30,https://www.reddit.com/r/MachineLearning/comments/1hzn0gg/d_have_transformers_won_in_computer_vision/,https://www.reddit.com/r/MachineLearning/comments/1hzn0gg/d_have_transformers_won_in_computer_vision/
[R] Tiny Language Models (below 10m parameters or only one transformer block) can generate paragraphs of coherent text and reason...provided training is limited to stories that only contain words that a typical 3 to 4-year-olds usually understand.,578,MysteryInc152,2023-05-16T10:00:43,https://www.reddit.com/r/MachineLearning/comments/13j0spj/r_tiny_language_models_below_10m_parameters_or/,https://www.reddit.com/r/MachineLearning/comments/13j0spj/r_tiny_language_models_below_10m_parameters_or/
[D] Everyone is so into LLMs but can the transformer architecture be used to improve more ‘traditional’ fields of machine learning ,153,noithatweedisloud,2024-12-26T06:40:47,https://www.reddit.com/r/MachineLearning/comments/1hmitcz/d_everyone_is_so_into_llms_but_can_the/,https://www.reddit.com/r/MachineLearning/comments/1hmitcz/d_everyone_is_so_into_llms_but_can_the/
[D] What's the most promising successor to the Transformer?,178,jsonathan,2025-02-15T06:17:01,https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/,https://www.reddit.com/r/MachineLearning/comments/1ipvau4/d_whats_the_most_promising_successor_to_the/
[D] Transformers are a type of CNN,326,Ozqo,2024-10-24T08:31:06,https://www.reddit.com/r/MachineLearning/comments/1gaxscv/d_transformers_are_a_type_of_cnn/,https://www.reddit.com/r/MachineLearning/comments/1gaxscv/d_transformers_are_a_type_of_cnn/
[R] Timeline of recent Large Language Models / Transformer Models,776,viktorgar,2023-04-16T19:53:45,https://i.redd.it/gl11ce50xaua1.png,https://www.reddit.com/r/MachineLearning/comments/12omnxo/r_timeline_of_recent_large_language_models/
[R] End-to-End Referring Video Object Segmentation with Multimodal Transformers,2029,Illustrious_Row_9971,2022-03-06T03:52:43,https://v.redd.it/pie3qopyqol81,https://www.reddit.com/r/MachineLearning/comments/t7qe6b/r_endtoend_referring_video_object_segmentation/
[Discussion] (Rant) Most of us just pretend to understand Transformers,562,sloppybird,2021-12-02T12:34:57,https://www.reddit.com/r/MachineLearning/comments/r76igz/discussion_rant_most_of_us_just_pretend_to/,https://www.reddit.com/r/MachineLearning/comments/r76igz/discussion_rant_most_of_us_just_pretend_to/
[P] I built a transformer that skips layers per token based on semantic importance,158,Silent_Status_4830,2025-05-18T03:25:06,https://www.reddit.com/r/MachineLearning/comments/1kpalhd/p_i_built_a_transformer_that_skips_layers_per/,https://www.reddit.com/r/MachineLearning/comments/1kpalhd/p_i_built_a_transformer_that_skips_layers_per/
"[D] So, Mamba vs. Transformers... is the hype real?",337,Instantinopaul,2024-01-07T11:19:08,https://www.reddit.com/r/MachineLearning/comments/190q1vb/d_so_mamba_vs_transformers_is_the_hype_real/,https://www.reddit.com/r/MachineLearning/comments/190q1vb/d_so_mamba_vs_transformers_is_the_hype_real/
"[d] Apple claims M2 Ultra ""can train massive ML workloads, like large transformer models.""",283,jl303,2023-06-05T20:01:27,https://www.reddit.com/r/MachineLearning/comments/141pxvc/d_apple_claims_m2_ultra_can_train_massive_ml/,https://www.reddit.com/r/MachineLearning/comments/141pxvc/d_apple_claims_m2_ultra_can_train_massive_ml/
"[Discussion] In this age of LLMs, What are the limitations of Transformer architecture and downside to it?",104,dontgimmehope,2023-12-25T11:35:07,https://www.reddit.com/r/MachineLearning/comments/18qh1hp/discussion_in_this_age_of_llms_what_are_the/,https://www.reddit.com/r/MachineLearning/comments/18qh1hp/discussion_in_this_age_of_llms_what_are_the/
"[R] Google Replaces BERT Self-Attention with Fourier Transform: 92% Accuracy, 7 Times Faster on GPUs",690,Yuqing7,2021-05-14T17:22:45,https://www.reddit.com/r/MachineLearning/comments/ncdy6m/r_google_replaces_bert_selfattention_with_fourier/,https://www.reddit.com/r/MachineLearning/comments/ncdy6m/r_google_replaces_bert_selfattention_with_fourier/
[R] Vision Transformers Don't Need Trained Registers,73,avd4292,2025-06-16T03:56:54,https://www.reddit.com/r/MachineLearning/comments/1lcja93/r_vision_transformers_dont_need_trained_registers/,https://www.reddit.com/r/MachineLearning/comments/1lcja93/r_vision_transformers_dont_need_trained_registers/
[D] Are GNNs obsolete because of transformers?,110,Master_Jello3295,2025-03-22T00:56:04,https://www.reddit.com/r/MachineLearning/comments/1jgwjjk/d_are_gnns_obsolete_because_of_transformers/,https://www.reddit.com/r/MachineLearning/comments/1jgwjjk/d_are_gnns_obsolete_because_of_transformers/
[R] nGPT: Normalized Transformer with Representation Learning on the Hypersphere,124,StartledWatermelon,2024-10-10T15:37:27,https://www.reddit.com/r/MachineLearning/comments/1g0lnij/r_ngpt_normalized_transformer_with_representation/,https://www.reddit.com/r/MachineLearning/comments/1g0lnij/r_ngpt_normalized_transformer_with_representation/
[R] Differential Transformer (Microsoft Research),203,Decent_Action2959,2024-10-08T14:10:25,https://arxiv.org/abs/2410.05258,https://www.reddit.com/r/MachineLearning/comments/1fz0pya/r_differential_transformer_microsoft_research/
[D] Why do transformers use embeddings with the same dimensionality in each layer?,130,timtom85,2024-03-19T19:35:46,https://www.reddit.com/r/MachineLearning/comments/1bit2f9/d_why_do_transformers_use_embeddings_with_the/,https://www.reddit.com/r/MachineLearning/comments/1bit2f9/d_why_do_transformers_use_embeddings_with_the/
[Discussion] Compare OpenAI and SentenceTransformer Sentence Embeddings,549,Simusid,2023-03-11T13:54:22,https://i.redd.it/7muze2s684na1.png,https://www.reddit.com/r/MachineLearning/comments/11okrni/discussion_compare_openai_and_sentencetransformer/
[D] HuggingFace transformers - Bad Design?,143,duffano,2024-08-16T23:30:30,https://www.reddit.com/r/MachineLearning/comments/1eu3auv/d_huggingface_transformers_bad_design/,https://www.reddit.com/r/MachineLearning/comments/1eu3auv/d_huggingface_transformers_bad_design/
[R] [D] My (Mostly Failed) Attempt to Improve Transformers by Enriching Embeddings with the Last Hidden State – Why It Didn't Scale,167,Academic_Sleep1118,2025-03-30T00:29:36,https://www.reddit.com/r/MachineLearning/comments/1jn0ha9/r_d_my_mostly_failed_attempt_to_improve/,https://www.reddit.com/r/MachineLearning/comments/1jn0ha9/r_d_my_mostly_failed_attempt_to_improve/
"[R] [ClsToken, AvgPool] can be a poor choice for transformer embedding models",29,agbrothers,2025-06-23T01:22:32,https://www.reddit.com/r/MachineLearning/comments/1li43eh/r_clstoken_avgpool_can_be_a_poor_choice_for/,https://www.reddit.com/r/MachineLearning/comments/1li43eh/r_clstoken_avgpool_can_be_a_poor_choice_for/
[P][R] Sparse Transformers: Run 2x faster LLM with 30% lesser memory,70,Economy-Mud-6626,2025-06-09T13:11:57,https://www.reddit.com/r/MachineLearning/comments/1l74fv7/pr_sparse_transformers_run_2x_faster_llm_with_30/,https://www.reddit.com/r/MachineLearning/comments/1l74fv7/pr_sparse_transformers_run_2x_faster_llm_with_30/
[R] MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers,277,redpnd,2023-05-15T10:17:25,https://arxiv.org/abs/2305.07185,https://www.reddit.com/r/MachineLearning/comments/13i43n0/r_megabyte_predicting_millionbyte_sequences_with/
Transformer-Based LLMs Are Not General Learners: A Universal Circuit Perspective [R],270,we_are_mammals,2024-01-05T21:39:40,https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/,https://www.reddit.com/r/MachineLearning/comments/18zie7z/transformerbased_llms_are_not_general_learners_a/
[D] Is my take on transformers in time series reasonable / where is it wrong?,37,ReinforcedKnowledge,2025-04-23T16:37:52,https://www.reddit.com/r/MachineLearning/comments/1k63r4a/d_is_my_take_on_transformers_in_time_series/,https://www.reddit.com/r/MachineLearning/comments/1k63r4a/d_is_my_take_on_transformers_in_time_series/
[D] Is it possible to convert music audio to guitar tabs or sheet music with transformers?,20,No-Score712,2025-06-23T11:46:53,https://www.reddit.com/r/MachineLearning/comments/1lieh3l/d_is_it_possible_to_convert_music_audio_to_guitar/,https://www.reddit.com/r/MachineLearning/comments/1lieh3l/d_is_it_possible_to_convert_music_audio_to_guitar/
[D] Best Way to Incorporate Edge Scores into Transformer After GNN?,16,AdInevitable1362,2025-05-10T11:21:35,https://www.reddit.com/r/MachineLearning/comments/1kj7ylw/d_best_way_to_incorporate_edge_scores_into/,https://www.reddit.com/r/MachineLearning/comments/1kj7ylw/d_best_way_to_incorporate_edge_scores_into/
"[D] Biggest roadblock in making ""GPT-4"", a ~20 trillion parameter transformer",350,AxeLond,2020-08-05T17:21:59,https://www.reddit.com/r/MachineLearning/comments/i49jf8/d_biggest_roadblock_in_making_gpt4_a_20_trillion/,https://www.reddit.com/r/MachineLearning/comments/i49jf8/d_biggest_roadblock_in_making_gpt4_a_20_trillion/
"H3 - a new generative language models that outperforms GPT-Neo-2.7B with only *2* attention layers! In H3, the researchers replace attention with a new layer based on state space models (SSMs). With the right modifications, it can outperform transformers. Also has no fixed context length.",483,MysteryInc152,2023-01-24T19:11:08,https://arxiv.org/abs/2212.14052,https://www.reddit.com/r/MachineLearning/comments/10kdeex/h3_a_new_generative_language_models_that/
[P] Guys did my model absolutely blew Transformer?,0,TwoSunnySideUp,2025-03-09T16:45:39,https://www.reddit.com/gallery/1j7bozz,https://www.reddit.com/r/MachineLearning/comments/1j7bozz/p_guys_did_my_model_absolutely_blew_transformer/
[P] BERT-Emotion: Lightweight Transformer Model (~20MB) for Real-Time Emotion Detection,28,boltuix_dev,2025-06-08T10:12:13,https://i.redd.it/rdx2534iho5f1.jpeg,https://www.reddit.com/r/MachineLearning/comments/1l68rlb/p_bertemotion_lightweight_transformer_model_20mb/
[D] Time series Transformers- Autogressive or all at once?,3,Sufficient_Sir_4730,2025-06-16T11:28:17,https://www.reddit.com/r/MachineLearning/comments/1lcqcd6/d_time_series_transformers_autogressive_or_all_at/,https://www.reddit.com/r/MachineLearning/comments/1lcqcd6/d_time_series_transformers_autogressive_or_all_at/
[R] Transformer²: Self-Adaptive LLMs,189,hardmaru,2025-01-15T00:41:09,https://www.reddit.com/r/MachineLearning/comments/1i1l8d4/r_transformer²_selfadaptive_llms/,https://www.reddit.com/r/MachineLearning/comments/1i1l8d4/r_transformer²_selfadaptive_llms/
[D] Why did DeepSeek open-source their work?,953,we_are_mammals,2025-01-27T07:48:28,https://www.reddit.com/r/MachineLearning/comments/1ib2vtx/d_why_did_deepseek_opensource_their_work/,https://www.reddit.com/r/MachineLearning/comments/1ib2vtx/d_why_did_deepseek_opensource_their_work/
[D] Normalization in Transformers,131,Collegesniffer,2024-08-18T06:52:16,https://www.reddit.com/r/MachineLearning/comments/1ev32c0/d_normalization_in_transformers/,https://www.reddit.com/r/MachineLearning/comments/1ev32c0/d_normalization_in_transformers/
[D] What are the OUTPUT embeddings in transformer? Where does it come from? (not the input embeddings),230,ShlomiRex,2024-01-28T12:31:51,https://i.redd.it/akrilj7fd6fc1.png,https://www.reddit.com/r/MachineLearning/comments/1ad1o11/d_what_are_the_output_embeddings_in_transformer/
[D] What Are the Fundamental Drawbacks of Mamba Compared to Transformers?,114,Alarmed-Profile5736,2024-02-24T07:11:08,https://www.reddit.com/r/MachineLearning/comments/1ayog60/d_what_are_the_fundamental_drawbacks_of_mamba/,https://www.reddit.com/r/MachineLearning/comments/1ayog60/d_what_are_the_fundamental_drawbacks_of_mamba/
[R] Simplified RNNs Achieve Transformer-Like Performance with Parallel Training and Reduced Parameters,119,Successful-Western27,2024-12-02T13:19:02,https://www.reddit.com/r/MachineLearning/comments/1h4urpr/r_simplified_rnns_achieve_transformerlike/,https://www.reddit.com/r/MachineLearning/comments/1h4urpr/r_simplified_rnns_achieve_transformerlike/
[R] Adam Optimizer Causes Privileged Basis in Transformer Language Models,70,rrenaud,2024-09-07T16:26:05,https://www.lesswrong.com/posts/yrhu6MeFddnGRSLtQ/adam-optimizer-causes-privileged-basis-in-transformer,https://www.reddit.com/r/MachineLearning/comments/1fbavdv/r_adam_optimizer_causes_privileged_basis_in/
[P] Non Diverse predictions for Time Series Custom Transformer using global Zscore and RevIn,0,Sufficient_Sir_4730,2025-06-14T06:56:19,https://www.reddit.com/r/MachineLearning/comments/1lb2eah/p_non_diverse_predictions_for_time_series_custom/,https://www.reddit.com/r/MachineLearning/comments/1lb2eah/p_non_diverse_predictions_for_time_series_custom/
[R] Differential Transformer,230,fliiiiiiip,2024-10-11T06:26:11,https://www.reddit.com/gallery/1g13gkd,https://www.reddit.com/r/MachineLearning/comments/1g13gkd/r_differential_transformer/
[D] Transformers are basically CNNs?,188,Veson,2023-10-20T22:11:27,https://www.reddit.com/r/MachineLearning/comments/17cmzcz/d_transformers_are_basically_cnns/,https://www.reddit.com/r/MachineLearning/comments/17cmzcz/d_transformers_are_basically_cnns/
[D] What are the biggest architectural innovations since Transformers?,222,MikeFent0n,2022-02-18T06:32:52,https://www.reddit.com/r/MachineLearning/comments/svb5bx/d_what_are_the_biggest_architectural_innovations/,https://www.reddit.com/r/MachineLearning/comments/svb5bx/d_what_are_the_biggest_architectural_innovations/
"[D] In Byte Latent Transformer, how is the decoded patch boundary determined?",45,TommyX12,2024-12-24T17:19:39,https://www.reddit.com/r/MachineLearning/comments/1hli20i/d_in_byte_latent_transformer_how_is_the_decoded/,https://www.reddit.com/r/MachineLearning/comments/1hli20i/d_in_byte_latent_transformer_how_is_the_decoded/
"[R] I've devised a potential transformer-like architecture with O(n) time complexity, reducible to O(log n) when parallelized.",87,Conscious-Gazelle-91,2024-08-15T12:03:51,https://www.reddit.com/r/MachineLearning/comments/1esteqd/r_ive_devised_a_potential_transformerlike/,https://www.reddit.com/r/MachineLearning/comments/1esteqd/r_ive_devised_a_potential_transformerlike/
[R] ConvNets vs Transformers,328,AdelSexy,2022-01-12T12:24:53,https://www.reddit.com/r/MachineLearning/comments/s252wb/r_convnets_vs_transformers/,https://www.reddit.com/r/MachineLearning/comments/s252wb/r_convnets_vs_transformers/
[D] Improving VQVAE+Transformer Text-to-Image Model in TensorFlow – Balancing Codebook Usage and Transformer Learning,4,TubaiTheMenace,2025-05-23T08:45:22,https://www.reddit.com/r/MachineLearning/comments/1ktenon/d_improving_vqvaetransformer_texttoimage_model_in/,https://www.reddit.com/r/MachineLearning/comments/1ktenon/d_improving_vqvaetransformer_texttoimage_model_in/
